{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\"> MBA em Ciência de Dados</span>\n",
    "# <span style=\"color:blue\">Técnicas Avançadas para Captura e Tratamento de Dados</span>\n",
    "\n",
    "## <span style=\"color:blue\"> Matriz Documento $\\times$ Palavras - Bag of Words</span>\n",
    "    \n",
    "## <span style=\"color:blue\">Avaliação</span>\n",
    "\n",
    "**Material Produzido por Luis Gustavo Nonato**<br>\n",
    "**Cemeai - ICMC/USP São Carlos**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os exercícios abaixo fazem uso da coleção de documentos presente no diretório `DocCol2` contido no arquivo <font style=\"font-family: monaco\"> DocCol.zip</font>, o qual pode ser baixado do Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1)\n",
    "Armazene os documentos disponíveis no diretório `DocCol2` em um dicionário onde a chave é o nome do arquivo e o valor é a string contida no arquivo. O documento contendo a string com o maior número de caracteres é:\n",
    "\n",
    "a) gr7<br>\n",
    "b) au2 <br>\n",
    "c) ch5<br>\n",
    "d) au8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: ['au2']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('.\\data\\DocCol2/*') #lendo todos os arquivos e carregando em files\n",
    "\n",
    "#criando os dicionarios\n",
    "docs ={}\n",
    "dic={}\n",
    "\n",
    "#lendo o files, splitando o nome de arquivo como Key e colocando o conteúdo do files como Value para o dicionario\n",
    "for fname in files:\n",
    "    key = fname.split('\\\\')[3]\n",
    "    with open(fname,'r') as f:\n",
    "        docs[key] = f.read()\n",
    "\n",
    "#criando um segundo dicionario que contém a chave e o tamanho do arquivo\n",
    "for k,v in docs.items():\n",
    "    dic[k] = len(v)\n",
    "\n",
    "x = [k for (k,v) in dic.items() if (max(dic.values()) == v)] #pegando o máximo do dicionário\n",
    "\n",
    "print('Resposta:',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2)\n",
    "Crie um dicionário chamado `docsXwords` onde as chaves são os nomes dos arquivos e os valores são as listas de palavras do documento correspondente. As palavras em cada uma das listas devem ser constituídas apenas por letras do alfabeto, estarem lexicamente normalizadas e conterem mais que 1 caracter. Qual o documento cuja lista de palavras resultante possui o **maior** número de palavras repetidas:\n",
    "\n",
    "a) gr22<br>\n",
    "b) ch30<br>\n",
    "c) au2<br>\n",
    "d) au8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta ['ch30']\n"
     ]
    }
   ],
   "source": [
    "docsXwords={}\n",
    "dic={}\n",
    "\n",
    "for fname in files:\n",
    "    key = fname.split('\\\\')[3]\n",
    "    with open(fname,'r') as f:\n",
    "        doc = f.read()\n",
    "        #palavras do doc\n",
    "        words = nltk.word_tokenize(doc)\n",
    "        #removendo simbolos, ou seja, contendo apenas letras do alfabeto\n",
    "        words = [w.lower() for w in words if w.isalpha()]\n",
    "        #removendo stop words -> Neste exercicio não precisamos remover as stopwords, estava errando isso\n",
    "        #words = [w for w in words if w not in stopwords.words('english')]\n",
    "        #normalização léxica utilizando stemming -> vou pegar as palavras e convertê-las no seu radical\n",
    "        words = [PorterStemmer().stem(w) for w in words]\n",
    "        #incluindo dados num dicionario com apenas letras do alfabeto, lexicamente normalizadas e maior que 1 caractere\n",
    "        docsXwords[key] = [w for w in words if len(w) > 1]\n",
    "\n",
    "for k,v in docsXwords.items():\n",
    "    dic[k] = len(v) - len(list(set(v)))\n",
    "\n",
    "x = [k for (k,v) in dic.items() if (max(dic.values()) == v)]\n",
    "print('Resposta',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3)\n",
    "Utilizando as listas de palavras do dicionário `docsXwords`, quais as três palavras que mais aparecem na coleção de documentos:\n",
    "\n",
    "a) the, is, of<br>\n",
    "b) that, is, of<br>\n",
    "c) the, of, to <br>\n",
    "d) to, is, of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   palavras  frequencia\n",
      "2       the        5014\n",
      "26       of        2627\n",
      "62       to        2611\n"
     ]
    }
   ],
   "source": [
    "qtd=[]\n",
    "\n",
    "for v in docsXwords.values():\n",
    "    qtd.extend(v)\n",
    "\n",
    "qtd = Counter(qtd)\n",
    "\n",
    "data = pd.DataFrame(qtd.items(),columns=['palavras','frequencia'])\n",
    "print(data.sort_values(by=['frequencia'],ascending=False)[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4)\n",
    "Qual o documento cuja lista de palavras possui o **menor** número de \"stop words\"? Quantas \"stop words\" aparecem neste documento:\n",
    "\n",
    "a) gr5 com 47 \"stop words\"<br>\n",
    "b) gr17 com 47 \"stop words\"<br>\n",
    "c) gr5 com 37 \"stop words\"<br>\n",
    "d) gr17 com 37 \"stop words\"\n",
    "\n",
    "**Dica**: Crie um dicionário onde a chave é o nome do documento e o valor o número de stop words no documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: gr5 com 47 \"stop words\"\n"
     ]
    }
   ],
   "source": [
    "stop={}\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for key,val in docsXwords.items():\n",
    "    words = [w for w in val if w in stop_words]\n",
    "    stop[key] = len(words)\n",
    "\n",
    "x = [str(k)+' com '+str(v)+' \"stop words\"' for (k,v) in stop.items() if (min(stop.values()) == v)]\n",
    "\n",
    "print('Resposta:',x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5) \n",
    "Utilize o dicionário `docsXwords` para construir\n",
    "uma matriz Documentos $\\times$ Palavras para a coleção de documentos do diretório `DocCol2`. Utilizando a distância cosseno, qual é o documento mais parecido com o documento 'ch7':\n",
    "\n",
    "a) ch8<br>\n",
    "b) ch16<br>\n",
    "c) ch5<br>\n",
    "d) au8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento mais parecido com ch7:  ch16\n"
     ]
    }
   ],
   "source": [
    "ddocs={}\n",
    "corpus=[]\n",
    "\n",
    "for k,v in docsXwords.items():\n",
    "    ddocs[k] = dict(Counter(v))\n",
    "\n",
    "for v in ddocs.values():\n",
    "    corpus.extend(list(v.keys()))\n",
    "\n",
    "corpus = list(set(corpus))\n",
    "\n",
    "rows = list(docs.keys())\n",
    "\n",
    "BoW = pd.DataFrame(data=np.zeros((len(rows),len(corpus))),columns=corpus,index=rows,dtype=int)\n",
    "\n",
    "for k in ddocs.keys():\n",
    "    BoW.loc[k,list(ddocs[k].keys())] = list(ddocs[k].values())\n",
    "\n",
    "ch7 = np.argwhere(BoW.index.values=='ch7')[0][0]\n",
    "\n",
    "X = BoW.values\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# calculando o cosseno utilizando a formula\n",
    "# cos(x,y) = np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "cosch7 = np.apply_along_axis(lambda x: \n",
    "             np.dot(X[ch7],x)/(np.linalg.norm(ch7)*np.linalg.norm(x)),1,X)\n",
    "\n",
    "# ordenando e pegando o maior valor (note que o elemento [-1] é o \n",
    "# próprio documento au8)\n",
    "sim_ch7 = np.argsort(cosch7)[-2]\n",
    "print('Documento mais parecido com ch7: ',BoW.index.values[sim_ch7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
