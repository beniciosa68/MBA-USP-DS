{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotações e Conceitos - Aula04 - ICD\n",
    "\n",
    "\n",
    "## Técnicas de Agrupamento de dados:\n",
    "\n",
    "\n",
    "#### Agrupamento de dados:\n",
    "\n",
    "   ##### Limitação:\n",
    "       Não há uma definição clara sobre o significado de cluster e como encontrá-los. Tudo depende de como você quer agrupar os dados, juntando características em comum no mesmo grupo e características diferentes em grupos diferentes.\n",
    "       \n",
    "       Primeira coisa é definir qual o objetivo do agrupamento dos dados. Depois precisamos definir a similaridade entre objetos e para isso precisamos primeiro definir uma medida de proximidade.\n",
    "       Dois tipos de medida de proximidade:\n",
    "           Medida de similaridade: Distância entre mesmo atributo é máxima. d(Xi,Xi) = máxima\n",
    "               Propriedades dado a distância entre dois pontos p e q:\n",
    "                   1) d(p,q) = 1, se p=q\n",
    "                   2) d(p,q) = d(q,p)\n",
    "               \n",
    "           Medida de dissimilaridade: Distância entre mesmo atributo é zero. d(Xi,Xi) = 0\n",
    "               Propriedades dado a distância entre dois pontos p e q:\n",
    "                   1) d(p,q) >=0 e d(p,q) = 0 se p=q\n",
    "                   2) d(p,q) = d(q,p)\n",
    "                   3) d(p,r) <= d(p,q) + d(q,r) - distância como se fosse um triângulo\n",
    "   \n",
    "   ##### Métricas de distância:       \n",
    "   <img src=\"img/metricas-distancia.gif\" width=\"500\" height=\"300\">\n",
    "   <img src=\"img/nominais-ordinais.gif\" width=\"500\" height=\"300\">\n",
    "       \n",
    "       Para dados nominais, podemos fazer por ex, 1 = homem e 0 = mulher, ou seja, classificação.\n",
    "       \n",
    "       Para dados ordinais, podemos fazer por ex, a diferença entre o primeiro colocado e o terceiro colocado, ou seja, posicional.\n",
    "\n",
    "   ##### Tipos de cluster:\n",
    "       Particionais ou hierárquicos:\n",
    "   <img src=\"img/tipos-cluster.gif\" width=\"500\" height=\"300\">\n",
    "\n",
    "\n",
    "   ##### Passos para agrupamento de dados:\n",
    "   <img src=\"img/passos.gif\" width=\"500\" height=\"300\">\n",
    "\n",
    "\n",
    "#### K-Means:\n",
    "        É o método mais simples de clusterização. Muito utilizado pois na prática pois apresenta simplicidade de uso, interpretabilidade e eficiência computacional. A idéia é agrupar os dados calculando a distância entre um elemento central (centróide) e os dados em si.\n",
    "        \n",
    "        Como funciona:\n",
    "        1) Pegue seus dados e escolha k pontos como sendo suas centróides iniciais. Defina aqui quantos cluster queremos criar, pois isso irá definir quantas centróides iremos trabalhar.\n",
    "        2) Você começa a calcular a distância entre esses pontos e define um grupo. Os pontos mais próximos do ponto 1 viram o cluster 1, os mais próximos do ponto 2 viram o cluster 2 e assim por diante.\n",
    "        3) Depois de fazer isso, você tira uma nova média e define os novos centróides (move os centróides de lugar). Aplica novamente o método de calcular as distâncias e reagrupa os dados.\n",
    "        4) Repete o processo 3 até que as médias não movam mais ou até definir um critério para mover a média num mínimo de distância.\n",
    "        5) Uma vez o critério do passo 4 esteja satisfeito, finalizo o método de clusterização.\n",
    "   <img src=\"img/k-means.gif\" width=\"500\" height=\"300\">   \n",
    "        \n",
    "        OBS: O algorítimo é sensível à posição inicial que escolhermos para as centróides (sementes). E é importante rodar o algorítimo diversas vezes para podermos obter resultados significativos.\n",
    "        \n",
    "   ##### Elbow Method (Método do Cotovelo): \n",
    "       É um método utilizado para nos ajudar a descobrir o número de clusters que iremos usar no agrupamento. Basicamente eu calculo a distância entre cada ponto e a centróide, depois somo tudo.\n",
    "   \n",
    "   <img src=\"img/metodo-cotovelo.gif\" width=\"500\" height=\"300\">\n",
    "               \n",
    "               onde Ci é um grupo e Nc é o número de grupos.\n",
    "               \n",
    "       O resultado disso será um gráfico no formato de um cotovelo, onde o eixo X é o número de clusters e o Y é o WSS da fórmula. Onde estiver a curva desse cotovelo, será o número ideal de clusters que eu posso criar. No exemplo abaixo, o número ideal são 4 clusters:\n",
    "       \n",
    "   <img src=\"img/metodo-cotovelo-grafico.gif\" width=\"600\" height=\"400\">\n",
    "\n",
    "\n",
    "#### Agrupamento Hierárquico:\n",
    "    A idéia desta técnica de agrupamento é gerar um endograma (que é basicamente uma árvore de resultados). Nesse endograma, a regra é a seguinte: caso um dado esteja num nível mais baixo, ele irá pertencer aos dados que estiverem num nível mais acima. No exemplo abaixo, C e B pertencem ao grupo do A.\n",
    "   <img src=\"img/metodo-hierarquico-grafico.gif\" width=\"500\" height=\"300\">\n",
    "\n",
    "\n",
    "    Uma questão fundamental é como calcular a distância entre os clusters. Alguns métodos podem ser usados para isso:\n",
    "    \n",
    "    1) Min (single linkage): \n",
    "        Pego a distância mínima entre dois pontos dos clusters que quero avaliar. Calculo a distância de todos os pontos do cluster 1 em relação ao cluster 2 e vejo qual a menor distância obtida. \n",
    "   <img src=\"img/single-linkage.gif\" width=\"300\" height=\"200\">\n",
    "    \n",
    "    2) Max (complete linkage): \n",
    "    Pego a distância máxima entre dois pontos dos clusters que quero avaliar. Calculo a distância de todos os pontos do cluster 1 em relação ao cluster 2 e vejo qual a maior distância obtida.\n",
    "   <img src=\"img/complete-linkage.gif\" width=\"300\" height=\"200\">\n",
    "   \n",
    "    3) Média dos grupos:\n",
    "    Pego a distância média obtida do cálculo dos grupos. Calculo a distância de todos os pontos do cluster 1 em relação ao cluster 2 e vejo qual a distância média obtida.\n",
    "   <img src=\"img/media-grupos.gif\" width=\"300\" height=\"200\">\n",
    "   \n",
    "    4) Distância entre centróides:\n",
    "        Pego a centróide de cada cluster e calculo a distância entre essas centróides. Para o cluster 1, calculo a média de todos os pontos e acho a centróide 1, faço o mesmo para o cluster 2 e então, calculo qual a distância entre esses pontos médios.\n",
    "   <img src=\"img/distancia-centroides.gif\" width=\"300\" height=\"200\">\n",
    "    \n",
    "    5) Método de Ward:\n",
    "        É utilizado para avaliar dois clusters para saber se eu vou juntar esses clusters ou não.\n",
    "        Como fazer:\n",
    "            1) Para o cluster 1, vou calcular a distância de todos os pontos em relação a centróide.\n",
    "            2) Faço o mesmo para o cluster 2.\n",
    "            3) Vou calcular um novo centróide entre os dois clusters e calcular a distância de todos os pontos em relação a esta centróide.\n",
    "            4) Aplica a fórmula. Ela é basicamente a distância da nova centróide entre os dois clusters juntos menos a soma das distâncias quando elas estão separadas.\n",
    "            \n",
    "   <img src=\"img/metodo-ward.gif\" width=\"500\" height=\"300\"> \n",
    "   \n",
    "       Em resumo, dependendo do método, a clusterização terá resultados diferentes:\n",
    "   <img src=\"img/metodos-clusterizacao.gif\" width=\"500\" height=\"300\">\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "#### Avaliando Agrupamentos:\n",
    "    Objetivos fundamentais dos agrupamentos:\n",
    "        - Determinar se os dados são ruídos, ou seja, evitar encontrar padrões em ruídos\n",
    "        - Comparar diferentes métodos de agrupamentos\n",
    "        - Comparar clusters\n",
    "    \n",
    "    São usados em 3 casos:\n",
    "    \n",
    "        Índice externo:\n",
    "            Neste caso eu conheço as classes e eu quero verificar se os clusters identificados correspondem aos grupos originais. Exemplo, tenho grupo de doentes e saudáveis, faço a clusterização e vejo se os elementos que caíram no grupo de doentes são realmente as pessoas doentes e se os do cluster dos saudáveis, idem.\n",
    "            \n",
    "        Índice interno:\n",
    "            Neste caso não tenho nenhuma informação de fora para comparar os clusters.\n",
    "        \n",
    "        Índice relativo:\n",
    "            Neste caso, é usado para comparar dois agrupamentos.\n",
    "\n",
    "\n",
    "   ##### Matriz de Similaridade: \n",
    "        É uma maneira visual de avaliar os agrupamentos. Ele é mais útil quando temos clusters muito bem definidos, senão, pode ficar dificil a identificação.\n",
    "        Como fazer:\n",
    "        1) Pego os objetos e calculo a distância entre esses objetos depois que identifiquei os clusters.\n",
    "        2) Vou ordenar os dados. Então temos os dados vermelhos são as menores distâncias entre os dados.\n",
    "        3) Neste caso, observa-se visualmente que a distância dos pontos dentro do mesmo cluster são menores e portanto aparecem em destaque com a cor vermelha em comparação ao resto dos pontos, logo, a tendência é termos uma matriz diagonal, igual do desenho abaixo:\n",
    "    \n",
    "   <img src=\"img/matriz-similaridade.gif\" width=\"600\" height=\"400\">\n",
    "   \n",
    "   ##### Índice interno: \n",
    "       Eu uso essa técnica para avaliar os agrupamentos sem ter informação externa sobre os dados. Neste caso, eu avalio dois pontos, baseados na premissa de que quão mais próximos os dados no mesmo cluster melhor e quanto mais afastado os dados de clusters diferentes, melhor.\n",
    "       Para isso usamos uma medida de coesão (quanto menor melhor) e uma medida de separação (quanto maior melhor), conforme mostrado abaixo:\n",
    "       \n",
    "   <img src=\"img/indice-interno.gif\" width=\"600\" height=\"400\">\n",
    "   \n",
    "       Exemplo:\n",
    "       Neste caso eu pego os dados para k=1 (1 cluster) e vejo que a medida de coesão deu 10 e de separação deu 0. Faço agora para k=2 (2 clusters) e, neste caso, a medida de coesão deu 1 e de separação deu 9. Comparando ambos, temos que a medida de coesão de k=2 é muito menor (melhor) e a medida de separação é muito maior (melhor também), logo, concluímos que para estes dados é muito melhor identificarmos 2 clusters ao invés de 1 cluster.\n",
    "       \n",
    "   <img src=\"img/indice-interno-exemplo.gif\" width=\"600\" height=\"400\">\n",
    "   \n",
    "   ##### Índice externo:\n",
    "       Eu uso essa técnica para avaliar os agrupamentos quando se tem o que chamamos de ground truth (quando conhecemos as classes dos dados que estamos analisando).\n",
    "       \n",
    "       Purity (Pureza):\n",
    "       Mede o quão puro é um cluster, ou seja, quando temos dados de apenas um tipo de elemento daquela classe no cluster isso significa que ele é 100% puro. Este método é muito útil para comparar agrupamentos e verificar qual modelo gera um agrupamento com maior pureza. Ela é calculada por:\n",
    "       \n",
    "   <img src=\"img/pureza.gif\" width=\"300\" height=\"200\">\n",
    "       \n",
    "       onde Ci é a classe obtida (ou seja, o cluster) e Tj é a partição.\n",
    "       \n",
    "       Exemplo:\n",
    "       \n",
    "       Neste caso, C1, C2 e C3 são clusters e T1, T2 e T3 são o número de amostras de cada tipo. Assim, por exemplo, para o cluster C1 temos 0 elementos do tipo T1 (iris-setosa), 47 elementos T2 (iris-versicolor) e 14 elementos T3 (iris-virginica). Para o cálculo, eu pego o máximo de cada cluster (C1 = 47, C2 = 50 e C3 = 36), somo tudo e divido pelo número total de elementos (150). Quanto mais próximo de 1, maior a pureza.\n",
    "       \n",
    "   <img src=\"img/exemplo-pureza.gif\" width=\"600\" height=\"400\">\n",
    "       \n",
    "       \n",
    "   ##### Matching (Acoplamento - teoria dos grafos):\n",
    "       Na teoria dos grafos um acoplamento, emparelhamento ou conjunto de arestas independentes em um grafo G é um conjunto de arestas sem vértices em comum, ou seja, você escolhe um vértice e acha quais linhas não tem vértices em comum. \n",
    "       \n",
    "       Exemplo:\n",
    "       No caso M1 abaixo, a aresta c->d não tem pontos em comum com a aresta a->b. Já a aresta c->d e c->a não pode entrar na conta, pois tem o vértice c em comum. A mesma coisa ocorre de arestas dos casos M2 e M3, onde temos arestas que não tem vértices em comum.\n",
    "       \n",
    "   <img src=\"img/maximum-matching.gif\" width=\"500\" height=\"300\">\n",
    "       \n",
    "   ##### Maximal Matching (Acoplamento Máximal):\n",
    "       O acoplamento maximal ocorre quando encontramos o máximo de arestas que não tem pontos em comum no acoplamento M e qualquer aresta que você adiciona, ele deixa de ser um acoplamento M.\n",
    "       No exemplo, se incluirmos a aresta c->b em M1, ele deixa de ser um acoplamento, pois a aresta c->b terá o vértice c em comum com as arestas c->d e a->b.\n",
    "       Exemplos de maximal matching são M1, M2 e M3:\n",
    "   \n",
    "   <img src=\"img/maximal-matching.gif\" width=\"500\" height=\"300\">\n",
    "       \n",
    "       \n",
    "   ##### Maximum Matching (Acoplamento Máximo):\n",
    "       Para o acoplamento máximo é o caso que conseguimos achar o máximo de arestas possíveis M de um grafo.\n",
    "       Exemplos de maximum matching são M1 e M2:\n",
    "   \n",
    "   <img src=\"img/maximum-matching-case.gif\" width=\"500\" height=\"300\">\n",
    "   \n",
    "   \n",
    "   ##### Mutual Information (MI):\n",
    "        A informção mútua (MI) entre duas variáveis X e Y mede quanto o conhecimento de uma dessas variáveis reduz a incerteza sobre a outra. Por exemplo, se  X e Y são independentes, então sabendo  X não fornece nenhuma informação sobre Y e vice-versa, portanto, suas informações mútuas são zero. No outro extremo, se X é uma função determinística de Y e Y é uma função determinística de X então todas as informações transmitidas por X é compartilhado com Y : saber X determina o valor de Y e vice versa. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
