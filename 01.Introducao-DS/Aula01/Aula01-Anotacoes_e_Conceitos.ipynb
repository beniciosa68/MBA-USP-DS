{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotações e Conceitos - Aula01 - ICD\n",
    "\n",
    "\n",
    "## Introdução:\n",
    "\n",
    "\n",
    "#### Medidas de posição:\n",
    "   ##### Moda:\n",
    "        A moda de um conjunto de dados pode ser definida como o valor que ocorre com mais frequência dentro deste conjunto. Por isso, é possível descobrir a moda de uma sequência de valores facilmente, apenas observando o número que mais aparece nela.\n",
    "        - Sistemas Mulimodais:\n",
    "            Quando procuramos o valor mais frequente dentro da sequência 12, 20, 56, 34, 15, 5, 7, 12, 5, percebemos que tanto o número 12 quanto o número 5 se repetem duas vezes. Sem problemas! Isso significa que a sequência é bimodal, ou seja, possui duas modas. Algo semelhante ocorre com a segunda sequência, 1, 9, 2, 1, 4, 6, 5, 3, 2, 9. Nela, os números 1, 2 e 9 são os valores mais frequentes. Assim, podemos dizer que a sequência é trimodal.\n",
    "    \n",
    "   ##### Média:\n",
    "        A média aritmética é a medida de tendência central mais conhecida e mais utilizada para representar um conjunto de valores. Ela pode ser dividida em dois tipos: a média aritmética simples e a média aritmética ponderada.\n",
    "        \n",
    "        média simples:\n",
    "   <img src=\"img/media_simples.gif\" alt=\"title\" >\n",
    "    \n",
    "        média ponderada:\n",
    "   <img src=\"img/media_ponderada.gif\" alt=\"title\" >   \n",
    "    \n",
    "    \n",
    "   ##### Mediana:\n",
    "        A mediana de um conjunto de valores é o valor que está no centro desse conjunto. Desta forma, a metade dos demais elementos do conjunto ficam abaixo da mediana, ou seja, são valores menores que ela, e a outra metade dos elementos fica acima da mediana, pois são valores maiores do que ela.\n",
    "        Sempre que vocês desejarem encontrar a mediana de uma sequência, devem começar reorganizando a mesma em ordem crescente ou decrescente, tanto faz. Aí é só ficar atento aos seguintes detalhes:\n",
    "            - se a sequência apresentar número de elementos ímpar, então, a mediana será o número que ocupar a posição central do conjunto de elementos;\n",
    "            - já se a sequência apresentar número de elementos par, então, a mediana será a média aritmética simples dos dois números que estiverem no centro do conjunto de elementos.\n",
    "\n",
    "#### Medidas de dispersão:\n",
    "   ##### Variância:\n",
    "        É uma medida de dispersão que mostra quão distantes os valores estão da média. O cálculo da variância populacional é obtido através da soma dos quadrados da diferença entre cada valor e a média aritmética, dividida pela quantidade de elementos observados:\n",
    "        \n",
    "   <img src=\"img/variancia.png\" alt=\"title\" >\n",
    "        \n",
    "        Quanto maior for a variância, mais distantes da média estarão os valores, e quanto menor for a variância, mais próximos os valores estarão da média.\n",
    "        Em algumas situações, apenas o cálculo da variância pode não ser suficiente, pois essa é uma medida de dispersão muito influenciada por valores que estão muito distantes da média. Além disso, o fato de a variância ser calculada “ao quadrado” causa uma certa camuflagem dos valores, dificultando sua interpretação.\n",
    "   \n",
    "   \n",
    "   ##### Desvio Padrão:\n",
    "       \n",
    "       É simplesmente o resultado positivo da raiz quadrada da variância. Na prática, o desvio padrão indica qual é o “erro” se quiséssemos substituir um dos valores coletados pelo valor da média. \n",
    "       \n",
    "   <img src=\"img/desvio-padrao.png\" alt=\"title\" >\n",
    "       \n",
    "       Podemos ver a utilização do desvio padrão na apresentação da média aritmética, informando o quão “confiável” é esse valor. Isso é feito da seguinte forma:\n",
    "   \n",
    "       Aplicando desvio padrão na média:\n",
    "   <img src=\"img/aplicar-desvio.gif\" alt=\"title\" >\n",
    "       \n",
    "   ##### Distância Interquartil (IQR):\n",
    "       A sigla IQR significa \"Interquartile Range\" (em português, Variação Interquartil) e é utilizada em análises estatísticas para ajudar nas conclusões a cerca de um conjunto de dados. Geralmente é mais preferível que a variância para se medir a dispersão de um conjunto de dados pois exclui a maioria dos elementos discrepantes do conjunto.\n",
    "       O IQR é definido como a diferença entre o quartil superior e o quartil inferior de um conjunto de dados:\n",
    "   \n",
    "   <img src=\"img/iqr.gif\" alt=\"title\" width=\"200\" height=\"100\">\n",
    "   \n",
    "       Para calcular:\n",
    "           * Organize seu conjunto de dados numéricos em ordem crescente.\n",
    "                Exemplo de dados em quantidade par: 4 7 9 11 12 20\n",
    "                Exemplo de dados em quantidade ímpar: 5 8 10 10 15 18 23\n",
    "                \n",
    "           * Encontre o ponto médio do seu conjunto pra dividir os dados na metade.\n",
    "                Exemplo par: 4 7 9 | 11 12 20\n",
    "                Exemplo ímpar: 5 8 10 (10) 15 18 23\n",
    "           \n",
    "           * Encontre a mediana das partes inferior e superior dos seus dados, excluindo o ponto médio se você tiver quantidade ímpar de elementos.\n",
    "           Exemplo par:\n",
    "                Mediana da metade inferior = 7 (Q1)\n",
    "                Mediana da metade superior = 12 (Q3)\n",
    "            Exemplo ímpar:\n",
    "                Mediana da metade inferior = 8 (Q1)\n",
    "                Mediana da metade superior = 18 (Q3)\n",
    "   \n",
    "            * Subtraia Q3 - Q1 para determinar o IQR.\n",
    "                Exemplo par: 12 - 7 = 5\n",
    "                Exemplo ímpar: 18 - 8 = 10\n",
    "            \n",
    "            \n",
    "   ##### Amplitude:\n",
    "        A amplitude de um conjunto, em Estatística, é a diferença entre o maior elemento desse conjunto e o menor. Em outras palavras, para encontrar a amplitude de uma lista de números, basta subtrair o menor elemento do maior.\n",
    "        \n",
    "   <img src=\"img/amplitude.gif\" alt=\"title\" >\n",
    "\n",
    "#### Correlação:\n",
    "   ##### Coeficiente de Pearson:\n",
    "       O coeficiente de correlação de Pearson, também chamado de \"coeficiente de correlação produto-momento\" ou simplesmente de \"ρ de Pearson\" mede o grau da correlação (e a direção dessa correlação - se positiva ou negativa) entre duas variáveis de escala métrica (intervalar ou de rácio/razão).\n",
    "       \n",
    "   <img src=\"img/pearson.gif\" alt=\"title\" >\n",
    "   onde x1, x2, xn... e y1, y2, yn... são os valores medidos de ambas as variáveis.\n",
    "   \n",
    "   Para além disso,\n",
    "   <img src=\"img/pearson-x-y.gif\" alt=\"title\" >\n",
    "   são as médias aritméticas de ambas as variáveis\n",
    "       \n",
    "       Este coeficiente, normalmente representado por ρ assume apenas valores entre -1 e 1.\n",
    "\n",
    "    ρ =  1 Significa uma correlação perfeita positiva entre as duas variáveis.\n",
    "    ρ = -1 Significa uma correlação negativa perfeita entre as duas variáveis - Isto é, se uma aumenta, a outra sempre diminui.\n",
    "    ρ =  0 Significa que as duas variáveis não dependem linearmente uma da outra. No entanto, pode existir uma dependência não linear. Assim, o resultado ρ = 0 deve ser investigado por outros meios.\n",
    "    \n",
    "    Interpretando ρ:\n",
    "        0.9 para mais ou para menos indica uma correlação muito forte.\n",
    "        0.7 a 0.9 positivo ou negativo indica uma correlação forte.\n",
    "        0.5 a 0.7 positivo ou negativo indica uma correlação moderada.\n",
    "        0.3 a 0.5 positivo ou negativo indica uma correlação fraca.\n",
    "        0 a 0.3 positivo ou negativo indica uma correlação desprezível.\n",
    "   \n",
    "    Interpretação geométrica:\n",
    "       O coeficiente de correlação não é outro senão o cosseno do ângulo α entre os dois vetores!\n",
    "       \n",
    "   <img src=\"img/pearson-coseno.gif\" alt=\"title\" >\n",
    "       \n",
    "    Se ρ =  1, o ângulo α = 0, os dois vetores são colineares (paralelos).\n",
    "    Se ρ =  0, o ângulo α = 90°, os dois vetores são ortogonais.\n",
    "    Se ρ = -1, o ângulo α = 180°, os dois vetores são colineares com sentidos opostos.\n",
    "   \n",
    "   ##### Coeficiente de Spearman:\n",
    "        O coeficiente de correlação de postos de Spearman ou ρ de Spearman é uma medida não paramétrica de correlação de postos (ranks ou índices ou posições). \n",
    "        \n",
    "         Postos são as posições, representados por números, que os valores ocupam quando colocados em ordem crescente. Por exemplo, para estes valores:\n",
    "         12\t17\t15\t19\t14\t16\t11\n",
    "         \n",
    "         Colocando em ordem crescente:\n",
    "         11\t12\t14\t15\t16\t17\t19\n",
    "         \n",
    "         Os postos serão:\n",
    "         valor 11\t12\t14\t15\t16\t17\t19\n",
    "         posto 1\t2\t3\t4\t5\t6\t7\n",
    "         \n",
    "         O posto do valor 14 é 3, o posto de 19 é 7 e assim por diante...\n",
    "        \n",
    "        \n",
    "        O coeficiente de Spearman avalia com que intensidade a relação entre duas variáveis pode ser descrita pelo uso de uma função monótona.\n",
    "        Uma função entre dois conjuntos ordenados é monótona quando ela preserva (ou inverte) a relação de ordem, ou seja se ela é crescente permanece crescente no domínio, se não permanece decrescente.\n",
    "        \n",
    "        A correlação de Spearman entre duas variáveis é igual à correlação de Pearson entre os valores de postos daquelas duas variáveis. Enquanto a correlação de Pearson avalia relações lineares, a correlação de Spearman avalia relações monótonas, sejam elas lineares ou não.\n",
    "        \n",
    "   Para uma amostra de tamanho n, onde os n dados brutos Xi Yi são convertidos em postos rgXi e rgYi, temos:\n",
    "   <img src=\"img/spearman.gif\" alt=\"title\" >   \n",
    "       em que:\n",
    "           \n",
    "            ρ  - denota o usual coeficiente de correlação de Pearson, mas aplicado às variáveis em postos.\n",
    "           \n",
    "            cov(rgx,rgy)  - é a covariância das variáveis em postos.\n",
    "           \n",
    "            sigma rgx e sigma rgy - são os desvios padrão das variáveis em postos.\n",
    "\n",
    "    Interpretação:\n",
    "    O sinal da correlação de Spearman indica a direção da associação entre X (a variável independente) e Y (a variável dependente). \n",
    "    \n",
    "    * Se Y tende a aumentar quando X aumenta, o coeficiente de correlação de Spearman é positivo. \n",
    "    * Se Y tende a diminuir quando X aumenta, o coeficiente de correlação de Spearman é negativo. \n",
    "    * Um coeficiente de Spearman igual a zero indica que não há tendência de que Y aumente ou diminua quando X aumenta. \n",
    "    \n",
    "    A correlação de Spearman aumenta em magnitude conforme X e Y ficam mais próximas de serem funções monótonas perfeitas uma da outra. Quando X e Y são perfeitamente monotonamente relacionadas, o coeficiente de correlação de Spearman se torna 1. Uma relação crescente monótona perfeita implica que, para quaisquer dois pares de valores de dados Xi,Yi e Xj,Yj , Xi − Xj e Yi − Yj terão sempre o mesmo sinal. Uma relação decrescente monótona perfeita implica que estas diferenças terão sempre sinais opostos.\n",
    "    O coeficiente de correlação de Spearman é frequentemente descrito como sendo \"não paramétrico\". Isto pode ter dois sentidos. Em primeiro lugar, uma correlação de Spearman perfeita ocorre quando X e Y estão relacionados por qualquer função monótona, em contraste com a correlação de Pearson, que só dá um valor perfeito quando X e Y estão relacionadas por uma função linear. O outro sentido em que a correlação de Spearman é não paramétrica se refere ao fato de que sua exata distribuição de amostragem pode ser obtida sem conhecimento (isto é, sem informação sobre os parâmetros) quanto à distribuição de probabilidade conjunta de X e Y.\n",
    "    \n",
    "\n",
    "#### Medidas de Entropia:\n",
    "   ##### Entropia de Shannon:\n",
    "       A teoria matemática da informação estuda a quantificação, armazenamento e comunicação da informação. \n",
    "       A medida chave em teoria da informação é a entropia. A entropia é o grau de casualidade, de indeterminação que algo possui. Ela está ligada à quantidade de informação. Fundamentalmente a entropia é uma medida de incerteza. Quanto maior a informação, maior a desordem, maior a entropia. Quanto menor a informação, menor a escolha, menor a entropia. Dessa forma, esse processo quantifica a quantidade de incerteza envolvida no valor de uma variável aleatória ou na saída de um processo aleatório. Por exemplo, a saída de um cara ou coroa de uma moeda honesta (com duas saídas igualmente prováveis) fornece menos informação (menor entropia) do que especificar a saída da rolagem de um dado de seis faces (com seis saídas igualmente prováveis).\n",
    "       \n",
    "       Informação de Shannon (h) ou surpresa:\n",
    "           Exemplo para explicar a medida de Informação de Shannon h ou surpresa. Imagine nesse caso, uma moeda desonesta (enviesada), que tem probabilidade Pcara = 0.9 de dar cara e probabilidade Pcoroa = 0.1 de dar coroa. Por você estar acostumado que a jogada dessa moeda quase sempre dê cara, esse resultado não te surpreende. Mas um resultado coroa te surpreende por conta da \"raridade\" do evento.  Pensando nisso, uma forma natural de se definir essa surpresa que se tem, seria como algo proporcional ao inverso da probabilidade p de ocorrência do evento, de modo que quando menor essa probabilidade maior a surpresa. Shannon definiu essa grandeza como:\n",
    "       \n",
    "   <img src=\"img/surpresa-shannon.gif\" alt=\"title\" >\n",
    "       \n",
    "       Entropia (H):\n",
    "           A fim de chegar na formulação matemática da entropia, imagine por exemplo uma variável aleatória X, que pode assumir dois valores distintos x1 e x2 com probabilidades p1 e p2, respectivamente. Seguindo a notação definida na seção para Variáveis aleatórias discretas, temos:\n",
    "       \n",
    "   <img src=\"img/var-aleatoria-shannon.gif\" alt=\"title\" >\n",
    "       \n",
    "       \n",
    "           A informação de Shannon associada a cada um dos valores é:\n",
    "       \n",
    "   <img src=\"img/informacoes-shannon.gif\" alt=\"title\" >\n",
    "       \n",
    "           Na prática, geralmente nós não estamos interessados em saber a surpresa de um valor em particular que uma variável aleatória pode assumir, e sim a surpresa associada com todos os possíveis valores que essa variável aleatória pode ter. De modo a obter a surpresa associada a todos possíveis valores que X pode assumir, defini-se a entropia H(X) como a informação de Shannon média:\n",
    "   \n",
    "   <img src=\"img/formula1-entropia-shannon.gif\" alt=\"title\" >\n",
    "   \n",
    "           Caso X, possa assumir m valores, a expressão anterior pode ser escrita de modo mais geral:\n",
    "   \n",
    "   <img src=\"img/formula2-entropia-shannon.gif\" alt=\"title\" >\n",
    "       \n",
    "       Exemplo:\n",
    "       \n",
    "   <img src=\"img/exemplo-entropia-dado.gif\" alt=\"title\" >\n",
    "       \n",
    "   \n",
    "   ##### Entropia de Relativa (Divergência de Kulback-Leibler):\n",
    "         A divergência de Kullback-Leibler (também chamada de entropia relativa) é uma medida de, como uma distribuição de probabilidades diverge de uma segunda distribuição de probabilidades esperada distribuição. As aplicações incluem a caracterização da entropia ('teoria da informação') em sistemas de informação, a aleatoriedade, em séries temporais, ganho de informação ao comparar com modelos estatísticos de inferência.\n",
    "         No caso mais simples, se a divergência de Kullback-Leibler for igual a 0, esta indica que podemos esperar um comportamento semelhante, se não o mesmo, entre duas distribuições diferentes, enquanto uma divergência de Kullback-Leibler de 1 indica que as duas distribuições se comportam de maneira diferente.\n",
    "         \n",
    "     Para uma distribuição discreta de probabilidade P e Q, a divergência de Kullback-Leibler de Q para P é definida como:\n",
    "         \n",
    "   <img src=\"img/entropia-relativa.gif\" alt=\"title\" >\n",
    "         \n",
    "         Em outras palavras, é a expectativa da diferença logarítmica entre as probabilidades P e Q, onde a expectativa é obtida usando as probabilidades P.\n",
    "         \n",
    "         \n",
    "         \n",
    "         \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
