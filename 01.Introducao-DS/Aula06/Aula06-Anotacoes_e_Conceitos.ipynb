{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotações e Conceitos - Aula06 - ICD\n",
    "\n",
    "\n",
    "## Métodos de Classificação:\n",
    "\n",
    "\n",
    "#### Knn (K-vizinhos mais próximos):\n",
    "   A ideia principal do KNN é determinar o rótulo de classificação de uma amostra baseado nas amostras vizinhas advindas de um conjunto de treinamento.\n",
    "   Passos para utilização do algoritmo:\n",
    "  \n",
    "   <img src=\"img/passos-knn.gif\" width=\"500\" height=\"300\">\n",
    "    \n",
    "   Para o cálculo de distância podemos usar várias métricas, isso varia e depende de cada caso:\n",
    "   \n",
    "   <img src=\"img/metricas-distancia.gif\" width=\"500\" height=\"300\">\n",
    "   \n",
    "   Graficamente falando:\n",
    "   <img src=\"img/metricas-distancia-graficos.gif\" width=\"500\" height=\"300\">\n",
    "   OBS: Minkowski também conhecido como Manhattan.\n",
    "   \n",
    "   As medidas de distância de uma maneira geral podem ser definidas como medidas de similaridade e dissimilaridade. \n",
    "   \n",
    "   Medida de similaridade: é para definir o grau de semelhança entre as instâncias e realizam o agrupamento de acordo com a sua coesão. Dado a distância entre dois grupos p e q, temos as seguinte propriedades:<br><br>\n",
    "    ● s(p, q) = 1 (ou máximo de similaridade) se p = q,<br> \n",
    "    ● s(p, q) = s(q, p) para todo p e q, onde s(p, q) é a similaridade entre os objetos p e q<br>\n",
    "   \n",
    "   \n",
    "   Medidade de dissimilaridade: é para definir o grau de semelhança de acordo com as diferenças dos atributos das instâncias. Dado a distância entre dois grupos p e q, temos as seguinte propriedades:<br><br>\n",
    "   ● d(p, q) ≥ 0 para todo p e q, e d(p, q) = 0 se, e somente se, p = q, <br>\n",
    "   ● d(p, q) = d(q,p) para todo p e q, <br>\n",
    "   ● d(p, r) ≤ d(p, q) + d(q, r) para todo p, q, e r, onde d(p, q) é a distância de dissimilaridade entre os pontos p e q.<br>\n",
    "\n",
    "   <img src=\"img/similaridade-dissimilaridade.gif\" width=\"400\" height=\"300\">\n",
    "\n",
    "\n",
    "   A região de decisão para classificar um objeto pode depender de k. A melhor maneira de encontrar o melhor valor de k é usar validação cruzada e uma medida para avaliar o resultado da classiﬁcação, como a acurácia.\n",
    "   \n",
    "   Propriedades:<br><br>\n",
    "   ● O algoritmo não “aprende” um modelo, apenas memoriza objetos de treinamento <br>\n",
    "   ● Adia computação para a fase de classiﬁcação <br>\n",
    "   ● O algoritmo pode ser entendido como não paramétrico, dependendo apenas do número de vizinhos k. <br>\n",
    "   ● É um classiﬁcador não-linear, não sendo restrito a regiões de separação lineares. <br>\n",
    "   ● Como geralmente a distância Euclidiana é considerada, é necessário normalizar ou padronizar os dados. <br>\n",
    "   ● Dado que o conjunto de treinamento seja relativamente grande, pode-se provar que o erro cometido na classiﬁcação é no máximo duas vezes maior do que o classiﬁcador Bayesiano, que é ótimo. <br>\n",
    "\n",
    "#### Regressão Logística:\n",
    "   Para a regressão linear simples, vimos que a saída de Y se comporta como uma reta, onde o objetivo é encontrar os parâmetros beta. Se considerarmos o Y como um número inteiro, podemos usar a regressão para classificar os dados.\n",
    "   Para isso, precisamos trabalhar com probabilidades, considerando p(y=1|x) e p(y=0|x). A função que usamos é a função logística, que retorna 0 ou 1.\n",
    "   \n",
    "   <img src=\"img/funcao-logistica.gif\" width=\"700\" height=\"600\">\n",
    "   \n",
    "   A classificação é feita, quando achamos as superfícies de separação, para isso fazemos o seguinte, aplicando a função logística:\n",
    "   \n",
    "   <img src=\"img/classificacao-logistica.gif\" width=\"500\" height=\"300\">\n",
    "   \n",
    "   Cada superfície calculada classifica o dado em uma região:\n",
    "   <img src=\"img/classificacao-logistica-graficos.gif\" width=\"600\" height=\"400\">\n",
    "\n",
    "#### Naive Bayes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
