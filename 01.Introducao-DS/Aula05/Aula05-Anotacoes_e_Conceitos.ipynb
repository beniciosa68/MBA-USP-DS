{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotações e Conceitos - Aula05 - ICD\n",
    "\n",
    "\n",
    "## Modelos de Regressão:\n",
    "\n",
    "\n",
    "#### Regressão Linear Simples:\n",
    "\n",
    "   Regressão linear simples se encaixa uma linha reta através do conjunto de pontos n de tal forma que faça a soma dos quadrados residuais do modelo ( isto é, as distâncias entre os pontos verticais do conjunto de dados e a linha reta) tão pequena quanto possível. O modelo é dado por:\n",
    "   $$\n",
    "    Y \\approx \\beta_0 + \\beta_1 X$\n",
    "   $$\n",
    "   \n",
    "   <img src=\"img/regressao-linear-simples.gif\" width=\"500\" height=\"300\">\n",
    "   \n",
    "Estimando os coeficiente através do método dos mínimos quadrados, obtemos:\n",
    "    $$\n",
    "    \\begin{cases}\n",
    "    \\hat{\\beta}_1=\\frac{\\sum_{i=1}^n (x_i- \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sigma_{xy}}{\\sigma_{xx}}\\\\\n",
    "    \\hat{\\beta}_0= \\bar{y}-\\hat{\\beta_1}\\bar{x}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "Para quantificar a acurácia do modelo, usamos o erro padrão residual (residual standard error):\n",
    "    $$\n",
    "    RSE = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}\n",
    "    $$\n",
    "\n",
    "Outra medida importante é o coeficiente R2, que mede a proporção da variabilidade em Y que pode ser explicada a partir de X.\n",
    "    $$\n",
    "    R^2 = 1-\\frac{\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}, \\quad 0\\leq R^2\\leq 1\n",
    "    $$\n",
    "\n",
    "Quanto mais próximo de um, melhor é o ajuste da regressão linear.\n",
    "\n",
    "#### Regressão Linear Múltipla:\n",
    "\n",
    " Chamamos Modelo de Regressão Linear Múltipla a qualquer modelo de regressão linear com duas ou mais variáveis explicativas.\n",
    " \n",
    " <img src=\"img/regressao-linear-multipla.gif\" width=\"500\" height=\"300\">\n",
    " \n",
    " \n",
    " A estimação dos coeficientes pelo método dos mínimos quadrados resulta na relação:\n",
    "    $$\n",
    "    \\beta = (\\text{X}^T \\text{X})^{-1} \\text{X}^T \\text{y}\n",
    "    $$\n",
    "onde\n",
    "    $$\n",
    "    \\text{X} = \\begin{pmatrix}\n",
    "    1 & x_{11} &x_{12} &\\ldots &x_{1d} \\\\\n",
    "    1 & x_{21} &x_{22} &\\ldots &x_{2d} \\\\\n",
    "    \\ldots & \\ldots &\\ldots &\\ldots &\\ldots \\\\\n",
    "    1 & x_{n1} &x_{n2} &\\ldots &x_{nd} \\\\\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "\n",
    "#### Variáveis Não Lineares:\n",
    "\n",
    "Notem que o modelo não precisa ter termos lineares em X, mas apenas nos parâmetros. Por exemplo, modelo abaixo ainda é linear nos parâmetros: $$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon$$\n",
    "\n",
    "\n",
    "#### Regularização:\n",
    "Nos modelos de regressão pode ocorrer o problema de overfitting. \n",
    "\n",
    "<img src=\"img/overfitting.gif\" width=\"500\" height=\"300\">\n",
    "\n",
    "A regularização é uma maneira de evitar isso. E uma das mais conhecidas é o método de ridge regression ou Tikhonov regularization e outro método bastante conhecido é o LASSO.\n",
    "\n",
    "##### Ridge Regression (Tikhonov regularization):\n",
    "A regressão de Ridge é uma técnica para analisar vários dados de regressão que sofrem de multicolinearidade. Quando ocorre multicolinearidade, as estimativas de mínimos quadrados são imparciais, mas suas variações são grandes e podem estar longe deo verdadeiro valor. Ao adicionar um grau de viés às estimativas de regressão, a regressão de crista reduz os erros padrão.\n",
    "\n",
    "<img src=\"img/ridge-regression.gif\" width=\"600\" height=\"500\">\n",
    "\n",
    "\n",
    "##### LASSO (Least Absolute Shrinkage and Selection Operator):\n",
    "Em estatística e aprendizado de máquina , o lasso ( operador de contração e seleção menos absoluto ; também Lasso ou LASSO) é um método de análise de regressão que executa a seleção de variáveis e a regularização , a fim de aprimorar a precisão das previsões e a interpretabilidade do modelo estatístico que produz. \n",
    "\n",
    "Enquanto ridge regression mantém os valores dos parâmetros pequenos, LASSO tende a selecionar alguns valores para serem diferentes de zero, enquanto que outros são exatamente iguais a zero. Assim, LASSO pode ser usado para selecionar atributos.\n",
    "\n",
    "<img src=\"img/ridge-vs-lasso.gif\" width=\"500\" height=\"300\"> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
