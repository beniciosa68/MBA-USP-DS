{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Questão 1)\n",
    "\n",
    "Considere a afirmação: o aprendizado por reforço é supervisionado. Em que sentido essa afirmação estaria correta?\n",
    "\n",
    "(a) Problemas de aprendizado por reforço recebem por entrada pares de exemplo e rótulo<br>\n",
    "<b>(b) O aprendizado por reforço é supervisionado por meio da observação do ambiente e retorno obtido na forma de uma recompensa após a realização de uma ação<br></b>\n",
    "(c) Problemas de aprendizado por reforço recebem por entrada muitos exemplos, alguns rótulos ligados a esse exemplos e executam por grande número de épocas<br>\n",
    "(d) O aprendizado por reforço é apenas semi-supervisionado, pois um agente observa parcialmente o estado do ambiente, portanto essa afirmação está incorreta.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "Quais os passos fundamentais em uma execução de um algoritmo de aprendizado por reforço em um dado instante t dentro um episódio?\n",
    "\n",
    " <b>(a) Executar uma ação selecionada por meio de uma política atual, recebimento de uma recompensa e observação que potencialmente modifica o estado do agente.<br></b>\n",
    " (b) Obter um exemplo de treinamento, predizer uma ação com base nesse exemplo e comparar com a ação que deveria ter sido realizada<br>\n",
    " (c) Executar uma sequência de ações e, após atingir um estado terminal, computar a recompensa total<br>\n",
    " (d) Amostrar uma ação dentre as possíveis segundo uma política e fornecer essa ação como entrada para uma rede neural que irá classificá-la entre uma ação correta e incorreta.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "Quais os algoritmos de treinamento que representam as duas principais abordagens no aprendizado por reforço?\n",
    "\n",
    " (a) Backpropagation e Stochastic Gradient Descent<br>\n",
    " (b) Deep Q-Network e Entropia Cruzada<br>\n",
    " <b>(c) Policy Learning e Value Learning<br></b>\n",
    " (d) Actor-Critic e Feed-forward network<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Na biblioteca `gym` carregue o ambiente `Blackjack-v0` que representa o Jogo 21, no qual se pode pedir mais cartas ou parar, o ambiente `Pendulum-v0` no qual o objetivo é equilibrar uma alavanca giratória num ângulo de 90 graus, o `CartPole-v1`, no qual o objetivo é equilibrar um poste sobre um carro, movimentando esse carro, e finalmente `Acrobot-v1` que é um pêndulo bi-articulado com dois segmentos, com objetivo de levantar a ponta do segundo segmento a uma altura ao menos igual ao tamanho dos segmento por cima da base.\n",
    "\n",
    "Como é o espaço de ações possíeis desses ambientes?\n",
    "\n",
    "(a) Blackjack: discreto 2 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -1 e 1, Acrobot: contínuo entre -1 e 1<br>\n",
    "(b) Blackjack: discreto 3 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -2 e 2, Acrobot: discreto 2 ações<br>\n",
    "<b>(c) Blackjack: discreto 2 ações, Pendulum:  contínuo entre -2 e 2, CartPole: discreto 2 ações, Acrobot: discreto 3 ações<br></b>\n",
    "(d) Blackjack: discreto 3 ações, Pendulum: contínuo entre -2 e 2, CartPole: discreto 3 ações, Acrobot: contínuo entre -1 e 1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente Blackjack: Discrete(2) Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Ambiente Pendulum: Box(-2.0, 2.0, (1,), float32) Box(-8.0, 8.0, (3,), float32)\n",
      "Ambiente CartPole: Discrete(2) Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Ambiente Acrobot: Discrete(3) Box(-28.274333953857422, 28.274333953857422, (6,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env1 = gym.make('Blackjack-v0')\n",
    "print('Ambiente Blackjack:', env1.action_space, env1.observation_space)\n",
    "\n",
    "env2 = gym.make(\"Pendulum-v0\")\n",
    "print('Ambiente Pendulum:', env2.action_space, env2.observation_space)\n",
    "\n",
    "env3 = gym.make(\"CartPole-v1\")\n",
    "print('Ambiente CartPole:', env3.action_space, env3.observation_space)\n",
    "\n",
    "env4 = gym.make('Acrobot-v1')\n",
    "print('Ambiente Acrobot:', env4.action_space, env4.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Carregue o ambiente `Blackjack-v0`. Esse problema gera recompensa +1 para vitória, 0 para pedir uma carta, e -1 para derrota. Inicialize o ambiente, execute 100 mil episódios (cada um até o estado terminal) e calcule a média de recompensas totais (MR), a média de ações por episódio (MA), a taxa de vitórias (TV) e a taxa de derrotas (TD), sendo vitórias e derrotas medidas após o final de cada episódio. Arredonde os valores para 1 casas decimal.\n",
    "\n",
    "(a) MR é negativo, MP está entre 2 e 3, TV é igual a TD<br>\n",
    "(b) MR é 0.0, MP é 1.4, TV é menor do que TD<br>\n",
    "(c) MR é positivo, MP está entre 1 e 2, TV é igual a TD<br>\n",
    "<b>(d) MR é negativo, MP está entre 1 e 2, TV é menor do que TD<br></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#inicialização do ambiente\n",
    "env = gym.make('Blackjack-v0')\n",
    "print(env.observation_space) #tupla com 3 valores discretos de tamanhos 32, 11 e 2\n",
    "print(env.action_space) #um valor discreto de tamanho 2\n",
    "\n",
    "#s = env.reset()\n",
    "#print(env.reset())\n",
    "#q_table = np.zeros([env.observation_space[0].n, env.action_space.n])\n",
    "#print(q_table[s,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da Tabela Q: (32, 2)\n",
      "\n",
      "Média de recompensas totais  (MR): -0.2\n",
      "Média de passos por episódio (MP): 1.0\n",
      "Taxa de vitórias (TV): 0.4\n",
      "Taxa de derrotas (TD): 0.6\n"
     ]
    }
   ],
   "source": [
    "#episodios\n",
    "n_episodes = 100000\n",
    "total_epochs = 0\n",
    "total_recs = 0\n",
    "win = 0\n",
    "lose = 0\n",
    "\n",
    "#montando tabela Q de aprendizado\n",
    "q_table = np.zeros([env.observation_space[0].n, env.action_space.n]) #colocando a maior dimensão discreta do modelo (32)\n",
    "\n",
    "print('Dimensões da Tabela Q:', q_table.shape)\n",
    "\n",
    "#semente\n",
    "random.seed(42)\n",
    "\n",
    "#rodando episodios\n",
    "for t in range(1,n_episodes+1):\n",
    "    s = env.reset() #reiniciando modelo formato (x, y, bool)\n",
    "    epochs, rec_total_i = 0,0\n",
    "    end = False\n",
    "    \n",
    "    while not end:\n",
    "        action = np.argmax(q_table[s,]) #ação com base na otimização do argumento máximo que retorne maior valor da função\n",
    "        s, r, end, info = env.step(action) #executa um passo\n",
    "        epochs +=1\n",
    "        rec_total_i += r\n",
    "        \n",
    "    #consolidando métrica por episodio\n",
    "    total_epochs += epochs\n",
    "    total_recs  += rec_total_i\n",
    "    \n",
    "    if rec_total_i > 0:\n",
    "        win += 1\n",
    "    else:\n",
    "        lose += 1\n",
    "\n",
    "print()\n",
    "print('Média de recompensas totais  (MR): %.1f' % (total_recs/n_episodes))\n",
    "print('Média de passos por episódio (MP): %.1f' % (total_epochs/n_episodes))\n",
    "print('Taxa de vitórias (TV): %.1f' % (win/n_episodes))\n",
    "print('Taxa de derrotas (TD): %.1f' % (lose/n_episodes))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
