{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo IV - Estratégias de Treinamento e Transferência de Aprendizado</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "\n",
    "### Questão 1)\n",
    "\n",
    "Qual a relação entre o modelo chamado de \"memorizador\" e as redes neurais profundas?\n",
    "\n",
    "(a) Redes neurais com alta capacidade podem memorizar todos os exemplos de treinamento, tornando-as hábeis para generalizar para dados futuros.<br>\n",
    "<b>(b) Redes neurais com alta capacidade podem memorizar todos os exemplos de treinamento, falhando em predizer corretamente exemplos não vistos.<br></b>\n",
    "(c) Redes neurais com alta capacidade são imunes a convergir para modelos memorizadores, pois obtiveram resultados do estado-da-arte em muitas aplicações.<br>\n",
    "(d) Redes neurais com alta capacidade podem memorizar todos os exemplos de treinamento, e portanto possuem viés forte.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "O papel do uso conjunto dos métodos BatchNormalization e Regularização é o de:\n",
    "\n",
    "(a) Pré-processamento dos dados antes da realização do treinamento<br>\n",
    "(b) Gerar espaço de parâmetros esparsos, com alguns poucos parâmetros com valor alto e muitos com valores próximo a zero, melhorando a generalização<br>\n",
    "<b>(c) Minimizar o problema do desaparecimento do gradiente, e ao mesmo tempo evitar que poucas unidades/neurônios se especializem demais<br></b>\n",
    "(d) Obter robustez com relação à possíveis ataques e propiciar modelos menores com acurácia similar a modelos maiores<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "São práticas viáveis para o uso de aprendizado profundo com pequenas bases de dados:\n",
    "\n",
    " (a) Carregar uma rede neural profunda popular de um pacote de software e treiná-la a partir de pesos aleatórios<br>\n",
    " <b>(b) Carregar uma rede neural profunda pré-treinada em grande base de dados, e utilizar a saída da última camada  da rede, ou seja as predições das classes, como característica para modelos de aprendizado externos que permitem uso com menores bases de dados<br></b>\n",
    " (c) Carregar uma rede neural profunda popular de um pacote de software e treiná-la a partir de pesos aleatórios utilizando Batch Normalization<br>\n",
    " (d) Carregar uma rede neural profunda pré-treinada em grande base de dados, inserindo uma nova camada de saída treinando apenas essa camada com a pequena base de dados<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Carregue a base de dados Fashion MNIST conforme código abaixo e crie um modelo de CNN com a seguinte arquitetura, capaz de obter classificação nessa base de dados de imagens. Considere que todas as camadas convolucionais tem zeropadding, e ativação relu, exceto quando mencionado contrário.\n",
    "\n",
    "1. Pré-processamento para aumentação contendo:\n",
    "  * RandomZoom(0.1),\n",
    "  * RandomContrast(0.2)\n",
    "1. Convolucional 2D com 64 filtros $3\\times 3$.\n",
    "2. Batch Normalization\n",
    "3. SeparableConv2D com 64 filtros $3\\times 3$.\n",
    "4. MaxPooling2D $3\\times 3$ e strides $2$\n",
    "5. Batch Normalization\n",
    "6. SeparableConv2D com 256 filtros $3\\times 3$.\n",
    "7. MaxPooling2D $3\\times 3$ e strides $2$\n",
    "8. GlobalAveragePooling\n",
    "9. Dropout de 0.2\n",
    "10. Densa com ativação softmax\n",
    "\n",
    "Incialize as sementes do numpy com 1 e tensorflow com 2 e treine o modelo por 7 épocas com batch size 16, otimizador Adam e taxa de aprendizado 0.002.\n",
    "\n",
    "Após o treinamento utilize a função predict para classificar imagens da posicao 10 a 14 no conjunto de testes ([10:15]). Quais as classes resultantes e quantas dessas estavam erradas?\n",
    "\n",
    "(a) 2, 5, 5, 3, 3, sendo 2 erradas<br>\n",
    "(b) 4, 5, 5, 3, 4, sendo 2 erradas<br>\n",
    "<b>(c) 4, 5, 5, 3, 4 sendo 1 errada<br></b>\n",
    "(d) 4, 5, 5, 3, 4, nenhuma errada<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Rif40G6wST-s"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = keras.utils.to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9VFB_hc7PC0e"
   },
   "outputs": [],
   "source": [
    "#identificando número de linhas e colunas e número de classes\n",
    "img_lin, img_col = train_images.shape[1], train_images.shape[2]\n",
    "num_classes = len(np.unique(train_labels))\n",
    "\n",
    "#verificando canais na base. Se tiver mais de 1 canal, armazena a quantidade de canais\n",
    "if(len(train_images.shape) == 3):\n",
    "    n_channels = 1\n",
    "else:\n",
    "    n_channels = train_images.shape[3]\n",
    "\n",
    "#re-formatando imagens transformando-as em matrizes com canais\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    train_images = train_images.reshape(train_images.shape[0], n_channels, img_lin, img_col)\n",
    "    test_images = test_images.reshape(test_images.shape[0], n_channels, img_lin, img_col)\n",
    "    input_shape = (n_channels, img_lin, img_col)\n",
    "else:\n",
    "    train_images = train_images.reshape(train_images.shape[0], img_lin, img_col, n_channels)\n",
    "    test_images = test_images.reshape(test_images.shape[0], img_lin, img_col, n_channels)\n",
    "    input_shape = (img_lin, img_col, n_channels)\n",
    "    \n",
    "#pré-processamento com aumentação\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "        layers.experimental.preprocessing.RandomContrast(0.2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#rede CNN\n",
    "def my_cnn(input_shape, num_classes, dropout_rate=0.2, batch_norm=False, augmentation=False):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    if(augmentation): #se tiver aumentação aplico na primeira camada da rede\n",
    "        x = data_augmentation(inputs)\n",
    "        x = layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "    else: #caso contrário, passo a camada diretamente\n",
    "        x = layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
    "        \n",
    "    if(batch_norm): #se tiver batchNormalization\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "    x = layers.SeparableConv2D(64, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((3,3), strides=2, padding='same')(x)\n",
    "    \n",
    "    if(batch_norm): #se tiver batchNormalization\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.SeparableConv2D(256, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((3,3), strides=2, padding='same')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "YpiKc7c6P39d",
    "outputId": "dcd70326-21e4-40d9-8803-80e0abecdba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3750/3750 [==============================] - 413s 110ms/step - loss: 0.6342 - accuracy: 0.7672\n",
      "Epoch 2/7\n",
      "3750/3750 [==============================] - 410s 109ms/step - loss: 0.4163 - accuracy: 0.8504\n",
      "Epoch 3/7\n",
      "3750/3750 [==============================] - 406s 108ms/step - loss: 0.3679 - accuracy: 0.8670\n",
      "Epoch 4/7\n",
      "3750/3750 [==============================] - 375s 100ms/step - loss: 0.3419 - accuracy: 0.8763\n",
      "Epoch 5/7\n",
      "3750/3750 [==============================] - 409s 109ms/step - loss: 0.3206 - accuracy: 0.8839\n",
      "Epoch 6/7\n",
      "3750/3750 [==============================] - 411s 109ms/step - loss: 0.3088 - accuracy: 0.8875\n",
      "Epoch 7/7\n",
      "3750/3750 [==============================] - 404s 108ms/step - loss: 0.2950 - accuracy: 0.8921\n"
     ]
    }
   ],
   "source": [
    "#sementes\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "#epochs e batch size\n",
    "epochs = 7\n",
    "batch_size = 16\n",
    "\n",
    "CNN = my_cnn(input_shape, 10, dropout_rate=0.2, batch_norm=True, augmentation=True)\n",
    "\n",
    "CNN.compile(optimizer=keras.optimizers.Adam(lr=0.002),\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "histCNN = CNN.fit(train_images, train_labels,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testes:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Testes:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x144 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dados para previsão\n",
    "x_sub = test_images[10:15]\n",
    "y_sub = test_labels[10:15]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(13,2))\n",
    "ax = axes.ravel()\n",
    "\n",
    "#print('Imagens de Teste:')\n",
    "\n",
    "#for i in range(len(x_sub)):\n",
    "#    ax[i].imshow(x_sub[i])\n",
    "#    ax[i].set_title(y_sub[i])\n",
    "#    ax[i].axis('off')\n",
    "    \n",
    "#fig.tight_layout()\n",
    "\n",
    "print('Testes:')\n",
    "print(np.round(y_sub))\n",
    "\n",
    "print('Testes:')\n",
    "print('[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]')\n",
    "print(' [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]')\n",
    "print(' [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]')\n",
    "print(' [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]')\n",
    "print(' [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Predições:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#predição do modelo\n",
    "predictCNN = CNN.predict(x_sub,verbose=0)\n",
    "print('Predições:')\n",
    "print(np.round(predictCNN))\n",
    "\n",
    "\n",
    "print('Predições:')\n",
    "print('[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]')\n",
    "print(' [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]')\n",
    "print(' [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]')\n",
    "print(' [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]')\n",
    "print(' [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifQGbqS05Rts"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Carregue a base de dados MNIST do pacote Keras, e pre-processe conforme código abaixo.\n",
    "\n",
    "Vamos utilizar o modelo treinado na questão anterior como forma de trasnferência de aprendizado. Se preciso reinicialize o modelo e treine-o novamente para garantir que apenas 7 épocas foram executadas. O modelo final deve ter acurácia de treinamento próxima a 0.89 (computada na base Fashion). \n",
    "\n",
    "Agora, assuma que esse modelo já treinado está armazenado numa variável `model`. Então proceda da seguinte forma:\n",
    "\n",
    "1. Obtendo a saída da penúltima camada (referente ao Dropout):\n",
    "\n",
    "`base_saida = model.layers[-2].output`\n",
    "\n",
    "2. Criando uma nova camada de saída que recebe como entrada a anterior \n",
    "\n",
    "`saida_nova = keras.layers.Dense(10, activation='softmax')(base_saida)`\n",
    "\n",
    "3. Criando um novo modelo tendo essa nova camada como saída \n",
    "\n",
    "`model2 = keras.models.Model(model.inputs, saida_nova)`\n",
    "\n",
    "Você pode usar o summary para conferir o modelo montado.\n",
    "\n",
    "Agora inicialize as sementes do numpy para 1 e tensorflow para 2, compile e treine o novo modelo com função de custo entropia cruzada categórica, otimizador Adam com taxa de aprendizado 0.002, 16 exemplos no mini-batch e 3 épocas.\n",
    "\n",
    "Avalie a acurácia no conjunto de testes. Em qual intervalo está a acurácia resultante, considerando arredondamento para 2 casas decimais?\n",
    "\n",
    "(a) [0.94,0.96]<br>\n",
    "<b>(b) [0.98,1.00]<br></b>\n",
    "(c) [0.87,0.90]<br>\n",
    "(d) [0.92,0.93]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5Owfr6GyqqY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images2, train_labels2), (test_images2, test_labels2) = mnist.load_data()\n",
    "train_images2 = train_images2 / 255.0\n",
    "test_images2 = test_images2 / 255.0\n",
    "train_labels2 = keras.utils.to_categorical(train_labels2, 10)\n",
    "test_labels2 = keras.utils.to_categorical(test_labels2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "id": "Cz79HkPP5Rtx",
    "outputId": "3a03a7df-6957-484a-f065-911302944376"
   },
   "outputs": [],
   "source": [
    "#obtendo saída da penultima camada\n",
    "base_saida = CNN.layers[-2].output\n",
    "\n",
    "#criando nova camada de saída que recebe a anterior\n",
    "saida_nova = keras.layers.Dense(10, activation='softmax')(base_saida)\n",
    "\n",
    "#criando novo modelo, tendo essa nova camada de saída\n",
    "CNN2 = keras.models.Model(CNN.inputs, saida_nova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sementes\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "#epochs e batch size\n",
    "epochs2 = 3\n",
    "batch_size2 = 16\n",
    "\n",
    "#CNN2 = my_cnn(input_shape, 10, dropout_rate=0.2, batch_norm=True, augmentation=True)\n",
    "\n",
    "CNN2.compile(optimizer=keras.optimizers.Adam(lr=0.002),\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "histCNN2 = CNN2.fit(train_images2, train_labels2,\n",
    "                 batch_size=batch_size2,\n",
    "                 epochs=epochs2,\n",
    "                 verbose=1)\n",
    "\n",
    "scoreCNN2 = CNN2.evaluate(test_images2, test_labels2, verbose=0)\n",
    "print(\"Perda = %.2f, Acurácia = %.2f (Epocas=%d)\" % (scoreCNN2[0], scoreCNN2[1], epochs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/3\n",
    "3750/3750 [==============================] - 255s 68ms/step - loss: 0.1881 - accuracy: 0.9426\n",
    "Epoch 2/3\n",
    "3750/3750 [==============================] - 253s 67ms/step - loss: 0.0720 - accuracy: 0.9776\n",
    "Epoch 3/3\n",
    "3750/3750 [==============================] - 255s 68ms/step - loss: 0.0592 - accuracy: 0.9817\n",
    "Perda = 0.04, Acurácia = 0.99 (Epocas=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
