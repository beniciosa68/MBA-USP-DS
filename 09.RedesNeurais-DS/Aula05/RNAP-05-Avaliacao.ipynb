{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo V - Redes neurais auto-associativas e geradoras</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Questão 1)\n",
    "\n",
    "Autoencoders do tipo Overcomplete possuem dimensão latente superior a da entrada. Como garantir o aprendizado sem que haja uma cópia simples dos dados?\n",
    "\n",
    "(a) Utilizando mais camadas, projetando um Autoencoder Overcomplete Profundo que permita obter uma camada latente com maior qualidade<br>\n",
    "(b) Substituindo a função de custo de perda ou erro quadrático pela função de custo de entropia cruzada<br>\n",
    "(d) Utilizando normalização do tipo Batch ou Layer para que os dados sejam modificados ao longo da rede neural, assim evitando que haja uma cópia direta da entrada para o código<br>\n",
    "<b>(d) Impondo uma restrição na projeção do código que penalize o uso de todas as dimensões do espaço latente e privilegie projeções esparsas para uma dada instância.<br></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "Considere uma arquitetura com dois elementos, organizada na forma encoder-decoder. O primeiro elemento, encoder, aprende a gerar uma camada latente com os parâmetros de distribuições de probabilidade, a partir da qual o segundo componente, decoder amostra exemplos a serem reconstruídos. Esse método é conhecido por:\n",
    "\n",
    " (a) Continuous Generalized Autoencoder<br>\n",
    " (b) Denoising Overcomplete Autoencoder<br>\n",
    " (c) Generative Adversarial Network<br>\n",
    " <b>(d) Variational Autoencoder</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "Nas redes geradoras adversariais, todos os dados de treinamento pertencem a classe positiva, enquanto que todos os dados gerados a partir de uma amostragem aleatória de uma distribuição pertence à classe negativa. É correto afirmar sobre o método de aprendizado de redes geradoras adversariais (em sua formulação original):\n",
    "\n",
    " (a) O modelo discriminador é um classificador, que produz uma única perda utilizada para treinar a rede como um todo. Sendo um classificador o responsável pelo ajuste dos parâmetros, consideramos o aprendizado como supervisionado.<br>\n",
    " (b) O modelo gerador produz a perda utilizada para treinar a rede como um todo. O aprendizado é não-supervisionado pelo fato de que os dados do gerador são obtidos a partir de um vetor amostrado a partir de uma distribuição aleatória.<br>\n",
    " <b>(c) O modelo discriminador (classificador) produz as perdas utilizadas para treinar a rede em associação com o gerador. Os rótulos são calculados não necessitando de anotação manual, sendo portanto não supervisionado.<br></b>\n",
    " (d) O modelo discriminador produz a perda utilizada para treinar a rede como um todo. O aprendizado é semi-supervisionado já que sabemos o rótulo das classes positivas.<br>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Carregue a base de dados `smarphone_activity_dataset.csv`, conforme abaixo, com uma divisão hold-out utilizando os 80% exemplos iniciais para treinamento e os restantes para teste.\n",
    "\n",
    "Projete um Autoencoder para produzir uma projeção em 2 dimensões, com as seguintes camadas:\n",
    "* Entrada (com as dimensões da base de dados)\n",
    "* Dropout de 0.3\n",
    "* Camada densa de 2 neurônios (código) e ativação linear\n",
    "* Camada densa de saída (com as dimensões da base de dados) e ativação tangente hiperbólica\n",
    "\n",
    "Inicialize as sementes `seed(1)` e `set_seed(2)` antes de instanciar o modelo, compilar e treinar.\n",
    "\n",
    "Utilize a função de custo mean absolute error (mae), otimizador Adam com taxa 0.01, batchsize 16 e treine por 30 épocas.\n",
    "\n",
    "Após o treinamento, obtenha um scatterplot do código de treinamento, e analise visualmente a distribuição das classes utilizando os 500 primeiros exemplos `:500`. Podemos identificar grupos com quais misturas de classes?\n",
    "\n",
    "(a) 2 grupos, classes: (1, 3 e 6); (2, 4 e 5) <br>\n",
    "<b>(b) 3 grupos, classes: (1 e 3); (2); (4, 5 e 6) <br></b>\n",
    "(c) 4 grupos, classes: (1 e 3); (2); (4 e 5); (6)<br>\n",
    "(d) 4 grupos, classes: (1 e 2); (3); (4); (5 e 6) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rif40G6wST-s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras import models\n",
    "from numpy.random import seed\n",
    "#from tensorflow.random import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"smartphone_activity_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotulos = np.array(df['activity'])\n",
    "features = np.array(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_train = 0.8\n",
    "\n",
    "n_train = int(features.shape[0]*perc_train)\n",
    "n_test = int(features.shape[0]*(1-perc_train))\n",
    "print(n_train)\n",
    "print(n_test)\n",
    "\n",
    "x_train = features[:n_train,:]\n",
    "y_train = rotulos[:n_train]\n",
    "\n",
    "x_test = features[n_train:,:]\n",
    "y_test = rotulos[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defina autoencoder\n",
    "\n",
    "input_data = tf.keras.layers.Input(shape=(x_train.shape[1],))\n",
    "\n",
    "x = keras.layers.Dropout(0.3)(input_data)\n",
    "x = keras.layers.Dense(2, activation='linear')(x)\n",
    "output = keras.layers.Dense(x_train.shape[1], activation='tanh')(x)\n",
    "\n",
    "autoencoder = keras.models.Model(input_data, output)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sementes\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "#epochs e batchsize\n",
    "epochs = 30\n",
    "batchsize = 16\n",
    "\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "                    loss='mae',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "hist_autoencoder = autoencoder.fit(x_train, x_train, batch_size=batchsize, epochs=epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub = x_train[:500]\n",
    "\n",
    "code = autoencoder.predict(x_sub)\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "plt.scatter(code[:,0],code[:,1],alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x_sub = x_train[:500,:]\n",
    "extract = keras.models.Model(autoencoder.inputs, autoencoder.layers[-2].output)\n",
    "code = extract.predict(x_sub)\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "sns.scatterplot(x=code[:,0],y=code[:,1],alpha=0.5,hue=y_train[:500], palette='tab10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifQGbqS05Rts"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Ainda utilizando a base de dados `smarphone_activity_dataset.csv` com a mesma divisão entre treinamento e teste, projete um Autoencoder profundo para produzir uma projeção em 50 dimensões, com as seguintes camadas:\n",
    "* Entrada (com as dimensões da base de dados)\n",
    "* 2 Camadas densas com 128 neurônios, ativação tanh\n",
    "* Dropout de 0.25\n",
    "* 1 Camada densa com 50 neurônios, ativação tahn, name='code'\n",
    "* 2 Camadas densas com 128 neurônios, ativação tanh\n",
    "* Camada densa de saída (com as dimensões da base de dados), ativação tanh\n",
    "\n",
    "Inicialize as sementes `seed(1)` e `set_seed(2)` antes de instanciar o modelo, compilar e treinar.\n",
    "\n",
    "Utilize a função de custo mean squared error (mse), otimizador Adam com taxa 0.001, batchsize 16 e treine por 50 épocas.\n",
    "\n",
    "Após o treinamento, calcule o valor final da função de custo para os dados de treinamento. Obtenha o código de 50 dimensões para o conjunto de treinamento e de teste. Treine um classificador svm (utilizando o pacote `SVC` do `sklearn`), utilizando parâmetro C=0.5 e `random_state=1`, utilizando o código de treinamento, e calcule a acurácia no código do conjunto de teste.\n",
    "\n",
    "Os valores observados de MSE de treinamento, e acurácia de classificação SVM no teste estão no intervalo:\n",
    "\n",
    "(a) MSE =[0.10, 0.25]; Acurácia = [0.70, 0.75] <br>\n",
    "(b) MSE =[0.10, 0.25]; Acurácia = [0.75, 0.77] <br>\n",
    "(c) MSE =[0.01, 0.05]; Acurácia = [0.80, 0.86] <br>\n",
    "<b>(d) MSE =[0.00, 0.04]; Acurácia = [0.88, 0.95] <br></b>\n",
    "\n",
    " **Justificativa**: Ver código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## projete e execute autoencoder\n",
    "input_data = tf.keras.layers.Input(shape=(x_train.shape[1],))\n",
    "x = keras.layers.Dense(128,activation='tanh')(input_data)\n",
    "x = keras.layers.Dense(128,activation='tanh')(x)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.Dense(50, activation='tanh',name='code')(x)\n",
    "x = keras.layers.Dense(128, activation='tanh')(x)\n",
    "x = keras.layers.Dense(128, activation='tanh')(x)\n",
    "output = keras.layers.Dense(x_train.shape[1],activation='tanh')(x)\n",
    "\n",
    "autoencoder = keras.models.Model(input_data, output)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exiba perda de treinamento\n",
    "#sementes\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "#epochs e batchsize\n",
    "epochs = 50\n",
    "batchsize = 16\n",
    "\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                    loss='mse',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "hist_autoencoder = autoencoder.fit(x_train, x_train, batch_size=batchsize, epochs=epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtenha códigos\n",
    "code_model = keras.models.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('dense_14').output)\n",
    "code_train = np.asarray(code_model.predict(x_train))\n",
    "code_test  = np.asarray(code_model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## treine e teste classificador\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(C = 0.5, random_state=1)\n",
    "model.fit(code_train, y_train)\n",
    "\n",
    "\n",
    "score = model.score(code_test,y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
