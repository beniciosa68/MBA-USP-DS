{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #tratamentos numéricos\n",
    "import pandas as pd #dataframes\n",
    "import seaborn as sns #distribuições\n",
    "import matplotlib.pyplot as plt #gráficos\n",
    "from sklearn.metrics import r2_score #calculo do r2\n",
    "from sklearn.decomposition import PCA #pca\n",
    "from statsmodels.formula.api import ols #regressão linear\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.svm import SVC #SVM\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import mean_squared_error #erro quadrático médio\n",
    "from sklearn.preprocessing import StandardScaler #padronização\n",
    "from sklearn.linear_model import LinearRegression #regressão linear\n",
    "from sklearn.neighbors import KNeighborsClassifier #k-vizinhos\n",
    "from sklearn.model_selection import cross_val_score #validação cruzada\n",
    "from sklearn.model_selection import StratifiedKFold #k-vizinhos\n",
    "from sklearn.model_selection import train_test_split #dividir base em treino e teste\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sgemm_product.csv',sep=',')\n",
    "#df1 = pd.read_csv('sgemm_product.csv',sep=',')\n",
    "#df = df1.sample(frac=0.4)\n",
    "print('Número de linhas e colunas:',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Significado dos dados da base\n",
    "    - Input:\n",
    "        MWG, NWG: per-matrix 2D tiling at workgroup level: {16, 32, 64, 128} (integer)\n",
    "        KWG: inner dimension of 2D tiling at workgroup level: {16, 32} (integer)\n",
    "        MDIMC, NDIMC: local workgroup size: {8, 16, 32} (integer)\n",
    "        MDIMA, NDIMB: local memory shape: {8, 16, 32} (integer)\n",
    "        KWI: kernel loop unrolling factor: {2, 8} (integer)\n",
    "        VWM, VWN: per-matrix vector widths for loading and storing: {1, 2, 4, 8} (integer)\n",
    "        STRM, STRN: enable stride for accessing off-chip memory within a single thread: {0, 1} (categorical)\n",
    "        SA, SB: per-matrix manual caching of the 2D workgroup tile: {0, 1} (categorical)\n",
    "    \n",
    "    - Output:\n",
    "        Run1, Run2, Run3, Run4: performance times in milliseconds for 4 independent runs using the same parameters.\n",
    "<img src='./img/img-SGEMM-GPU.gif'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descrição estatística\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento e transformação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando se temos duplicidade dos dados\n",
    "df[df.duplicated() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando se temos dados faltantes\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando outliers dos tempos de resposta\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df.iloc[:,-4:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detectando e removendo outliers\n",
    "def detect_remove_outlier(data):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    mask = ((data > (Q1 - 1.5 * IQR)) & (data < (Q3 + 1.5 * IQR)))\n",
    "    data_clean = data[mask]\n",
    "    return data_clean\n",
    "\n",
    "df_clean = detect_remove_outlier(df)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando dados faltantes obtidos pela função de outlier\n",
    "df_clean.isnull().sum().sort_values(ascending=False).head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropando as linhas com NaN\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean = df_clean.reset_index()\n",
    "print('Tamanho da base sem outliers:',df_clean.shape)\n",
    "df_clean.isnull().sum().sort_values(ascending=False).head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando boxplot dos tempos de resposta após tratamento de outliers\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df_clean.iloc[:,-4:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados correlacionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz de correlação\n",
    "plt.figure(figsize=(16,5))\n",
    "\n",
    "plt.imshow(df_clean.corr(), cmap='Reds', interpolation='none', aspect='auto')\n",
    "plt.xticks(range(len(df_clean.corr())), df_clean.corr().columns, rotation='vertical')\n",
    "plt.yticks(range(len(df_clean.corr())), df_clean.corr().columns)\n",
    "plt.suptitle('Matriz de Correlação', fontsize=15, fontweight='bold')\n",
    "plt.grid(False)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = 0.30 # correlação mínima de 30%\n",
    "#var = []\n",
    "#for i in df_clean.corr().iloc[:,0:15]:\n",
    "#    for j in df_clean.corr().iloc[:,0:15]:\n",
    "#        if(i != j):\n",
    "#            if np.abs(df_clean.corr()[i][j]) > p: # se maior do que |p|\n",
    "#                var.append([i,j])\n",
    "#print('Variáveis mais correlacionadas:\\n', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um dataframe temporario\n",
    "#df_clean_temp = pd.DataFrame(df_clean)\n",
    "\n",
    "#dropando colunas menos correlacionaveis:\n",
    "#df_clean_temp.drop(df_clean.iloc[:,2:8],axis = 1, inplace = True)\n",
    "#df_clean_temp.drop(df_clean.iloc[:,10:14],axis = 1, inplace = True)\n",
    "#df_clean_temp.drop(df_clean.iloc[:,-3:],axis = 1, inplace = True)\n",
    "\n",
    "#plotando correlação entre algumas variáveis que mais interessaram \n",
    "#dado o heatmap apresentado  e as variáveis mais correlacionáveis:\n",
    "#fig = sns.pairplot(df_clean_temp,diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados distribuídos e balanceados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando distribuição dos tempos de resposta\n",
    "plt.figure(figsize=(16,5))\n",
    "#fig = sns.pairplot(df_clean.iloc[:,-4:])[0]\n",
    "#fig = df_clean.iloc[:,-4:].hist(bins=100)\n",
    "fig = sns.distplot(df_clean.iloc[:,-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando estatísticas dos atributos alvo das 4 execuções\n",
    "#df_clean = df_clean.reset_index()\n",
    "df_clean.iloc[:,-4:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean.drop(df_clean.iloc[:,-1:],axis = 1, inplace = True)\n",
    "#df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidando os atributos de predição em um único apenas, considerando suas médias\n",
    "run = df_clean.iloc[:,-4:].mean(axis=1)\n",
    "\n",
    "df_clean['Run (ms)'] = pd.Series(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_clean.copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "fig = sns.distplot(data.iloc[:,-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando log na variável alvo\n",
    "run_log = np.log(data['Run (ms)'])\n",
    "data.insert(20,'Run Log (ms)',run_log)\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "fig = sns.distplot(data.iloc[:,-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caso queira apagar a última coluna para dar rollback nos dados\n",
    "#data.drop(data.iloc[:,-1:],axis = 1, inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando discretização dos dados no tempo de resposta\n",
    "#threshold = [0,100,200,300,400,500,600]\n",
    "threshold = [3,4,5,6]\n",
    "#discretização \n",
    "#run_disc = pd.cut(data['Run (ms)'], bins=threshold)\n",
    "run_disc = pd.cut(data['Run Log (ms)'], bins=threshold)\n",
    "\n",
    "data.insert(21,'Run Discretizado (ms)',run_disc)\n",
    "\n",
    "#realizando o mapping numbers\n",
    "data['Run Discretizado Mapping (ms)'] = data['Run Discretizado (ms)'].astype('category').cat.codes\n",
    "\n",
    "data['Run Discretizado Mapping (ms)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incluindo na base de dados uma transformação dos valores para torná-los um problema de classificação binária\n",
    "\n",
    "#avg = data['Run (ms)'].mean()\n",
    "#data.loc[data['Run (ms)'] <= avg, 'Run Bin'] = 0\n",
    "#data.loc[data['Run (ms)'] >  avg, 'Run Bin'] = 1\n",
    "\n",
    "avg = data['Run Log (ms)'].mean()\n",
    "data.loc[data['Run Log (ms)'] <= avg, 'Run Bin'] = 0\n",
    "data.loc[data['Run Log (ms)'] >  avg, 'Run Bin'] = 1\n",
    "\n",
    "run_bin = data[['Run Bin']].values.astype('int')\n",
    "data.drop(data.iloc[:,-1:],axis = 1, inplace = True)\n",
    "\n",
    "data.insert(22,'Run Binario',run_bin)\n",
    "\n",
    "print('Se menor ou igual que ',avg,'então será 0')\n",
    "print('Se maior que ',avg,'então será 1')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando balanceamento entre as variáveis categóricas (multiclasse)\n",
    "fig1 = data['Run Discretizado (ms)'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando balanceamento entre as variáveis categóricas mapping (multiclasse)\n",
    "fig = data['Run Discretizado Mapping (ms)'].value_counts().plot(kind='bar')\n",
    "\n",
    "mask0 = (data['Run Discretizado Mapping (ms)'] == -1)\n",
    "#mask1 = (data['Run Discretizado Mapping (ms)'] == 1)\n",
    "#mask2 = (data['Run Discretizado Mapping (ms)'] == 2)\n",
    "mask3 = (data['Run Discretizado Mapping (ms)'] == 1)\n",
    "mask4 = (data['Run Discretizado Mapping (ms)'] == 0)\n",
    "mask5 = (data['Run Discretizado Mapping (ms)'] == 2)\n",
    "#mask6 = (data['Run Discretizado Mapping (ms)'] == 6)\n",
    "\n",
    "print('Distribuição das classes:')\n",
    "print(\"[000,100]: %.2f\" % (len(data[mask0].index.values)/len(data['Run Discretizado Mapping (ms)']) * 100),'%')\n",
    "#print(\"[100,200]: %.2f\" % (len(data[mask1].index.values)/len(data['Run Discretizado Mapping (ms)']) * 100),'%')\n",
    "#print(\"[200,300]: %.2f\" % (len(data[mask2].index.values)/len(data['Run Discretizado Mapping (ms)']) * 100),'%')\n",
    "print(\"[300,400]: %.2f\" % (len(data[mask3].index.values)/len(data['Run Discretizado Mapping (ms)']) * 100),' %')\n",
    "print(\"[400,500]: %.2f\" % (len(data[mask4].index.values)/len(data['Run Discretizado Mapping (ms)']) * 100),' %')\n",
    "print(\"[500,600]: %.2f\" % (len(data[mask5].index.values)/len(data['Run Discretizado Mapping (ms)']) * 100),' %')\n",
    "#print(\"[500,600]: %.2f\" % (len(data[mask6].index.values)/len(data['Run Discretizado Mapping (ms)']) * 100),' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando balanceamento entre as variáveis binárias\n",
    "fig = data['Run Binario'].value_counts().plot(kind='bar')\n",
    "\n",
    "mask0 = (data['Run Binario'] == 0)\n",
    "mask1 = (data['Run Binario'] == 1)\n",
    "\n",
    "print('Distribuição das classes:')\n",
    "print(\" <= Média: %.2f\" % (len(data[mask0].index.values)/len(data['Run Binario']) * 100),'%')\n",
    "print(\" >  Média: %.2f\" % (len(data[mask1].index.values)/len(data['Run Binario']) * 100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separando os dados em conjunto de treinamento e testes, na proporção 80-20\n",
    "perc = 0.9\n",
    "\n",
    "#salvando y como serie classificação multiclasse\n",
    "x_train_m, y_train_m = data.iloc[0:int(len(data)*perc),1:-9],data['Run Discretizado (ms)'].iloc[0:int(len(data)*perc)]\n",
    "x_test_m, y_test_m = data.iloc[int(len(data)*perc):int(len(data)),1:-9],data['Run Discretizado (ms)'].iloc[int(len(data)*perc):int(len(data))]\n",
    "\n",
    "#salvando y como serie classificação multiclasse mapping number\n",
    "x_train_map, y_train_map = data.iloc[0:int(len(data)*perc),1:-9],data['Run Discretizado Mapping (ms)'].iloc[0:int(len(data)*perc)]\n",
    "x_test_map, y_test_map = data.iloc[int(len(data)*perc):int(len(data)),1:-9],data['Run Discretizado Mapping (ms)'].iloc[int(len(data)*perc):int(len(data))]\n",
    "\n",
    "#salvando y como dataframe (incluir [[]]) classificação multiclasse\n",
    "#x_train, y_train = data.iloc[0:int(len(data)*0.8),1:-7],data[['Run Discretizado (ms)']].iloc[0:int(len(data)*0.8)]\n",
    "#x_test, y_test = data.iloc[int(len(data)*0.8):int(len(data)),1:-7],data[['Run Discretizado (ms)']].iloc[int(len(data)*0.8):int(len(data))]\n",
    "\n",
    "#salvando y como classificação binário\n",
    "x_train_b, y_train_b = data.iloc[0:int(len(data)*perc),1:-9],data['Run Binario'].iloc[0:int(len(data)*perc)]\n",
    "x_test_b, y_test_b = data.iloc[int(len(data)*perc):int(len(data)),1:-9],data['Run Binario'].iloc[int(len(data)*perc):int(len(data))]\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_m = sc.fit_transform(x_train_m)\n",
    "x_test_m = sc.fit_transform(x_test_m)\n",
    "\n",
    "x_train_map = sc.fit_transform(x_train_map)\n",
    "x_test_map = sc.fit_transform(x_test_map)\n",
    "\n",
    "x_train_b = sc.fit_transform(x_train_b)\n",
    "x_test_b = sc.fit_transform(x_test_b)\n",
    "\n",
    "print('Tamanho da amostra de treinamento:',len(x_train_b))\n",
    "print('Tamanho da amostra de teste:',len(x_test_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.3, random_state = 0)\n",
    "#treinando um classificador\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(x_train_map,y_train_map)\n",
    "ZY_ = clf.predict(x_test_map)\n",
    "\n",
    "\n",
    "disp = plot_confusion_matrix(clf,x_test_map,y_test_map,cmap=plt.cm.Blues,normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM\n",
    "print('Linear Model',end='\\n')\n",
    "lsvclassifier = SVC(kernel='linear')\n",
    "lsvclassifier.fit(x_train_b, y_train_b)\n",
    "\n",
    "#Applying k-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = lsvclassifier, X = x_train_b, y = y_train_b, cv = 5)\n",
    "mean_svm_linear=accuracies.mean()\n",
    "std_svm_linear=accuracies.std()\n",
    "\n",
    "#After using 5 fold cross validation\n",
    "print('After 5 fold cross validation:')\n",
    "print('Mean of Accuracies: ',mean_svm_linear*100,end='\\n')\n",
    "print('Standard deviation of Accuracies',std_svm_linear*100,end='\\n')\n",
    "\n",
    "#Predict SVM\n",
    "y_predl = lsvclassifier.predict(x_test_b)\n",
    "\n",
    "#Confusion Matrix\n",
    "print('Test Output:')\n",
    "print('Confusion Matrix:')\n",
    "print(sk.confusion_matrix(y_test_b,y_predl))\n",
    "print('Classification Report:')\n",
    "print(sk.classification_report(y_test_b,y_predl))\n",
    "print('Accuracy: ',sk.accuracy_score(y_test_b, y_predl, normalize=True, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando PCA e plotando dados com conjunto original de dados de treino (multiclasse)\n",
    "pca = PCA(n_components=6, random_state=1)\n",
    "pca_result = pca.fit_transform(x_train_m)\n",
    "\n",
    "#realizando o plot\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],alpha='auto', hue=y_train_m, palette='tab10')\n",
    "plt.title('Scatterplot com projeção PCA do conjunto de dados multiclasse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando PCA e plotando dados com conjunto original de dados de treino (multiclasse mapping number)\n",
    "pca = PCA(n_components=6, random_state=1)\n",
    "pca_result = pca.fit_transform(x_train_map)\n",
    "\n",
    "#realizando o plot\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],alpha='auto', hue=y_train_map, palette='tab10')\n",
    "plt.title('Scatterplot com projeção PCA do conjunto de dados multiclasse mapping number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando PCA e plotando dados com conjunto original de dados de treino (binário)\n",
    "pca = PCA(n_components=2, random_state=1)\n",
    "pca_result = pca.fit_transform(x_train_b)\n",
    "\n",
    "#realizando o plot\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],alpha='auto', hue=y_train_b, palette='prism')\n",
    "plt.title('Scatterplot com projeção PCA do conjunto de dados binário')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#01.K-vizinhos com classificação binária\n",
    "\n",
    "#cv_knn = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#mauc_knn = []\n",
    "#macc_knn = []\n",
    "#vk = []\n",
    "#for k in range(1, 21):\n",
    "#    vauc_knn = []\n",
    "#    vacc_knn = []\n",
    "#    for train_index, test_index in cv_knn.split(x_train_b, y_train_b):\n",
    "#        x_train, x_test = x_train_b[train_index], x_train_b[test_index]\n",
    "#        y_train, y_test = y_train_b[train_index], y_train_b[test_index]\n",
    "#        model_knn = KNeighborsClassifier(n_neighbors=k, metric = 'euclidean')\n",
    "#        model_knn.fit(x_train,y_train)\n",
    "#        y_pred_knn = model_knn.predict(x_test) \n",
    "#        score = accuracy_score(y_pred_knn, y_test)\n",
    "#        vauc_knn.append(roc_auc_score(y_test, y_pred_knn))\n",
    "#        vacc_knn.append(accuracy_score(y_pred_knn, y_test))\n",
    "#    macc_knn.append(np.mean(vacc_knn))\n",
    "#    mauc_knn.append(np.mean(vauc_knn))\n",
    "#    vk.append(k)\n",
    "    \n",
    "#best_k = np.argmax(mauc_knn)+1\n",
    "#print('Melhor k:', best_k, ' AUC:',mauc_knn[best_k-1])\n",
    "#plt.figure(figsize=(10,5))\n",
    "#plt.plot(vk, mauc_knn, '-ro', label= 'AUC')\n",
    "#plt.plot(vk, macc_knn, '-bo', label = 'Accuracy')\n",
    "#plt.xlabel('k', fontsize = 15)\n",
    "#plt.ylabel('Score', fontsize = 15)\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='.\\img\\melhor-k.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-vizinhos, aplicando o modelo com o melhor k\n",
    "\n",
    "model_KNN = KNeighborsClassifier(n_neighbors = 3, metric='euclidean')\n",
    "model_KNN.fit(x_train_b,y_train_b)\n",
    "y_pred_KNN = model_KNN.predict(x_test_b)\n",
    "score_KNN = accuracy_score(y_pred_KNN, y_test_b)\n",
    "\n",
    "print('+--------------Kvizinhos--------------+')\n",
    "print('| Accuracy: ', '%.2f' % accuracy_score(y_pred_KNN, y_test_b),'                    |')\n",
    "print('| F1 score: ', '%.2f' % f1_score(y_test_b, y_pred_KNN, average='weighted', labels=np.unique(y_pred_KNN)),'                    |')\n",
    "print('| Precision:', '%.2f' % precision_score(y_test_b, y_pred_KNN, average='weighted', labels=np.unique(y_pred_KNN)),'                    |')\n",
    "print('| Recall:   ', '%.2f' % recall_score(y_test_b, y_pred_KNN, average='weighted', labels=np.unique(y_pred_KNN)),'                    |')\n",
    "print('+-------------------------------------+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-vizinhos, aplicando o modelo com o melhor k para multiclasse com mapping number\n",
    "\n",
    "model_KNN = KNeighborsClassifier(n_neighbors = 3, metric='euclidean')\n",
    "model_KNN.fit(x_train_map,y_train_map)\n",
    "y_pred_KNN = model_KNN.predict(x_test_map)\n",
    "score_KNN = accuracy_score(y_pred_KNN, y_test_map)\n",
    "\n",
    "print('+--------------Kvizinhos--------------+')\n",
    "print('| Accuracy: ', '%.2f' % accuracy_score(y_pred_KNN, y_test_map),'                    |')\n",
    "print('| F1 score: ', '%.2f' % f1_score(y_test_map, y_pred_KNN, average='weighted', labels=np.unique(y_pred_KNN)),'                    |')\n",
    "print('| Precision:', '%.2f' % precision_score(y_test_map, y_pred_KNN, average='weighted', labels=np.unique(y_pred_KNN)),'                    |')\n",
    "print('| Recall:   ', '%.2f' % recall_score(y_test_map, y_pred_KNN, average='weighted', labels=np.unique(y_pred_KNN)),'                    |')\n",
    "print('+-------------------------------------+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando SVM\n",
    "\n",
    "#com ponderação (pegando como base a distribuição de classes encontradas anteriormente):\n",
    "peso_0 = 0.01\n",
    "peso_1 = 0.05\n",
    "peso_2 = 0.04\n",
    "peso_3 = 0.14\n",
    "peso_4 = 0.16\n",
    "peso_5 = 0.60\n",
    "class_weight = {0: peso_0, 1: peso_1, 2: peso_2, 3: peso_3, 4: peso_4, 5: peso_5}\n",
    "\n",
    "model_SVM = SVC(kernel='linear', C = 0.5, \n",
    "            random_state=1,\n",
    "            class_weight = class_weight)\n",
    "\n",
    "model_SVM.fit(x_train_map,y_train_map)\n",
    "y_pred_SVM = model_SVM.predict(x_test_map)\n",
    "\n",
    "score_SVM = accuracy_score(y_pred_SVM, y_test_map)\n",
    "\n",
    "print('+-----------------SVM-----------------+')\n",
    "print('| Accuracy: ', '%.2f' % accuracy_score(y_pred_SVM, y_test_map),'                    |')\n",
    "print('| F1 score: ', '%.2f' % f1_score(y_test_map, y_pred_SVM, average='weighted', labels=np.unique(y_pred_SVM)),'                    |')\n",
    "print('| Precision:', '%.2f' % precision_score(y_test_map, y_pred_SVM, average='weighted', labels=np.unique(y_pred_SVM)),'                    |')\n",
    "print('| Recall:   ', '%.2f' % recall_score(y_test_map, y_pred_SVM, average='weighted', labels=np.unique(y_pred_SVM)),'                    |')\n",
    "print('+-------------------------------------+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando SVM\n",
    "\n",
    "#com ponderação (pegando como base a distribuição de classes encontradas anteriormente):\n",
    "peso_0 = 0.4\n",
    "peso_1 = 0.6\n",
    "class_weight = {0: peso_0, 1: peso_1}\n",
    "\n",
    "model_SVM = SVC(kernel='linear', C = 0.5, \n",
    "            random_state=1,\n",
    "            class_weight = class_weight)\n",
    "\n",
    "model_SVM.fit(x_train_b,y_train_b)\n",
    "y_pred_SVM = model_SVM.predict(x_test_b)\n",
    "\n",
    "score_SVM = accuracy_score(y_pred_SVM, y_test_b)\n",
    "\n",
    "print('+-----------------SVM-----------------+')\n",
    "print('| Accuracy: ', '%.2f' % accuracy_score(y_pred_SVM, y_test_b),'                    |')\n",
    "print('| F1 score: ', '%.2f' % f1_score(y_test_b, y_pred_SVM, average='weighted', labels=np.unique(y_pred_SVM)),'                    |')\n",
    "print('| Precision:', '%.2f' % precision_score(y_test_b, y_pred_SVM, average='weighted', labels=np.unique(y_pred_SVM)),'                    |')\n",
    "print('| Recall:   ', '%.2f' % recall_score(y_test_b, y_pred_SVM, average='weighted', labels=np.unique(y_pred_SVM)),'                    |')\n",
    "print('+-------------------------------------+')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Métodos de Predição e Classificação com Ciência de Dados\n",
    "\n",
    "Tratamento:\n",
    " Técnicas Avançadas de Captura\n",
    "    Aula 2 - Desbalanceamento SMOTE e data augmentation, outliers\n",
    "    Aula 3 - Discretização, Normalização, Transformação, Codificação Variáveis Categóricas\n",
    "     \n",
    "Agrupamento:\n",
    " Intro Ciencia de Dados\n",
    "    Aula 4 - k-means, agrupamento hierarquico, avaliando agrupando\n",
    " \n",
    "  Aprendizado de Máquina:\n",
    "    Aula 8 - Agrupamento e algoritmos\n",
    "    \n",
    "Classificação, Regressão e Predição:\n",
    " Intro Ciencia de Dados\n",
    "    Aula 6 - KNN, Regressão Logistica, Naive Bayes\n",
    "    Aula 7 - Árvores e Esembles\n",
    "    Aula 8 - SVM  e Avaliando Classificadores\n",
    "\n",
    " Aprendizado de Máquina:\n",
    "    Aula 5 - Regressão Logística, Teorema de Bayes, Naive Bayes\n",
    "    Aula 7 - SVM, Multiclasses, Boosting, Viés e Variancia (Bias-variance tradeoff), Ensembles árvores \n",
    "\n",
    " Técnicas Avançadas de Captura\n",
    "    Aula 4 - PCA (para redução de dimensionalidade)\n",
    " \n",
    " Estatistica\n",
    "    Aula 7 - Regressão linear, inferência\n",
    "    Aula 8 - Regressão padronizada, inferência bayesiana, estimação bayesiana\n",
    "    \n",
    " Redes Neurais e Deep-Learning\n",
    "    Aula 2 - CNN (Redes Neurais Convolucionais)\n",
    "    Aula 3 - Parametrizações de CNN\n",
    "    Aula 4 - Transferência de aprendizado\n",
    "    Aula 5 - Autoencoder\n",
    "    Aula 6 - LSTM e GRU, Rede Transformer\n",
    "    Aula 8 - Redes de regressão e classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idéias de predição na massa de dados\n",
    "\n",
    "#Balanceamento: verificar SMOTE\n",
    "\n",
    "# - Regressão Linear Simples \n",
    "# - Regressão Logistica\n",
    "\n",
    "#?????? objetivo 1: estimar desempenho\n",
    "# - Classificadores para estimar desempenho (defino uma classificação com faixas de tempos e aplico os classificadores)\n",
    "#   - Classificação Binaria e Multiclasse\n",
    "#    - Defino classes com as faixas de desempenho que eu quero\n",
    "#    - Faço leave-one-out pra separar a base em treino e teste\n",
    "#    - Aplico os modelos de classificadores de machine learning\n",
    "#      - Redes neurais perceptron\n",
    "#      - SVM\n",
    "#      - KNN\n",
    "#      - NNGE\n",
    "#      - PCA (otimizar atrinbutos)\n",
    "#      - Random Forest\n",
    "#      - ResNet\n",
    "#      - Inception\n",
    "#    - Aplico na base de teste\n",
    "#    - Comparo os modelos e acurácias\n",
    "\n",
    "#regressão linear, ML gradient descent, árvore de decisão, redes neurais perceptron e k-vizinhos.\n",
    "\n",
    "# objetivo secundario: redução de dimensionalidade\n",
    "# tecnicas de redução do numero de atributos (rodar ML com melhores atributos e melhorar a acuracia do modelo)\n",
    "\n",
    "# objetivo secundario: otimização do algoritmo\n",
    "# descobrir os melhores parametros que minimizam o tempo de execução (melhor desempenho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressão linear simples com scikit-learn\n",
    "\n",
    "y_regression = df_clean['Run1 (ms)']\n",
    "x_regression = df_clean.drop(['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)','Run (ms)'],axis = 1, inplace=False)\n",
    "#print(x_regression)\n",
    "#y_regression = data['Run (ms)']\n",
    "#x_regression2 = data.drop(['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)','Run (ms)', 'Run Discretizado (ms)'],axis = 1, inplace=False)\n",
    "#print(x_regression2)\n",
    "y_regression_np = y_regression.to_numpy()\n",
    "x_regression_np = x_regression.to_numpy()\n",
    "\n",
    "p = 0.2\n",
    "x_regression_train, x_regression_test, y_regression_train, y_regression_test = train_test_split(x_regression_np, y_regression_np, test_size = p, random_state = 42)\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(x_regression_train, y_regression_train)\n",
    "y_pred_lm = lm.predict(x_regression_test)\n",
    "\n",
    "l = plt.plot(y_pred_lm, y_regression_test, 'co')\n",
    "plt.setp(l, markersize=10)\n",
    "plt.setp(l, markerfacecolor='C0')\n",
    "\n",
    "plt.ylabel(\"y\", fontsize=15)\n",
    "plt.xlabel(\"Prediction\", fontsize=15)\n",
    "\n",
    "# mostra os valores preditos e originais\n",
    "xl = np.arange(min(y_regression_test), 1.2*max(y_regression_test),(max(y_regression_test)-min(y_regression_test))/10)\n",
    "yl = xl\n",
    "plt.plot(xl, yl, 'r--')\n",
    "\n",
    "#calculando o R2 score\n",
    "R2 = r2_score(y_regression_test, y_pred_lm)\n",
    "print('R2:', '%.2f' % R2)\n",
    "#calculando o erro quadrático médio para a regressão linear múltipla\n",
    "RSME = mean_squared_error(y_regression_test, y_pred_lm)\n",
    "print(\"RSME:\", '%.2f' % RSME)\n",
    "\n",
    "plt.show(True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
