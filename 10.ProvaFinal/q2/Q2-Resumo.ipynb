{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientações - Prof. Louzada\n",
    "\n",
    "2. Uma questão será relacionada com as disciplinas “Programação para Ciência de Dados” e “Técnicas Avançadas de Captura e Tratamento de Dados”. Essa questão envolve a leitura de um arquivo no formato .CSV de modo a identificar e tratar dados faltantes e outliers. Ao concluir a questão e assinalar a resposta que julgue correta no Moodle, os seguintes passos devem ser realizados para concluir a submissão da resposta: \n",
    "   2.1) Exportar o notebook que foi utilizado para resolver a questão da prova em formato .py e fazer upload no Moodle. Atenção: não deve ser feito upload de um arquivo notebook (.ipynb), mas sim de um arquivo texto .py contendo os códigos python que foram utilizados para resolver às questões. O arquivo .py pode ser gerado realizando-se as seguintes ações: File --> Download as --> Python (.py) disponível no Jupyter Notebook. ou File --> Download .py no Google Colab.Caso a resolução da questão não tenha sido feita usando Jupyter, o código desenvolvido para responder à questão deve ser enviado em um arquivo ASCII (Texto) salvo na extensão .py\n",
    "   2.2) O arquivo deve ser nomeado com o nome e o sobrenome do aluno, SEM ESPAÇO entre as partes de seu nome. Exemplo: moacirponti.py\n",
    "   2.3) É OBRIGATÓRIO conter no cabeçalho (início) do arquivo um comentário com o seu nome completo. Por exemplo, #Moacir Ponti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 07 - Programacao Ciencia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dados faltantes\n",
    "Dados faltantes são muito comuns em aplicações reais, ou seja, valores dos elementos não são fornecidos para algumas entradas da série ou DataFrame. Dados faltantes são representados pelo <font color='blue'>pandas</font> com o símbolo 'NaN'.\n",
    "\n",
    "Pandas fornece diversas funcionalidades para lidar com dados faltantes, por exemplo:\n",
    "- detectar dados faltantes com os métodos <font color='blue'>isnull</font> e <font color='blue'>notnull</font>\n",
    "- remover linhas que contenham dados faltantes utilizando o método <font color='blue'>dropna</font>\n",
    "   - linhas faltando qualquer elemento \n",
    "   - linhas faltando todos os dados\n",
    "   - linhas faltando um número específico de dados\n",
    "- Substituir os valores faltantes (NaN) por um valor específico utilizando o método  <font color='blue'>fillna</font>\n",
    "- Especificar valores para substituir dados faltantes em operações aritmétricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bibliotecas\n",
    "import pandas as pd\n",
    "\n",
    "# o método 'isnull' cria uma máscara booleana onde os valores True corresondem as entradas com dados faltantes\n",
    "df.isnull()\n",
    "# o método 'dropna' remove todas as linhas onde um dado faltante aparece\n",
    "df.dropna()\n",
    "# remove linhas onde todos os dados estejam faltando\n",
    "df.dropna(how='all')\n",
    "# remove colunas que contenham algum dado faltante\n",
    "df.dropna(how='any',axis=1)\n",
    "# remove linhas com 3 ou mais dados faltantes\n",
    "df.dropna(thresh=2)\n",
    "# Substitui valores faltantes por zero e guarda em um novo DataFrame. O data frame original não é afetado\n",
    "df1 = df.fillna(0)\n",
    "# substitui o valor faltante da coluna 1 por -1 da coluna 3 por 0 e da coluna 4 por 1\n",
    "df.fillna({1:-1,3:0,4:1})\n",
    "\n",
    "# detecta linhas duplicadas\n",
    "df.duplicated()\n",
    "# removerlinhas duplicadas\n",
    "df.drop_duplicates()\n",
    "\n",
    "# modificando os valores da coluna 'type' para que fiquem em letras maiusculas\n",
    "df['type'] = df['type'].map(lambda x: x.upper())\n",
    "\n",
    "#mapeando um dicionario para aplicar classificação em uma nova coluna no dataframe\n",
    "calories_by_type = {'Cheese':'high','Apple':'low','Bread':'high'}\n",
    "df['calories'] = df['food'].map(calories_by_type)\n",
    "\n",
    "# substitui valores no DataFrame todo utilizando duas listas, a primeira lista corresponde aos valores que serão modificados\n",
    "# a segunda lista contém os novos valores. No exemplo abaixo, 2 será substituido por -2 e 'cla' por 'clam'\n",
    "dfr.replace([2,'cla'],[-2,'clam'])\n",
    "#utilizando dicionario como parametro\n",
    "dfr.replace({2:-2,'cla':'clam'})\n",
    "#utilizando dicionário em colunas específicas\n",
    "dfr.replace({'c1':{4:-4,2:-2},'c3':{'ela':'eles'}})\n",
    "\n",
    "#renomeando rótulos de linhas ou colunas. Neste exemplo, o primeiro parametro renomeia os indices acrescentando um a na string. \n",
    "#No segundo converte o nome das colunas pra maiusculo\n",
    "df.rename(index=lambda x: str(x)+'a', columns=lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 03 e Prova Final - Introducao Ciencia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Amostragem de dados\n",
    "       No caso de termos muitos dados e pouco poder computacional, podemos processar uma parte simplesmente pegando uma amostra do todo, porém, quanto mais dados, maior tende ser a acurácia do modelo. O ideal é fazer um balanço entre a eficiência computacional e a acurácia dos dados.\n",
    "       Um dos problemas da amostragem é o balanceamento, ou seja, quando pegamos um dado, que só representa um tipo de informação, induzindo a análise para conclusões que podem não ser verdadeiras ou altamente influenciáveis.\n",
    "       \n",
    "       Amostragem aleatório simples:\n",
    "           Com reposição: Vou retirando amostras aleatórias sem me preocupar se os dados que saíram estão repetidos ou não.\n",
    "           Sem reposição: Vou retirando amostras aleatórias e me preocupando com os dados que já saíram, não repetindo.\n",
    "           \n",
    "       Amostragem estratificada:\n",
    "           Manter o mesmo número de objetos para cada classe de dados ou manter um número de dados proporcionais. Ex, temos uma amostra com 100 homens e 20 mulheres, então vamos tentar deixar ambos com uma quantidade igual de elementos, ou aumentando o número de mulheres ou diminuindo o número de homens. Algumas técnicas pra isso são acréscimo ou eliminação.\n",
    "           \n",
    "       Amostragem progressiva:\n",
    "           Vai processando uma amostra pequena e vai incluindo dados até a amostra ficar com uma acurácia preditiva pára de melhorar, então não precisamos mais continuar processando dados, uma vez que não teremos mais ganho com isso.\n",
    "       \n",
    "   \n",
    "   ##### Limpeza de dados\n",
    "       Ruídos: erros ou valores diferentes do esperado. Ex. Idade com valores negativos.\n",
    "       Inconsistências: valores que não combinam ou contradizem a lógica. Ex. pessoa de 1,95m de altura pesando 10Kg.\n",
    "       Redundâncias: objetos/atributos com mesmos valores. Ex. 2 entradas iguais, influenciando no algoritimo.\n",
    "       Dados incompletos: ausência de atributos.\n",
    "           Soluções:  Eliminar objetos com valores ausentes.\n",
    "                      Preencher manualmente valores faltantes.\n",
    "                      Utilizar algum método/heurística/algoritmo para prever/estimar valores faltantes automaticamente.\n",
    "                      \n",
    "   \n",
    "   ##### Transformação de valores\n",
    "       Quando os algoritimos tem dificuldades de usar os dados no seu formato original, então nesse caso, você realiza uma transformação, passando ele para um formato que seja inteligivel tanto para a análise quanto para o algoritimo de processamento. Por ex, one-hot encoding scheme:\n",
    "   \n",
    "   ###### One-Hot Enconding    \n",
    "   <img src=\"img/one-hot-encoding.jpg\" width=\"500\" height=\"400\">\n",
    "   \n",
    "\n",
    "    Normalização (MinMax Scaling):\n",
    "        Os dados serão ajustados de forma que o valor máximo será 1 e o valor mínimo será 0.\n",
    "        Desvantagem: Sensível a outliers.\n",
    " \n",
    " <img src=\"img/min-max-scaling.png\">        \n",
    " <img src=\"img/graf-min-max-scaling.png\" width=\"300\" height=\"200\">   \n",
    "        \n",
    "    Padronizaçao (Z-Score Normalization:)\n",
    "        Os dados serão ajustados de forma que a média será 0 e o desvio padrão será 1.\n",
    "        \n",
    "  <img src=\"img/z-normalization.gif\" width=\"150\" height=\"100\">    \n",
    "  <img src=\"img/graf-z-normalization.png\" width=\"500\" height=\"400\">\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemplo de padronização (converter os dados para terem média 0 e desvio padrão 1) - exemplo da prova de estatística\n",
    "#(variável menos média dividido pelo desvio padrão)\n",
    "Tamanho_p = (data.Tamanho-np.mean(data.Tamanho))/(np.std(data.Tamanho))\n",
    "\n",
    "#Exemplo de normalização (converter os valores no intervalo de 0 e 1)\n",
    "#(variavel menos minimo dividido pela amplitude que é p max menos o mínimo)\n",
    "def normalize(train):\n",
    "    d_max = np.max(train)\n",
    "    d_min = np.min(train)\n",
    "    return ((train - d_min) / (d_max-d_min))\n",
    "\n",
    "SxNorm = normalize(Sx)\n",
    "\n",
    "\n",
    "#Outra forma de normalização:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#converte dataframe para array\n",
    "X = np.array(data[data.columns[0:data.shape[1]-1]])\n",
    "# prepara a função para transformar os dados\n",
    "scaler = StandardScaler().fit(X)\n",
    "# realiza a padronização (média=0, variância = 1)\n",
    "rescaledX = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers:\n",
    "Podemos dizer que uma observação é um outlier se ao menos uma das variáveis está fora dos limites máximos do boxplot. Ou seja, se o valor é menor do que (Q1 - 1.5 * IQR) ou maior do que (Q3 + 1.5 * IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando uma função para detectar outliers usando o método do IQR, se encontrar outlier, substitui por um NaN\n",
    "def detect_outlier(data):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    mask = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)))\n",
    "    data[mask] = np.nan\n",
    "    return data\n",
    "\n",
    "#DICA:\n",
    "#Plotar os gráficos de outlier com boxplot\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df.iloc[:,-4:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 02 - Tecnicas Avancadas Ciencia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detectando outliers\n",
    "\n",
    "**Relembrando - outliers, pontos \"fora-da-curva\" ou pontos aberrantes** : exemplos ou instâncias que, dentre do espaco de possíveis valores, recaem num intervalo *fora* daquele relativo a maior parte dos exemplos de uma base de dados.\n",
    "\n",
    "Detectar outliers por meio de análise exploratória é útil para entender o comportamento da base de dados.\n",
    "\n",
    "Existem também métodos **estatísticos** e de **aprendizado de máquina** que auxiliam nesse processo e que podem facilitar essa análise, detectando *outliers* de forma automática.\n",
    "\n",
    "1. Dispersão: desvio padrão e intervalo interquartil\n",
    "2. Distribuição: Normal univaridada\n",
    "3. Agrupamento\n",
    "\n",
    "\n",
    "#### 1. Desvio padrão e amplitude inter-quartil (por dispersão)\n",
    "\n",
    "Para cada atributo, podemos estudar como os valores estão relacionados com a dispersão dos dados.\n",
    "\n",
    "Entre as medidas de dispersão temos:\n",
    "* desvio padrão (*standard deviation*)\n",
    "    Seja $\\mu$ a média de uma variável,\n",
    "    $$\\sigma = \\frac{\\sqrt{ \\sum_i (x_i - \\mu)^2}}{n}$$\n",
    "\n",
    "\n",
    "* amplitude - ou intervalo - interquartil (IQR, *interquartile range*)\n",
    "    Sejam:\n",
    "    - $Q_{1}$ o valor relativo aos primeiros 25% dados,\n",
    "    - $Q_{2}$ o valor relativo aos primeiros 50% dados (mediana),\n",
    "    - $Q_{3}$ o valor relativo aos primeiros 75% dos dados,\n",
    "    \n",
    "    $$IQR = Q_{3} - Q_{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando outliers e inliers por IQR\n",
    "\n",
    "# definindo o primeiro e terceiro interquartil (IQR)\n",
    "Q1 = data['total'].quantile(0.25)\n",
    "Q3 = data['total'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#encontrando média e desvio padrão\n",
    "desvp = data['total'].std()\n",
    "media = data['total'].mean()\n",
    "\n",
    "print(\"IQR = %.2f\" % IQR)\n",
    "print(\"media = %.2f, desvio padrao = %.2f\" % (media, desvp))\n",
    "\n",
    "# apenas outliers segundo IQR\n",
    "dataout_iqr = data[(data['total'] < Q1-(IQR*1.5)) \n",
    "                    | (data['total'] > Q3+(IQR*1.5))]\n",
    "# apenas inliers segundo IQR\n",
    "dc_iqr = data[(data['total'] >= Q1-(IQR*1.5)) \n",
    "              & (data['total'] <= Q3+(IQR*1.5))]\n",
    "\n",
    "# apenas outliers segundo std\n",
    "dataout_std = data[(data['total'] < media-(desvp*2)) \n",
    "                   | (data['total'] > media+(desvp*2))]\n",
    "# apenas inliers segundo std\n",
    "dc_std = data[(data['total'] >= media-(desvp*2)) \n",
    "                   & (data['total'] <= media+(desvp*2))]\n",
    "\n",
    "#DICA:\n",
    "#removendo outliers\n",
    "dc = data.copy()\n",
    "\n",
    "for var in data:\n",
    "    print(var)\n",
    "    \n",
    "    # verifica se variável é numerica\n",
    "    if np.issubdtype(dc[var].dtype, np.number):\n",
    "        print('\\tnumérica: removendo outliers via IQR')\n",
    "        Q1 = dc[var].quantile(0.25)\n",
    "        Q2 = dc[var].quantile(0.50)\n",
    "        Q3 = dc[var].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        print(\"\\tmediana = %.2f IQR = %.2f\" % (Q2,IQR))\n",
    "        # apenas inliers segundo IQR\n",
    "        dc = dc[(dc[var] >= Q1-(IQR*1.5)) & (dc[var] <= Q3+(IQR*1.5))]\n",
    "\n",
    "#Plotando resultados\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(121); data.boxplot(['total'])\n",
    "plt.title('Original')\n",
    "\n",
    "plt.subplot(122); dc.boxplot(['total']); \n",
    "plt.title('Apos remocao de outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Agrupamento - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "Outra técnica consiste em utilizar aprendizado não-supervisionado, inferindo agrupamentos e verificando se há pontos isolados em certos grupos.\n",
    "\n",
    "Vamos considerar um par de atributos para considerar ao mesmo tempo: rent e hoa\n",
    "\n",
    "O método utilizado será o DBSCAN - *Density-Based Spatial Clustering of Applications with Noise*, mas outros também podem ser empregados na mesma lógica:\n",
    "* agrupamentos (clusters) isolados com poucos pontos tendem a indicar outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando outliers e inliers por agrupamento\n",
    "\n",
    "#bibliotecas\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "#dados\n",
    "X1 = np.array(dc['rent'])\n",
    "X2 = np.array(dc['hoa'])\n",
    "X = np.vstack((X1,X2)).T\n",
    "\n",
    "# aprende o agrupamento\n",
    "# eps = distancia máxima para dois pontos serem considerados vizinhos\n",
    "#     (depende bastante da amplitude dos atributos)\n",
    "# min_samples = minimo de exemplos numa vizinhanca para considerar um \n",
    "#               agrupamento\n",
    "db = DBSCAN(eps = 200, min_samples=3).fit(X)\n",
    "clusters = db.labels_\n",
    "\n",
    "# número de rótulos -1 sao considerados outliers!\n",
    "n_outl_ = list(clusters).count(-1)\n",
    "# retirando os outliers, quantos clusters foram encontrados:\n",
    "n_clusters_ = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "# índices dos outliers\n",
    "outl_ind = np.where(clusters==-1)\n",
    "\n",
    "print('Número de agrupamentos estimado: %d' % n_clusters_)\n",
    "print('Número de outliers estimados: %d' % n_outl_)\n",
    "print(\"Coeficiente de silhueta: %0.3f\"\n",
    "      % metrics.silhouette_score(X, clusters))\n",
    "\n",
    "plt.plot(X1, X2,'.')\n",
    "plt.plot(X1[outl_ind], X2[outl_ind],'xr')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
