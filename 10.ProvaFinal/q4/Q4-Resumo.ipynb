{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientações - Prof. Louzada\n",
    "4. Uma questão terá como foco as disciplinas “Redes Neurais e Arquiteturas Profundas” e “Análise de Dados com Base em Processamento Massivo em Paralelo”. Essas duas disciplinas compreenderão uma questão da prova, a qual será dividida em itens. A questão envolverá código e deverá ser feita usando um notebook. Neste notebook, todas as bibliotecas, bases de dados, inicializações, instalações, importações, geração de dataFrames, geração de visões temporárias e atualização dos tipos de dados necessárias para a realização da questão já estarão realizadas. Portanto, o aluno irá se preocupar somente com a especificação do código solicitado. Assim como feito na avaliação final da disciplina de “Análise de Dados com Base em Processamento Massivo em Paralelo”, os itens da questão poderão ser encontrados utilizando o menu de navegação. Isso facilitará o rápido acesso ao local do notebook no qual cada item da questão se encontra. Adicionalmente, também será indicado o local do notebook no qual o código desenvolvido para resolver os itens deve ser especificado. Quanto ao conteúdo a ser cobrado em cada uma dessas disciplinas, destaca-se que: \n",
    "   O foco da disciplina de “Redes Neurais e Arquiteturas Profundas” é no projeto (arquitetura em camadas), treinamento (otimização e estratégias) e predição (de exemplos com o modelo treinado) usando uma rede neural.\n",
    "   O foco da disciplina de “Análise de Dados com Base em Processamento Massivo em Paralelo” é na especificação de consultas OLAP, segundo a mesma filosofia usada na avaliação final da disciplina.   Ou seja, a especificação de consultas OLAP usando a linguagem SQL e/ou os métodos de pyspark.sql. Ao concluir a questão e assinalar a resposta que julgue correta no Moodle, os seguintes passos devem ser realizados para concluir a submissão da resposta: \n",
    "   \n",
    "   4.1) Exportar o notebook que foi utilizado para resolver a questão da prova em formato .py e fazer upload no Moodle. Atenção: não deve ser feito upload de um arquivo notebook (.ipynb), mas sim de um arquivo texto .py contendo os códigos python que foram utilizados para resolver às questões. O arquivo .py pode ser gerado realizando-se as seguintes ações: File --> Download as --> Python (.py) disponível no Jupyter Notebook. ou File --> Download .py no Google Colab. Caso a resolução da questão não tenha sido feita usando o Jupyter, o código desenvolvido para responder à questão deve ser enviado em um arquivo ASCII (Texto) salvo na extensão .py \n",
    "   \n",
    "   4.2) Exportar o notebook que foi utilizado para resolver a questão da prova em formato .pdf e fazer upload no Moodle.\n",
    "   \n",
    "   4.3) Os arquivos devem ser nomeados com o nome e o sobrenome do aluno, SEM ESPAÇO entre as partes de seu nome. Exemplo: moacirponti.py e moacirponti.pdf \n",
    "   \n",
    "   4.4) É OBRIGATÓRIO conter no cabeçalho (início) do arquivo um comentário com o seu nome completo. Por exemplo,# Moacir Ponti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 02, 03 e Avaliação Final - Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='.\\img\\cnn.JPG'>\n",
    "<img src='.\\img\\convolucao.JPG'>\n",
    "\n",
    "Zero-padding: para compensar a impossibilidade de computar todos os valores; amplia-se a entrada de forma que o volume de saída seja igual ao de entrada\n",
    "<img src='.\\img\\zero-padding.JPG'>\n",
    "Convolução em profundidade: quando a entrada possui mais do que 1 canal. O filtro terá k × k × p, onde p é a quantidade de canais de entrada\n",
    "<img src='.\\img\\profundidade.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceitos:\n",
    "\n",
    "Machine Learning e Deep Learning depende de entender otimização e conhecer bem:\n",
    "\n",
    "I Função de custo/perda e intuição de seus valores\n",
    "\n",
    "I (Intuição) do gradiente da função\n",
    "\n",
    "I Inicialização\n",
    "\n",
    "I Algoritmo de otimização\n",
    "\n",
    "I Taxa de aprendizado\n",
    "\n",
    "I Tamanho do batch\n",
    "\n",
    "I Convergência ao longo do treinamento\n",
    "\n",
    "\n",
    "##### Função de custo: \n",
    "Métrica que indique o custo de escolher o modelo atual Idealmente deve ser convexa e produzir um gradiente com boa magnitude. Difícil, considerando todas as direções do hiper-espaço de parâmetros.\n",
    "\n",
    "Mean-squared-error: erro médio quadrático/perda\n",
    "quadrática\n",
    "I utilizada para valores contínuos,\n",
    "I mede a divergência quadrática de cada valor de entrada com\n",
    "relação à saída\n",
    "\n",
    "I Cross-entropy: entropia cruzada\n",
    "I mais comum e recomendada para probabilidades\n",
    "I teoria da informação\n",
    "I intuição: o numero de bits adicionais necessários para\n",
    "representar o evento de referência ao invés do predito.\n",
    "\n",
    "##### Gradiente\n",
    "Codifica as taxas de alteração no espaço de parâmetros\n",
    "I queremos andar na direção do vale, em busca do mínimo global\n",
    "\n",
    "Backpropagation\n",
    "I utiliza a derivada ao longo das camadas para adaptar os pesos\n",
    "I as funções de custo e de ativação tem que produzir derivada\n",
    "útil\n",
    "\n",
    "\n",
    "Vanishing gradient\n",
    "I se ativações geram valores muito baixos não é possível adaptar\n",
    "I usar precisão dupla (double) e escalar as funções é uma\n",
    "possibilidade\n",
    "I esse é um dos motivadores do uso de ReLU ao invés de\n",
    "Sigmóides como função de ativação\n",
    "\n",
    "##### Funções:\n",
    "<img src='.\\img\\funcoes.JPG'>\n",
    "\n",
    "##### Inicialização\n",
    "Aleatória portanto o resultado é diferente a cada execução.\n",
    "Escolhas comuns\n",
    "I Pesos: valor aleatório pela distribuição normal entre 0-1\n",
    "I Bias: 0 (zero)\n",
    "A complexidade do treinamento dificulta múltiplas execuções\n",
    "I Importante fazer experimentos piloto em pequenos\n",
    "subconjuntos de dados\n",
    "\n",
    "##### Otimização\n",
    "Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Batch Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Momentum :Interpreta o custo como um terreno montanhoso.\n",
    "\n",
    "Adam: Utiliza momentos do gradiente: o segundo momento é usado para normalizar o primeiro, evitando outliers/pontos de inflexão\n",
    "\n",
    "##### Tamanho de batch\n",
    "Há uma relação entre tamanho de batch e taxa de aprendizado.\n",
    "Padrão é 32\n",
    "\n",
    "batches maiores: estimativas mais suaves, difícil manter na\n",
    "memória, exige ajustar bem a taxa de aprendizado,\n",
    "\n",
    "batches menores: estimativas mais ruidosas, mas que\n",
    "mostraram vantagens em encontrar melhores mínimos\n",
    "\n",
    "##### Taxa de atualização\n",
    "Padrão é 0.01\n",
    "\n",
    "pode ser pouco adequado para alguns otimizadores\n",
    "\n",
    "pode ser pouco adequado para batchs maiores (ou muito\n",
    "pequenos)\n",
    "\n",
    "É recomendado iniciar com um valor maior, e reduzir a taxa\n",
    "progressivamente (learning rate scheduling).\n",
    "\n",
    "##### Convergencia ao longo do treinamento\n",
    "O gráfico do custo diz muito sobre o aprendizado\n",
    "\n",
    "Acompanhe o custo ao longo de épocas, se possível com\n",
    "conjunto de validação (idealmente não deve ser o teste!)\n",
    "\n",
    "Inicie com experimentos com poucos exemplos\n",
    "\n",
    "Explore os hiperparâmetros tentando obter \"overfitting\" para\n",
    "um subconjunto de exemplos, obtendo custo próximo a zero, e\n",
    "depois refine a busca num conjunto maior.\n",
    "\n",
    "#### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rede neural convolucional (CNN)\n",
    "CNN = keras.Sequential()\n",
    "CNN.add(keras.layers.Conv2D(32,kernel_size=(3,3),strides=(2,2),padding='same', activation='relu',\n",
    "                            input_shape=(img_lin, img_col,1)))\n",
    "\n",
    "CNN.add(keras.layers.Conv2D(64,kernel_size=(1,3),padding='same',strides=(1,2),activation='relu'))\n",
    "CNN.add(keras.layers.Conv2D(64,kernel_size=(3,1),padding='same',strides=(2,1),activation='relu'))\n",
    "CNN.add(keras.layers.Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "CNN.add(keras.layers.GlobalAveragePooling2D())\n",
    "CNN.add(keras.layers.Dense(num_classes,activation='softmax'))\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denoising overcomplete autoencoder para pré-treinamento baseado em auto-supervisão\n",
    "def denoising_over_AE(input_shape):\n",
    "    input = keras.layers.Input(shape=(input_shape))\n",
    "    #encoder:\n",
    "    encoder = keras.layers.BatchNormalization()(input)\n",
    "    encoder = keras.layers.Dense(32, activation='relu')(encoder)\n",
    "    encoder = keras.layers.Dense(32, activation='relu')(encoder)\n",
    "    encoder = keras.layers.Dropout(0.2)(encoder)\n",
    "    encoder = keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = keras.layers.Dense(28, activation='relu',name='code')(encoder)\n",
    "    #decoder\n",
    "    decoder = Dense(32,activation='relu',name='input_decoder')(encoder)\n",
    "    decoder = Dense(32,activation='relu')(decoder)\n",
    "    decoder = Dense(28,activation='tanh')(decoder)\n",
    "    #autoencoder\n",
    "    autoencoder = keras.models.Model(input, decoder)\n",
    "    autoencoder.summary()\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "#Treinar com perda MSE por 20 épocas com batch size 16 utilizando o conjunto U\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "\n",
    "modelo_A = denoising_over_AE(28)\n",
    "\n",
    "modelo_A.compile(loss='mse',\n",
    "                 optimizer=keras.optimizers.Adam(lr=lr))\n",
    "\n",
    "historyDenoising = modelo_A.fit(x=UNoise, y=U,\n",
    "                                 epochs=epochs, \n",
    "                                 batch_size = batch_size,\n",
    "                                 callbacks=[callbacklr],\n",
    "                                 verbose=1)\n",
    "\n",
    "\n",
    "#DICA:\n",
    "#realizando PCA pra imprimir dados\n",
    "pca_S_Orig = PCA(n_components=2, random_state=1)\n",
    "pca_S_Orig_result = pca_S_Orig.fit_transform(SxNorm)\n",
    "\n",
    "#realizando o plot\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "sns.scatterplot(x=pca_S_Orig_result[:,0],y=pca_S_Orig_result[:,1],alpha='auto', hue=Sy['Class'], palette='prism')\n",
    "plt.title('Scatterplot com projeção PCA do conjunto de S original')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rede neural profunda densa, utilizando como base o encoder do modelo A, \n",
    "#e inserindo uma nova camada densa de classificação com ativação sigmóide.\n",
    "\n",
    "#obtendo saída da camada de encode do modelo A\n",
    "base_saida = modelo_A.layers[-4].output\n",
    "\n",
    "#criando nova camada de saída que recebe a anterior\n",
    "saida_nova = Dense(1, activation='sigmoid')(base_saida)\n",
    "\n",
    "#modelo B, com a nova camada de saida\n",
    "modelo_B = keras.models.Model(modelo_A.inputs, saida_nova)\n",
    "modelo_B.summary()\n",
    "\n",
    "#taxa de aprendizado inicial de 0.001 e com decaimento em todas as épocas, exponencial a -0.3\n",
    "lr = 0.001\n",
    "\n",
    "def scheduler_B (epoch, lr):\n",
    "    if epoch < 1:\n",
    "        return lr\n",
    "    else:\n",
    "        return np.round(lr * tf.math.exp(-0.3),4)\n",
    "\n",
    "callbacklr = tf.keras.callbacks.LearningRateScheduler(scheduler_B)\n",
    "\n",
    "#ponderar o total de cada classe e formar o peso, pois as mesmas estão muito desbalanceadas\n",
    "#Uso de pesos para as classes: 0.1 para classe 0 (majoritária), e 0.9 para a classe 1 (minoritária)\n",
    "peso_0 = 0.1\n",
    "peso_1 = 0.9\n",
    "class_weight = {0: peso_0, 1: peso_1}\n",
    "\n",
    "#Treinar com perda MSE por 8 épocas com batch size 16\n",
    "batch_size = 16\n",
    "epochs = 8\n",
    "\n",
    "#Compute como métricas, além da perda, precisão e revocação (precision / recall)\n",
    "metrics = [\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "]\n",
    "\n",
    "#compilando o modelo B\n",
    "modelo_B.compile(loss='mse',\n",
    "                 optimizer=keras.optimizers.Adam(lr=lr),\n",
    "                 metrics=metrics)\n",
    "\n",
    "#como não foi comentado nada nas questões referentes a este item, por padrão, fiz o treinamento com os dados sem ruído\n",
    "#sem ruído\n",
    "history = modelo_B.fit(x=Sx, y=Sy,\n",
    "                       epochs = epochs, \n",
    "                       batch_size = batch_size,\n",
    "                       callbacks = [callbacklr],\n",
    "                       class_weight = class_weight,\n",
    "                       verbose=1)\n",
    "\n",
    "#DICA:\n",
    "#Exiba o gráfico da precisão e revocação no treinamento calculada ao longo das épocas\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "fig = plt.plot(history.history['precision'],'b')\n",
    "fig = plt.plot(history.history['recall'], color='tab:orange', linestyle='--')\n",
    "fig = plt.title('Gráfico de Precisão e Revocação Calculado ao Longo das Épocas')\n",
    "fig = plt.xlabel('Épocas')\n",
    "fig = plt.legend([\"precision\",\"recall\"], loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo DNN com a mesma arquitetura usada no modelo B, mas sem pré-treinamento\n",
    "def dnn(input_shape):\n",
    "    input = Input(shape=(input_shape))\n",
    "    #encoder:\n",
    "    encoder = BatchNormalization()(input)\n",
    "    encoder = Dense(32, activation='relu')(encoder)\n",
    "    encoder = Dense(32, activation='relu')(encoder)\n",
    "    encoder = Dropout(0.2)(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Dense(28, activation='relu',name='code')(encoder)\n",
    "    #decoder\n",
    "    decoder = Dense(1, activation='sigmoid')(encoder)\n",
    "    dnn = keras.models.Model(input,decoder)\n",
    "    dnn.summary()\n",
    "    return dnn\n",
    "\n",
    "#compilando o modelo DNN\n",
    "modelo_DNN.compile(loss='mse',\n",
    "                   optimizer=keras.optimizers.Adam(lr=lr),\n",
    "                   metrics=metrics)\n",
    "\n",
    "#como não foi comentado nada nas questões referentes a este item, por padrão, fiz o treinamento com os dados sem ruído\n",
    "#sem ruído\n",
    "historyDNN = modelo_DNN.fit(x=Sx, y=Sy,\n",
    "                       epochs = epochs, \n",
    "                       batch_size = batch_size,\n",
    "                       callbacks = [callbacklr],\n",
    "                       class_weight = class_weight,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificador SVM treinado nos dados originais S.\n",
    "\n",
    "#aplicando ponderaração das classes\n",
    "peso_0 = 0.1\n",
    "peso_1 = 0.9\n",
    "class_weight = {0: peso_0, 1: peso_1}\n",
    "\n",
    "#definindo modelo SVM\n",
    "model = SVC(kernel='linear', C = 0.5, \n",
    "            random_state=1,\n",
    "            class_weight = class_weight)\n",
    "\n",
    "#como não foi comentado nada nas questões referentes a este item, por padrão, fiz o treinamento com os dados sem ruído\n",
    "#aplicando reshape do Sy, apenas para manter num formato sem warning do sklearn de array de 1d\n",
    "\n",
    "#sem ruído\n",
    "model.fit(Sx,np.asarray(Sy).reshape(-1)) \n",
    "\n",
    "#com ruído\n",
    "#model.fit(SxNoise,np.asarray(Sy).reshape(-1))\n",
    "\n",
    "#predizendo as classes para S e T:\n",
    "predict_Sy = model.predict(Sx)\n",
    "predict_Ty = model.predict(Tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Avaliação Final - Processamento Massivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparação ambiente\n",
    "import os \n",
    "\n",
    "#import modificado para ambiente windows\n",
    "os.environ['SPARK_HOME'] = 'D:\\\\Program\\\\spark-3.0.0-bin-hadoop2.7'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='notebook'\n",
    "\n",
    "#importando o módulo findspark\n",
    "import findspark\n",
    "#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#estava tomando erro no JAVA_HOME que não estava configurado. \n",
    "#Dê preferencia para instalar em um local sem espaço em branco no nome do diretório em ambiente windows.\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta da Questão 1\n",
    "\n",
    "#------------------------------------------------------CÓDIGO COMENTADO------------------------------------------------------#\n",
    "# 01. selecionando dataAno como ANO, funcSexo como SEXO e a média do salário com arredondamento de 2 casas como MEDIASALARIO #\n",
    "# 02. especificando as relações de data com pagamento contendo mesma dataPK                                                  #\n",
    "# 03. especificando as relações de funcionario com pagamento contendo mesmo funcPK                                           #\n",
    "# 04. agrupando os resultados por ANO e SEXO                                                                                 #\n",
    "# 05. ordenando o ANO e SEXO de forma ascendente                                                                             #\n",
    "# 06. apresentando os 20 primeiros resultados sem truncamento das strings                                                    #\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "consultaSQL = \"\"\"\n",
    "SELECT dataAno AS `ANO`, funcSexo AS `SEXO`, ROUND(AVG(salario),2) AS `MEDIASALARIO`\n",
    "FROM data JOIN pagamento ON (data.dataPK = pagamento.dataPK)\n",
    "          JOIN funcionario ON (funcionario.funcPK = pagamento.funcPK)\n",
    "GROUP BY ANO, SEXO\n",
    "ORDER BY ANO ASC, SEXO ASC  \n",
    "\"\"\"\n",
    "spark.sql(consultaSQL).show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta da Questão 2\n",
    "\n",
    "#importando a função round para poder fazer o arredondamento dos valores agregados\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "#------------------------------------------------------CÓDIGO COMENTADO-------------------------------------------------------#\n",
    "# 01. utilizando dados de funcionario                                                                                         #\n",
    "# 02. junto com dados de pagamento pela chave primária funcPK                                                                 #\n",
    "# 03. junto com dados de data pela chave primária dataPK                                                                      #\n",
    "# 04. selecionando os dados requisitados dataAno, funcSexo, funcRegiaoNome e salario                                          #\n",
    "# 05. listando todas as agregações com cube usando as dataAno, funcSexo, funcRegiaoNome e agregando pela média dos salários.  #\n",
    "#     O método cube serve para criar subtotais para todas as combinações de atributos definidos na lista de agrupamento       #\n",
    "# 06. arredondando média dos salários para duas casas decimais                                                                #\n",
    "# 07. renomeando dataAno para ANO                                                                                             #\n",
    "# 08. renomeando funcSexo para SEXO                                                                                           #\n",
    "# 09. renomeando funcRegiaoNome para REGIAO                                                                                   #\n",
    "# 10. renomeando avg(salario) para MEDIASALARIO                                                                               #\n",
    "# 11. ordenando por dataAno, funcSexo e funcRegiaoNome, todos ascendentes                                                     #\n",
    "# 12. apresentando os 20 primeiros resultados sem truncamento das strings                                                     #\n",
    "#-----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "funcionario\\\n",
    "    .join(pagamento, on='funcPK')\\\n",
    "    .join(data, on='dataPK')\\\n",
    "    .select('dataAno','funcSexo','funcRegiaoNome','salario')\\\n",
    "    .cube('dataAno','funcSexo','funcRegiaoNome').avg('salario')\\\n",
    "    .withColumn('avg(salario)',round('avg(salario)',2))\\\n",
    "    .withColumnRenamed('dataAno','ANO')\\\n",
    "    .withColumnRenamed('funcSexo','SEXO')\\\n",
    "    .withColumnRenamed('funcRegiaoNome','REGIAO')\\\n",
    "    .withColumnRenamed('avg(salario)','MEDIASALARIO')\\\n",
    "    .orderBy('dataAno','funcSexo','funcRegiaoNome', ascending=True)\\\n",
    "    .show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta da Questão 3\n",
    "\n",
    "#------------------------------------------------------CÓDIGO COMENTADO------------------------------------------------------#\n",
    "# 01. selecionando dataAno como ANO                                                                                          #\n",
    "#     aplicando um select para extrair o ano atual e subtraindo de funcAnoNascimento para definir a IDADE                    #\n",
    "#     selecionando funcRegiaoNome como REGIAO                                                                                #\n",
    "#     realizando a soma dos salarios arredondando para 2 casas decimais como TOTALSALARIO                                    #\n",
    "# 02. especificando as relações de data com pagamento contendo mesma dataPK                                                  #\n",
    "# 03. especificando as relações de funcionario com pagamento contendo mesmo funcPK                                           #\n",
    "# 04. aplicando condição para funcionárias apenas do sexo feminino                                                           #\n",
    "# 05. que nasceram entre 1970 (inclusive) e 1990 (inclusive) aplicando a função BETWEEN                                      #\n",
    "# 06. que vivem no SUDESTE ou no NORDESTE (tanto pelo nome da região quanto pela sigla)                                      #\n",
    "# 07. agrupando os dados por ANO, IDADE e REGIAO                                                                             #\n",
    "# 08. ordenando por ANO, IDADE e REGIAO de forma ascendente                                                                  #\n",
    "# 09. apresentando os 20 primeiros resultados sem truncamento das strings                                                    #\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "consultaSQL = \"\"\"\n",
    "SELECT dataAno AS `ANO`, ((SELECT EXTRACT (YEAR FROM CURRENT_DATE)) - funcAnoNascimento) AS `IDADE`, funcRegiaoNome AS `REGIAO`, ROUND(SUM(salario),2) AS `TOTALSALARIO`\n",
    "FROM data JOIN pagamento   ON (data.dataPK = pagamento.dataPK)\n",
    "          JOIN funcionario ON (funcionario.funcPK = pagamento.funcPK)\n",
    "WHERE funcSexo = 'F' \n",
    "      AND funcAnoNascimento BETWEEN 1970 AND 1990\n",
    "      AND (funcRegiaoNome = 'SUDESTE' OR  funcRegiaoNome = 'NORDESTE' OR funcRegiaoSigla = 'SE' OR funcRegiaoSigla = 'NE')\n",
    "GROUP BY ANO, IDADE, REGIAO \n",
    "ORDER BY ANO ASC, IDADE ASC, REGIAO ASC\n",
    "\"\"\"\n",
    "spark.sql(consultaSQL).show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reposta da Questão 4\n",
    "\n",
    "#------------------------------------------------------CÓDIGO COMENTADO------------------------------------------------------#\n",
    "# 01. utilizando dados de negociacao                                                                                         #\n",
    "# 02. juntando com dados de equipe pela chave primária equipePK                                                              #\n",
    "# 03. juntando com dados de data pela chave primária dataPK                                                                  #\n",
    "# 04. juntando com dados de cliente pela chave primária clientePK                                                            #\n",
    "# 05. aplicando condição para equipePK sendo 1, 3  e 5 e clientes da cidade de SAO CARLOS                                    #\n",
    "# 06. selecionando os dados requisitados dataAno, equipeNome, filialNome, clienteSetor e receita                             #\n",
    "# 07. agrupando por dataAno, equipeNome, filialNome, clienteSetor e agregando pela soma das receitas                         #\n",
    "# 08. arredondando a soma da receitas para 2 casas decimais                                                                  #\n",
    "# 09. renomeando dataAno para ANO                                                                                            #\n",
    "# 10. renomeando equipeNome para NOMEEQUIPE                                                                                  #\n",
    "# 11. renomeando filialNome para NOMEFILIAL                                                                                  #\n",
    "# 12. renomeando clienteSetor para SETORCLIENTE                                                                              #\n",
    "# 13. renomeando sum(receita) para TOTALRECEITA                                                                              #\n",
    "# 14. ordenando por ANO, NOMEEQUIPE, NOMEFILIAL e SETORCLIENTE de forma ascendente                                           #\n",
    "# 15. apresentando os 20 primeiros resultados sem truncamento das strings                                                    #\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "negociacao\\\n",
    "    .join(equipe, on='equipePK')\\\n",
    "    .join(data, on='dataPK')\\\n",
    "    .join(cliente, on='clientePK')\\\n",
    "    .where('(equipePK = 1 OR equipePK = 3 OR equipePK = 5) AND clienteCidade = \"SAO CARLOS\"')\\\n",
    "    .select('dataAno', 'equipeNome', 'filialNome', 'clienteSetor', 'receita')\\\n",
    "    .groupBy('dataAno', 'equipeNome', 'filialNome', 'clienteSetor').sum('receita')\\\n",
    "    .withColumn('sum(receita)',round('sum(receita)',2))\\\n",
    "    .withColumnRenamed('dataAno','ANO')\\\n",
    "    .withColumnRenamed('equipeNome','NOMEEQUIPE')\\\n",
    "    .withColumnRenamed('filialNome','NOMEFILIAL')\\\n",
    "    .withColumnRenamed('clienteSetor','SETORCLIENTE')\\\n",
    "    .withColumnRenamed('sum(receita)','TOTALRECEITA')\\\n",
    "    .orderBy('ANO','NOMEEQUIPE','NOMEFILIAL', 'SETORCLIENTE',ascending=True)\\\n",
    "    .show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta da Questão 5\n",
    "\n",
    "#------------------------------------------------------CÓDIGO COMENTADO------------------------------------------------------#\n",
    "#    Neste caso vamos usar a operação drill-across que compara medidas numéricas de tabelas de fatos diferentes              #\n",
    "#    utilizando uma dimensão em comum (no caso data).                                                                        #\n",
    "# 01. selecionando dataAnoP (dataAno já consolidada na subquery de salário) como ANO                                         #\n",
    "#     selecionando a soma dos salarios arredondando com 2 casas como TOTALSALARIO                                            #\n",
    "#     selecionando a soma das receitas arredondando com 2 casas como TOTALRECEITA                                            #\n",
    "#    Vamos executar dois sub-selects no comando FROM. O primeiro responsável pelos dados de salário e o segundo de receita   #\n",
    "# 02. selecionando dataAno de data e soma dos salarios de funcionario                                                        #\n",
    "# 03. especificando as relações de pagamento com data contendo mesma dataPK                                                  #\n",
    "# 04. especificando as relações de funcionario com pagamento contendo mesmo funcPK                                           #\n",
    "# 05. aplicando condição para funcionárias apenas do sexo feminino                                                           #\n",
    "# 06. aplicando condição para funcionárias do RIO DE JANEIRO (filtrando tanto pelo nome do estado quanto pela sigla)         #\n",
    "# 07. agrupando os dados por dataAno                                                                                         #\n",
    "# 08. aplicando os resultados do sub-select como dataAnoP para dataAno e salario como o total do salário das funcionárias    #\n",
    "# 09. cláusula para fazer a junção com outro sub-select, agora aplicado nos dados de receita                                 #\n",
    "# 10. selecionando dataAno de data e soma das receitas de negociação                                                         #\n",
    "# 11. especificando as relações de data com negociação contendo mesma dataPK                                                 #\n",
    "# 12. especificando as relações de equipe com negociação contendo mesmo equipePK                                             #\n",
    "# 13. aplicando condição para equipes do RIO DE JANEIRO (filtrando tanto pelo nome do estado quanto pela sigla)              #\n",
    "# 14. agrupando os dados por dataAno                                                                                         #\n",
    "# 15. aplicando os resultados do sub-select como dataAnoN para dataAno e receita como o total do receita das equipes         #\n",
    "# 16. aplicando condição para juntar a dimensão em comum, dataAnoP do salário igual ao dataAnoN da receita                   #\n",
    "# 17. agrupando por data, no caso dataAnoP                                                                                   #\n",
    "# 18. ordenando por data, no caso dataAnoP de forma ascendente                                                               #\n",
    "# 19. apresentando os 20 primeiros resultados sem truncamento das strings                                                    #\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "consultaSQL = \"\"\"\n",
    "SELECT dataAnoP AS `ANO`, ROUND(SUM(salario),2) AS `TOTALSALARIO`, ROUND(SUM(receita),2) AS `TOTALRECEITA`\n",
    "FROM ( SELECT dataAno, SUM(salario)\n",
    "       FROM pagamento JOIN data        ON (pagamento.dataPK = data.dataPK)\n",
    "                      JOIN funcionario ON (funcionario.funcPK = pagamento.funcPK)\n",
    "        WHERE funcSexo = 'F'\n",
    "              AND (funcEstadoNome = 'RIO DE JANEIRO' OR funcEstadoSigla = 'RJ')\n",
    "        GROUP BY dataAno\n",
    "      ) AS sal(dataAnoP, salario)\n",
    "      JOIN \n",
    "      ( SELECT dataAno, SUM(receita)\n",
    "        FROM data JOIN negociacao ON (data.dataPK = negociacao.dataPK)\n",
    "                  JOIN equipe     ON (equipe.equipePK = negociacao.equipePK)\n",
    "        WHERE (filialEstadoNome = 'RIO DE JANEIRO' OR filialEstadoSigla = 'RJ')\n",
    "        GROUP BY dataAno\n",
    "      ) AS rec(dataAnoN, receita)\n",
    "WHERE dataAnoP = dataAnoN\n",
    "GROUP BY dataAnoP\n",
    "ORDER BY dataAnoP ASC\n",
    "\"\"\"\n",
    "spark.sql(consultaSQL).show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta da Questão 6\n",
    "\n",
    "#------------------------------------------------------CÓDIGO COMENTADO------------------------------------------------------#\n",
    "#    Neste caso vamos usar a operação drill-across que compara medidas numéricas de tabelas de fatos diferentes              #\n",
    "#    utilizando uma dimensão em comum (no caso data). Neste caso, vamos precisar separar as consultas em dois blocos,        #\n",
    "#    sendo o primeiro referente aos dados de salario e o segundo referente aos dados de receitas.                            #\n",
    "# 01. utilizando dados de pagamento e atribuindo ao bloco 1 nomeado de pag                                                   #\n",
    "# 02. juntando com dados de data pela chave primária dataPK                                                                  #\n",
    "# 03. juntando com dados de funcionario pela chave primária funcPK                                                           #\n",
    "# 04. aplicando condição para identificar funcionárias do estado do RIO DE JANEIRO (tanto por nome quanto por sigla)         #\n",
    "# 05. selecionando os dados de dataAno e salario como resultados do bloco 1                                                  #\n",
    "# 06. agrupando os dados por dataAno                                                                                         #\n",
    "# 07. agregando os dados de salário como o total do salário das funcionárias                                                 #\n",
    "# 08. utilizando dados de negociacao e atribuindo ao bloco 2 nomeado de neg                                                  #\n",
    "# 09. juntando com dados de data pela chave primária dataPK                                                                  #\n",
    "# 10. juntando com dados de equipe pela chave primária equipePK                                                              #\n",
    "# 11. aplicando condição para identificar equipes do estado do RIO DE JANEIRO (tanto por nome quanto por sigla)              #\n",
    "# 12. selecionando os dados de dataAno e receita como resultados do bloco 2                                                  #\n",
    "# 13. agrupando os dados por dataAno                                                                                         #\n",
    "# 14. agregando os dados de receita como o total da receita das equipes                                                      #\n",
    "# 15. utilizando dados do bloco 1 nomeado de pag                                                                             #\n",
    "# 16. juntando com dados do bloco 2 nomeado de neg pela dimensão em comum entre ambas, no caso, dataAno                      #\n",
    "# 17. selecionando os dados de dataAno, total de salarios e total de receitas                                                #\n",
    "# 18. arredondando a soma dos salários para 2 casas decimais                                                                 #\n",
    "# 19. arredondando a soma da receitas para 2 casas decimais                                                                  #\n",
    "# 20. renomeando dataAno para ANO                                                                                            #\n",
    "# 21. renomeando sum(salario) para TOTALSALARIO                                                                              #\n",
    "# 22. renomeando sum(receita) para TOTALRECEITA                                                                              #\n",
    "# 23. ordenando por data, no caso ANO de forma ascendente                                                                    #\n",
    "# 24. apresentando os 20 primeiros resultados sem truncamento das strings                                                    #\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "pag = pagamento\\\n",
    "   .join(data, on='dataPK')\\\n",
    "   .join(funcionario, on='funcPK')\\\n",
    "   .where('funcSexo = \"F\" AND (funcEstadoNome = \"RIO DE JANEIRO\" OR funcEstadoSigla = \"RJ\")')\\\n",
    "   .select('dataAno', 'salario')\\\n",
    "   .groupBy('dataAno')\\\n",
    "   .sum('salario')\n",
    "\n",
    "neg = negociacao\\\n",
    "   .join(data, on='dataPK')\\\n",
    "   .join(equipe, on='equipePK')\\\n",
    "   .where('filialEstadoNome = \"RIO DE JANEIRO\" OR filialEstadoSigla = \"RJ\"')\\\n",
    "   .select('dataAno', 'receita')\\\n",
    "   .groupBy('dataAno')\\\n",
    "   .sum(\"receita\")\n",
    "\n",
    "pag\\\n",
    "   .join(neg, on='dataAno')\\\n",
    "   .select('dataAno', 'sum(salario)', 'sum(receita)')\\\n",
    "   .withColumn('sum(salario)', round('sum(salario)',2))\\\n",
    "   .withColumn('sum(receita)', round('sum(receita)',2))\\\n",
    "   .withColumnRenamed('dataAno', 'ANO')\\\n",
    "   .withColumnRenamed(\"sum(salario)\", 'TOTALSALARIO')\\\n",
    "   .withColumnRenamed(\"sum(receita)\", 'TOTALRECEITA')\\\n",
    "   .orderBy('ANO',ascending=True)\\\n",
    "   .show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta da Questão 7\n",
    "\n",
    "#------------------------------------------------------CÓDIGO COMENTADO------------------------------------------------------#\n",
    "#    Neste caso vamos usar a operação drill-across que compara medidas numéricas de tabelas de fatos diferentes              #\n",
    "#    utilizando uma dimensão em comum (no caso data).                                                                        #\n",
    "# 01. selecionando dataAnoPF (dataAno já consolidada na subquery de salário para sexo feminino) como ANO                     #\n",
    "#     selecionando a soma dos salarios femininos arredondando com 2 casas como TOTALSALARIOMULHERES                          #\n",
    "#     selecionando a soma dos salarios masculinos arredondando com 2 casas como TOTALSALARIOHOMENS                           #\n",
    "#     selecionando a soma das receitas arredondando com 2 casas como TOTALRECEITA                                            #\n",
    "#    Nestes casos, tivemos alguns números sendo apresentados com notação científica, portanto, aplicamos a função CAST       #\n",
    "#    AS DECIMAL, para a saída ficar no formato adequado (requerido no exercício). Como parâmetros do DECIMAL passamos o      #\n",
    "#    valor 10 (tamanho considerado adequado - número de dígitos permitidos na parte inteira) e 2 (número de casas decimais). #\n",
    "#    Vamos executar três sub-selects no comando FROM. O primeiro responsável pelos dados de salário feminino                 #\n",
    "#    o segundo responsável pelos dados de salário masculino e o terceiro responsável pelos dados de receita.                 #\n",
    "# 02. selecionando dataAno de data e soma dos salarios como salarioF (que terá os dados para funcionárias de sexo feminino)  #\n",
    "# 03. especificando as relações de pagamento com data contendo mesma dataPK                                                  #\n",
    "# 04. especificando as relações de funcionario com pagamento contendo mesmo funcPK                                           #\n",
    "# 05. aplicando condição para funcionárias apenas do sexo feminino                                                           #\n",
    "# 06. agrupando os dados por dataAno                                                                                         #\n",
    "# 07. aplicando os resultados do sub-select como dataAnoPF para dataAno e salarioF como o total do salário das mulheres      #\n",
    "# 08. cláusula para juntarmos o segundo sub-select                                                                           #\n",
    "# 09. selecionando dataAno de data e soma dos salarios como salarioM (que terá os dados para funcionárias de sexo masculino) #\n",
    "# 10. especificando as relações de pagamento com data contendo mesma dataPK                                                  #\n",
    "# 11. especificando as relações de funcionario com pagamento contendo mesmo funcPK                                           #\n",
    "# 12. aplicando condição para funcionários apenas do sexo masculino                                                          #\n",
    "# 13. agrupando os dados por dataAno                                                                                         #\n",
    "# 14. aplicando os resultados do sub-select como dataAnoPM para dataAno e salarioM como o total do salário dos homens        #\n",
    "# 15. cláusula para juntarmos o terceiro sub-select                                                                          #\n",
    "# 16. selecionando dataAno de data e soma das receitas de negociação                                                         #\n",
    "# 17. especificando as relações de data com negociação contendo mesma dataPK                                                 #\n",
    "# 18. especificando as relações de equipe com negociação contendo mesmo equipePK                                             #\n",
    "# 19. agrupando os dados por dataAno                                                                                         #\n",
    "# 20. aplicando os resultados do sub-select como dataAnoN para dataAno e receita como o total do receita das equipes         #\n",
    "# 21. aplicando condição para juntar a dimensão em comum, dataAnoPF igual ao dataAnoN, igual ao dataAnoPM                    #\n",
    "# 22. agrupando por data, no caso dataAnoPF                                                                                  #\n",
    "# 23. ordenando por data, no caso dataAnoPF de forma ascendente                                                              #\n",
    "# 24. apresentando os 20 primeiros resultados sem truncamento das strings                                                    #\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "consultaSQL = \"\"\"\n",
    "SELECT dataAnoPF AS `ANO`, CAST(ROUND(SUM(salarioF),2) AS DECIMAL(10,2)) AS `TOTALSALARIOMULHERES`, CAST(ROUND(SUM(salarioM),2) AS DECIMAL(10,2)) AS `TOTALSALARIOHOMENS`, CAST(ROUND(SUM(receita),2) AS DECIMAL(10,2)) AS `TOTALRECEITA`\n",
    "FROM  ( SELECT dataAno, SUM(salario) AS `salarioF`\n",
    "        FROM pagamento JOIN data        ON (pagamento.dataPK = data.dataPK)\n",
    "                       JOIN funcionario ON (funcionario.funcPK = pagamento.funcPK)\n",
    "        WHERE funcSexo = 'F'\n",
    "        GROUP BY dataAno\n",
    "      ) AS salF(dataAnoPF, salarioF)\n",
    "      JOIN \n",
    "      ( SELECT dataAno, SUM(salario) AS `salarioM`\n",
    "        FROM pagamento JOIN data        ON (pagamento.dataPK = data.dataPK)\n",
    "                       JOIN funcionario ON (funcionario.funcPK = pagamento.funcPK)\n",
    "        WHERE funcSexo = 'M'\n",
    "        GROUP BY dataAno\n",
    "      ) AS salM (dataAnoPM, salarioM)\n",
    "      JOIN \n",
    "      ( SELECT dataAno, SUM(receita)\n",
    "        FROM data JOIN negociacao ON (data.dataPK = negociacao.dataPK)\n",
    "                  JOIN equipe     ON (equipe.equipePK = negociacao.equipePK)\n",
    "        GROUP BY dataAno\n",
    "      ) AS rec(dataAnoN, receita)\n",
    "WHERE (dataAnoPF = dataAnoN AND dataAnoPF = dataAnoPM)\n",
    "GROUP BY dataAnoPF\n",
    "ORDER BY dataAnoPF ASC\n",
    "\"\"\"\n",
    "spark.sql(consultaSQL).show(20,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
