{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orienta√ß√µes - Prof. Louzada\n",
    "3. Uma quest√£o ser√° relacionada √†s disciplinas ‚ÄúEstat√≠stica para Ci√™ncia de Dados‚Äù e ‚ÄúAprendizado Din√¢mico‚Äù. Nessa quest√£o, ser√° priorizada a modelagem estat√≠stica em aspectos pr√°ticos, como a estima√ß√£o de par√¢metros, testes de hip√≥teses, an√°lise de res√≠duos, modelos com componentes autorregressivas, integradas e de m√©dia m√≥vel. Ser√° disponibilizado um notebook com pacotes com vers√µes espec√≠ficas recomendadas e alguns comandos. O enunciado trar√° informa√ß√µes adicionais necess√°rias para a resolu√ß√£o da quest√£o e as respostas dever√£o ser apresentadas no moodle. O notebook com as an√°lises desenvolvidas dever√° ser adicionado em upload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 04, 05, 06, 07 e Avalia√ß√£o Final - Estatistica Ciencia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='.\\img\\estimacao-intervalar.jpg'>\n",
    "<img src='.\\img\\hipotese.jpg'>\n",
    "<img src='.\\img\\hipotese2.jpg'>\n",
    "<img src='.\\img\\unicaudal.jpg'>\n",
    "<img src='.\\img\\bicaudal.jpg'>\n",
    "\n",
    "\n",
    "#### Conceitos:\n",
    "Estimativa: Valor do estimador calculado com os dados de uma amostra\n",
    "\n",
    "Distribui√ß√£o Normal: Distribui√ß√£o de probabilidades de um estimador\n",
    "\n",
    "Erro padr√£o: Medida da variabilidade de um estimador\n",
    "\n",
    "Par√¢metro: Quantidade, normalmente desconhecida, que especifica uma distribui√ß√£o de probabilidades na popula√ß√£o\n",
    "\n",
    "Estimador: Fun√ß√£o da amostra que representa valores plaus√≠veis para o par√¢metro desconhecido de interesse\n",
    "\n",
    "Fun√ß√£o de Verossimilhan√ßa: √â um produt√≥rio de uma fun√ß√£o de dsitribui√ß√£o de probabilidade (que pode ser qq uma, bernoulli, poisson, normal, etc). Cada uma ter√° sua f√≥rmula especifica. Precisamos maximizar essa fun√ß√£o de verossimilhan√ßa para podermos obter os melhores par√¢metros para o nosso estimador, gerando a melhor estimativa. Porque estimativa? Pq n√£o conhecemos todos os par√¢metros dos nossos dados, temos apenas uma amostra. \n",
    "\n",
    "Infer√™ncia Estat√≠stica: a id√©ia √© que a partir dessa amostra conseguimos ESTIMAR o todo. Para maximizar, derivamos e igualamos a zero. E para saber o qu√£o confi√°vel √© essa estimativa, n√≥s encontramos o intervalo de confian√ßa. Ele √© a amplitude (maior menos menor) da curva de distribui√ß√£o.\n",
    "\n",
    "#### Teste de hip√≥teses\n",
    "\n",
    "Passos de um teste de hip√≥teses\n",
    "\n",
    "‚óè Especificar (em termos dos par√¢metros) as hip√≥teses H0 e Ha\n",
    "\n",
    "‚óè Especificar a estat√≠stica do teste e sua distribui√ß√£o, sob H0\n",
    "\n",
    "‚óè Fixar o n√≠vel de signific√¢ncia do teste (Œ±)\n",
    "\n",
    "‚óè Calcular o p-valor (ou a regi√£o cr√≠tica do teste)\n",
    "\n",
    "‚óè Decidir entre H0e Ha, comparando o p-valor com Œ± (ou verificando se a estat√≠stica do teste pertence ou n√£o √† regi√£o cr√≠tica)\n",
    "\n",
    "Para o n√≠vel descritivel do p-valor, temos: \n",
    "p-valor > alpha: decide por Ho, \n",
    "p-valor <= alpha: decide por Ha\n",
    "\n",
    "Erro do tipo II: √© quando aceitamos a hip√≥tese nula, sendo a hip√≥tese alternativa verdadeira\n",
    "\n",
    "Poder do teste: pi (poder do teste) = 1 - beta, sendo beta √© a Probabilidade(erro tipo II). O alpha √© a Probabilidade(erro tipo I). O objetivo √© fixar alpha para diminuir ao m√°ximo o erro do tipo I que √© mais cr√≠tico, por√©m, para somar 100% ainda temos que considerar as probabilidades sem erros, portanto, 100% seria: P(erro tipo I) + P(erro tipo II) + P(sem erros para H0) + P(sem erros para Ha)\n",
    "\n",
    "A bicaudal leva em considera√ß√£o um hip√≥tese que pode ocorrer dos dois lados, com intervalos tanto > quanto <, portanto, \n",
    "cabe numa situa√ß√£o em que H0 √© igual e Ha √© diferente. Portanto a afirmativa √© verdadeira.\n",
    "\n",
    "alpha (nivel de significancia) = P(erro tipo I) = probabilidade de escolhermos Ha, sendo H0 verdadeira.\n",
    "\n",
    "Aumentar ou diminuir o alpha n√£o influ√™ncia no resultado do p-valor.\n",
    "\n",
    "\n",
    "\n",
    "#### Estat√≠stica na pr√°tica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estat√≠stica na pr√°tica:\n",
    "\n",
    "#funcoes\n",
    "def fatorial(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1 \n",
    "    else:\n",
    "        return n * fatorial(n - 1) \n",
    "    \n",
    "def combinacao(n,x):\n",
    "    return (fatorial(n))/(fatorial(x)*fatorial(n-x))\n",
    "\n",
    "def binomial(n,p,x):\n",
    "    return combinacao(n,x)*(p**x)*((1-p)**(n-x))\n",
    "\n",
    "def bernoulli(p,x):\n",
    "    return (p**x)*((1-p)**(1-x))\n",
    "\n",
    "#exemplo de aplica√ß√£o:\n",
    "#P(X>=4) = 1 - P(X<=3)\n",
    "#P(X<=3) = P(X=0) + P(X=1) + P(X=2) + P(X=3)\n",
    "P_X_menor_igual_3 = binomial(10,0.2,0) + binomial(10,0.2,1) + binomial(10,0.2,2) + binomial(10,0.2,3)\n",
    "P_X_maior_igual_4 = 1 - P_X_menor_igual_3\n",
    "\n",
    "#Esperan√ßa e vari√¢ncia:\n",
    "#Binomial\n",
    "E(X) = np\n",
    "V(X) = np.(1-p)\n",
    "\n",
    "#Poisson\n",
    "E(Y) = np = lambda\n",
    "V(Y) = np = lambda\n",
    "\n",
    "#Testes para verificar normalidade da distribui√ß√£o:\n",
    "\n",
    "#Teste de Kolmogorov-Smirnov:\n",
    "ks_stat_homem, ks_p_homem = kstest(peso_homem,'norm',args=(peso_homem.describe()[1],peso_homem.describe()[2]),N=len(peso_homem))\n",
    "ks_stat_mulher, ks_p_mulher = kstest(peso_mulher,'norm',args=(peso_mulher.describe()[1],peso_mulher.describe()[2]),N=len(peso_mulher))\n",
    "#para n√∫mero da amostra > 50:\n",
    "ks_val_homem = 1.36/(np.sqrt(len(peso_homem)))\n",
    "ks_val_mulher = 1.36/(np.sqrt(len(peso_mulher)))\n",
    "ks_res_homem = 'Distribui√ß√£o √© normal' if ks_val_homem >= ks_stat_homem else 'Distribui√ß√£o N√ÉO √© normal'\n",
    "ks_res_mulher = 'Distribui√ß√£o √© normal' if ks_val_mulher >= ks_stat_mulher else 'Distribui√ß√£o N√ÉO √© normal'\n",
    "\n",
    "\n",
    "#Teste Anderson-Darling:\n",
    "ad_stat_homem, ad_p_homem, ad_ic_homem = anderson(peso_homem,'norm')\n",
    "ad_stat_mulher, ad_p_mulher, ad_ic_mulher = anderson(peso_mulher,'norm')\n",
    "#para ic 5%\n",
    "ad_res_homem = 'Distribui√ß√£o √© normal' if ad_stat_homem < ad_p_homem[2] else 'Distribui√ß√£o N√ÉO √© normal'\n",
    "ad_res_mulher = 'Distribui√ß√£o √© normal' if ad_stat_mulher < ad_p_mulher[2] else 'Distribui√ß√£o N√ÉO √© normal'\n",
    "\n",
    "\n",
    "#Teste de Shapiro-Wilk:\n",
    "#fun√ß√£o shapiro -> √© um teste estat√≠stico considerando que temos uma amostra normal, ou seja, retorna o p-valor para a normalidade\n",
    "#o primeiro valor do shapiro √© o resultado de um teste estat√≠stico e o segundo √© o p-valor, que √© a probabilidade de aceitarmos\n",
    "#ou rejeitarmos uma hip√≥tese baseada nesse teste estat√≠stico feito.\n",
    "sh_stat_homem, sh_p_homem = shapiro(peso_homem)\n",
    "sh_stat_mulher, sh_p_mulher = shapiro(peso_mulher)\n",
    "#para ic 5%\n",
    "sh_res_homem = 'Distribui√ß√£o √© normal' if sh_p_homem > 0.05 else 'Distribui√ß√£o N√ÉO √© normal'\n",
    "sh_res_mulher = 'Distribui√ß√£o √© normal' if sh_p_mulher > 0.05 else 'Distribui√ß√£o N√ÉO √© normal'\n",
    "\n",
    "#Avaliando resultados dos testes de normalidade HOMEM\n",
    "print('Resultados dos testes de normalidade para homem:')\n",
    "print()\n",
    "print('|----------Teste----------|----------Estat√≠stica--------|----------P-Valor----------|----------Resultado (IC 5%)----------|')\n",
    "print('|    Kolmogorov-Smirnov   |','   ',ks_stat_homem,'    |','   ',ks_p_homem,'   |','       ',ks_res_homem,'      |')\n",
    "print('|-------------------------|-----------------------------|---------------------------|-------------------------------------|')\n",
    "print('|    Anderson-Darling     |','   ',ad_stat_homem,'     |','   ',ad_p_homem[2],'                |','       ',ad_res_homem,'      |')\n",
    "print('|-------------------------|-----------------------------|---------------------------|-------------------------------------|')\n",
    "print('|    Shapiro-Wilk         |','   ',sh_stat_homem,'     |','   ',sh_p_homem,'  |','       ',sh_res_homem,'  |')\n",
    "print('|-------------------------|-----------------------------|---------------------------|-------------------------------------|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar se as amostras s√£o independentes ou dependentes\n",
    "\n",
    "#teste Z de depend√™ncia\n",
    "#para este caso vamos considerar peso como uma vari√°vel quantitativa e compar√°-la entre os dois sexos.\n",
    "stat_peso_homem = [np.mean(peso_homem),np.std(peso_homem),np.std(peso_homem)**2]\n",
    "stat_peso_mulher = [np.mean(peso_mulher),np.std(peso_mulher),np.std(peso_mulher)**2]\n",
    "#valor da estat√≠stica do teste\n",
    "Z = (stat_peso_homem[0]-stat_peso_mulher[0])/(math.sqrt((stat_peso_homem[2])/ math.sqrt(len(peso_homem)+len(peso_mulher))))  # estat√≠stiva do teste Z\n",
    "z_pc = norm.ppf(0.05)  #c√°lculo do ponto cr√≠tico (x_corte)\n",
    "z_pvalor = norm.cdf(Z)  #c√°lculo do p-valor\n",
    "z_res = 'Amostras independentes' if z_pvalor > 0.05 else 'Amostras dependentes'\n",
    "print('Resultado:',z_pvalor)\n",
    "print(z_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar a igualde das vari√¢ncias\n",
    "\n",
    "#A hip√≥tese nula √© que as vari√¢ncias s√£o iguais, e a hip√≥tese alternativa √© que as vari√¢ncias n√£o s√£o iguais. \n",
    "#Use o teste de Levene quando os dados forem provenientes de distribui√ß√µes cont√≠nuas, mas n√£o necessariamente normais.\n",
    "#O m√©todo de c√°lculo para o teste de Levene √© uma modifica√ß√£o do procedimento de Levene (Levene, 1960) que foi desenvolvido \n",
    "#por Brown e Forsythe (1974). Esse m√©todo considera as dist√¢ncias das observa√ß√µes da mediana da amostra em vez da m√©dia da \n",
    "#amostra. O uso da mediana da amostra, em vez da m√©dia da amostra, torna o teste mais robusto para amostras de menor dimens√£o \n",
    "#e torna o procedimento assintoticamente livre de distribui√ß√£o. Se o valor de p for menor do que o de seu n√≠vel de Œ±, \n",
    "#rejeite a hip√≥tese nula de que as vari√¢ncias s√£o iguais.\n",
    "\n",
    "#teste de levene\n",
    "lv_stat, lv_p = levene(peso_homem, peso_mulher)\n",
    "lv_res = 'N√£o existe grande diferen√ßa na vari√¢ncia.' if lv_p > 0.05 else 'Existe diferen√ßa na vari√¢ncia.'\n",
    "print('Resultado:',lv_p)\n",
    "print(lv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar hip√≥teses com teste de hip√≥tese\n",
    "\n",
    "#teste t de Student - t-student bicaudal\n",
    "t_stat, t_p = ttest_ind(peso_homem, peso_mulher)\n",
    "t_res = 'Aceita hip√≥tese H0' if t_p > 0.05 else 'Rejeita hip√≥tese H0'\n",
    "print('Resultado:',t_p)\n",
    "print(t_res)\n",
    "\n",
    "#teste unicaudal\n",
    "# Podemos fazer um teste unicaudal com H0: p = 0.9 versus Ha: p > 0.9\n",
    "# Note que, neste caso, o Ha est√° √† direita de H0 e, portanto, o p-valor deve ser calculado a partir da cauda direita\n",
    "n = df.eval('Ano == \"2001-2005\"').sum()\n",
    "p_observado = proporcao.loc['2001-2005']\n",
    "p_valor = 1- stats.norm.cdf(p_observado, .9, np.sqrt(.9 * .1 / n)) # calculado com a cauda da direita\n",
    "p_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar teste se duas amostras tem distribui√ß√µes diferentes:\n",
    "from scipy import stats\n",
    "#usaremos o teste n√£o param√©trico Mann‚ÄìWhitney U\n",
    "#A hip√≥tese nula √© de que as distribui√ß√µes s√£o iguais.\n",
    "stats.mannwhitneyu(inalterado, melhorado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regress√£o linear simples\n",
    "from statsmodels.formula.api import ols\n",
    "#Modelo 1: Tamanho\n",
    "mod1 = ols('Peso ~ Tamanho',data=data)\n",
    "res1 = mod1.fit()\n",
    "print(res1.summary())\n",
    "\n",
    "\n",
    "#regress√£o linear m√∫ltipla\n",
    "\n",
    "#Modelo 2: Tamanho, Homem e Acima45 com todas as intera√ß√µes poss√≠veis\n",
    "mod2 = ols('Peso ~ Tamanho * Homem * Acima45',data=data)\n",
    "res2 = mod2.fit()\n",
    "print(res2.summary())\n",
    "\n",
    "#Modelo 3: Tamanho, Homem, Acima45 e respectivas intera√ß√µes\n",
    "mod3 = ols('Peso ~ Tamanho * Homem + Tamanho * Acima45 + Homem * Acima45',data=data)\n",
    "res3 = mod3.fit()\n",
    "print(res3.summary())\n",
    "\n",
    "#Modelo 4: Tamanho, Homem, Acima45. Intera√ß√µes Tamanho - Acima45 e Homem - Acima45\n",
    "mod4 = ols('Peso ~ Tamanho + Tamanho * Acima45 + Homem * Acima45',data=data)\n",
    "res4 = mod4.fit()\n",
    "print(res4.summary())\n",
    "\n",
    "#Modelo 5: Tamanho, Homem, Acima45. Intera√ß√£o: Tamanho - Acima45.\n",
    "mod5 = ols('Peso ~ Tamanho + Homem + Tamanho * Acima45',data=data)\n",
    "res5 = mod5.fit()\n",
    "print(res5.summary())\n",
    "\n",
    "#Modelo 6: Tamanho, Homem, Acima45 e sem intera√ß√µes\n",
    "mod6 = ols('Peso ~ Tamanho + Homem + Acima45',data=data)\n",
    "res6 = mod6.fit()\n",
    "print(res6.summary())\n",
    "\n",
    "#Modelo 7: Tamanho, Homem, Acima45, sem intera√ß√µes, com Tamanho padronizado\n",
    "#padroniza√ß√£o (vari√°vel menos m√©dia dividido pelo desvio padr√£o)\n",
    "Tamanho_p = (data.Tamanho-np.mean(data.Tamanho))/(np.std(data.Tamanho))\n",
    "mod7 = ols('Peso ~ Tamanho_p + Homem + Acima45',data=data)\n",
    "res7 = mod7.fit()\n",
    "print(res7.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicolinearidade:\n",
    "Voltando com as suspeitas de multicolinearidade identificadas no in√≠cio da an√°lise, iremos utilizar o c√°lculo do fator de infla√ß√£o da vari√¢ncia (VIF) para as vari√°veis explicativas do modelo. O crit√©rio a ser utilizado para an√°lise do VIF ser√°:\n",
    "\n",
    "VIF for igual √† 1 n√£o h√° multicolinearidade entre os fatores;\n",
    "\n",
    "VIF acima de 1, as preditoras podem estar correlacionadas.\n",
    "\n",
    "De 1 at√© 5: indica alguma correla√ß√£o, por√©m, n√£o o suficiente para impactar no modelo;\n",
    "De 5 at√© 10: alta correla√ß√£o podendo gerar impacto no modelo;\n",
    "Acima de 10: coeficientes de regress√£o est√£o mal estimados devidos √† multicolinearidade;\n",
    "\n",
    "Com estes resultados para o VIF, temos a indica√ß√£o que existe alguma correla√ß√£o entre as vari√°veis, por√©m, ela n√£o √© suficiente para impactar no modelo, portanto, vamos considerar esse como sendo o modelo final ajustado.\n",
    "\n",
    "As suposi√ß√µes do nosso modelo ajustado precisam ser validadas para que os resultados sejam confi√°veis, para isso vamos realizar a an√°lise de res√≠duos. A id√©ia por tr√°s √© que se o modelo for apropriado, os res√≠duos devem refletir algumas propriedades:\n",
    "\n",
    "i.  ùúÄùëñ  e  ùúÄùëó  s√£o independentes  (ùëñ‚â†ùëó) ;\n",
    "\n",
    "ii.  ùëâùëéùëü(ùúÄùëñ)=ùúé2  (constante);\n",
    "\n",
    "iii.  ùúÄùëñ‚àºùëÅ(0,ùúé2)  (normalidade);\n",
    "\n",
    "iv. Modelo √© linear;\n",
    "\n",
    "v. N√£o existir outliers (pontos at√≠picos) influentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula o VIF (Infla√ß√£o da Vari√¢ncia)\n",
    "var = res6.model.exog\n",
    "vif = [variance_inflation_factor(var, i) for i in range(var.shape[1])]\n",
    "\n",
    "print('|==================|==============================|')\n",
    "print('|     Vari√°veis    |              VIF             |')\n",
    "print('|------------------|------------------------------|')\n",
    "print('|   Intercept','     |    ',vif[0],'      |',)\n",
    "print('|   Tamanho','       |    ',vif[1],'       |',)\n",
    "print('|   Homem','         |    ',vif[2],'      |',)\n",
    "print('|   Acima45','       |    ',vif[3],'      |',)\n",
    "print('|------------------|------------------------------|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo par√¢metros no modelo 7, modelo final ajustado\n",
    "\n",
    "# valores preditos de E(Y)\n",
    "ypred=res7.fittedvalues\n",
    "\n",
    "# objeto para a an√°lise de pontos influentes\n",
    "infl = res7.get_influence()\n",
    "\n",
    "# diagonal da matriz hat\n",
    "hii = infl.hat_matrix_diag\n",
    "\n",
    "# res√≠duo studentizado (internamente)\n",
    "res_stud = infl.resid_studentized_internal\n",
    "\n",
    "# res√≠duo studentizado com i-√©sima observa√ß√£o deletada (externamente)\n",
    "res_stud_del = infl.resid_studentized_external\n",
    "\n",
    "# DFFITS\n",
    "(dffits,k) = infl.dffits\n",
    "\n",
    "# Dist√¢ncia de Cook\n",
    "(cook,p) = infl.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagn√≥stico de independ√™ncia\n",
    "\n",
    "#Este teste serve para detectar depend√™ncia nos res√≠duos de uma an√°lise de regress√£o. \n",
    "#A estat√≠stica do teste sempre ir√° varias entre 0 e 4. Quanto mais pr√≥ximo de 0, maior a evid√™ncia de uma correla√ß√£o \n",
    "#positiva e quanto mais pr√≥xima de 4, correla√ß√£o negativa. Uma estat√≠stica pr√≥xima de 2, indica que n√£o temos correla√ß√£o \n",
    "#nos res√≠duos, ou seja, s√£o independentes.\n",
    "\n",
    "#teste de durbin watson\n",
    "print('Estat√≠stica de Durbin Watson:',durbin_watson(res7.resid,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagn√≥stico de homoscedasticidade (vari√¢ncia constante)\n",
    "\n",
    "plt.scatter(ypred,res_stud) # gr√°fico dos res√≠duos versus valores ajustados (valores preditos)\n",
    "plt.ylabel('res√≠duo studentizado')\n",
    "plt.xlabel('valor ajustado')\n",
    "plt.hlines(0,xmin=min(ypred),xmax=max(ypred),color='gray')\n",
    "print('Observa√ß√µes mais cr√≠ticas:')\n",
    "print(df.index[res_stud<-3],df.index[res_stud>3]) # identifica as observa√ß√µes mais cr√≠ticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagn√≥stico de normalidade\n",
    "\n",
    "probplot(res_stud, plot=plt)\n",
    "plt.xlabel('quantis te√≥ricos')\n",
    "plt.ylabel('res√≠duos ordenados')\n",
    "plt.show()\n",
    "print(data.index[res_stud<-2],data.index[res_stud>2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pontos influentes (DFFITS e dist√¢ncia de cook)\n",
    "\n",
    "#dffits\n",
    "plt.scatter(data.index, dffits)\n",
    "plt.ylabel('DFFITS')\n",
    "plt.hlines(0,xmin=1,xmax=102,color='gray')\n",
    "plt.xlabel('√≠ndice')\n",
    "print(df.index[dffits>1]) # valores que est√£o acima de 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O DFFITS mede a influ√™ncia que a observa√ß√£o i tem sobre seu pr√≥prio valor ajustado. Uma observa√ß√£o √© um ponto influente, se:\n",
    "* $ |DFFITS_{(i)}|~>~ 1 $, para amostras pequenas ou m√©dias\n",
    "* $ |DFFITS_{(i)}|~>~ 2\\sqrt{(p+1)/n} $, para amostras grandes, no qual $ (p+1) $ √© o n√∫mero de par√¢metros.\n",
    "\n",
    "Neste caso, n√£o temos valores que est√£o acima de 1, portanto, n√£o temos pontos influentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist√¢ncia de cook\n",
    "plt.scatter(data.index, cook)\n",
    "plt.ylabel('dist√¢ncia de Cook')\n",
    "plt.xlabel('√≠ndice')\n",
    "print(data.index[cook>f.ppf(.5, 5, 126)]) # valores que est√£o acima do percentil 50 de uma distribui√ß√£o F(p,n-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dist√¢ncia de Cook mede a influ√™ncia da observa√ß√£o i sobre todos n valores ajustados. Uma observa√ß√£o √© um ponto influente, se:\n",
    "* $ D_i~>~1 $.\n",
    "\n",
    "Neste caso, n√£o temos valores que est√£o acima de 1, portanto, n√£o temos pontos influentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagonal da matriz chap√©u\n",
    "plt.scatter(data.index, hii)\n",
    "plt.ylabel('$h_{ii}$')\n",
    "plt.xlabel('√≠ndice')\n",
    "print(data.index[hii>.1])   # h_ii>2p/n (neste caso, .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diagonal da matriz chap√©u $ H $ √© uma medida padronizada da dist√¢ncia da i-√©sima observa√ß√£o para o centro do espa√ßo definido pelas vari√°veis explicativas. Uma observa√ß√£o √© um ponto influente, se:\n",
    "* $ h_{ii} > 2(p+1)/n $\n",
    "\n",
    "Neste caso, n√£o temos ocorr√™ncias destacadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°ficos dos res√≠duos\n",
    "sm.graphics.influence_plot(res7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 03, 04 e Avalia√ß√£o Final - Aprendizado Dinamico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O que √© uma s√©rie temporal?\n",
    "Uma s√©rie temporal √© qualquer conjunto de observa√ß√µes ordenadas no tempo.\n",
    "\n",
    "S√£o objetivos gerais dos estudos de s√©ries temporais:\n",
    "\n",
    "‚Ä¢ Identificar padr√µes como tend√™ncia, sazonalidade, observa√ß√µes discrepantes (outliers);\n",
    "\n",
    "‚Ä¢ Usar a varia√ß√£o passada de uma s√©rie para predizer valores futuros. Embora n√£o seja poss√≠vel prever exatamente os valores futuros, podemos predizer um comportamento aproximado das pr√≥ximas observa√ß√µes;\n",
    "\n",
    "‚Ä¢ Entender a varia√ß√£o conjunta de duas s√©ries, e utilizar uma s√©rie para explicar a varia√ß√£o em\n",
    "outra s√©rie.\n",
    "\n",
    "#### Tend√™ncia\n",
    "\n",
    "<img src='.\\img\\tendencia.jpg'>\n",
    "\n",
    "#### Sazonalidade\n",
    "Comportamento da s√©rie temporal tende a se repetir a cada s per√≠odos de tempo.\n",
    "\n",
    "<img src='.\\img\\sazonalidade.jpg'>\n",
    "\n",
    "Aditiva. A s√©rie apresenta flutua√ß√µes sazonais mais ou menos constantes n√£o importando o\n",
    "n√≠vel global da s√©rie.\n",
    "\n",
    "Multiplicativa. O tamanho das flutua√ß√µes sazonais varia dependendo do n√≠vel global da\n",
    "s√©rie.\n",
    "\n",
    "M√©todos para estimar a sazonalidade:\n",
    "\n",
    "M√©todo da regress√£o (m√©todo determin√≠stico) em que as covari√°veis s√£o vari√°veis peri√≥dicas, por exemplo seno, cosseno ou vari√°veis ‚Äòdummy‚Äô, que s√£o vari√°veis indicadoras\n",
    "\n",
    "M√©dias m√≥veis (m√©todo estoc√°stico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decomposi√ß√£o dos dados de mortes em tend√™ncia e sazonalidade:\n",
    "decomposicao = seasonal_decompose(data[data['deaths']>0]['deaths'],model='multiplicative', period=7) #filtrando mortes > 0\n",
    "fig = decomposicao.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### An√°lise de tend√™ncia:\n",
    "        √â poss√≠vel observar uma tend√™ncia crescente no per√≠odo de abril. A curva segue depois com uma tend√™ncia decrescente at√© meados de junho. No final de junho, temos um pequeno per√≠odo de eleva√ß√£o, seguido logo de queda. Depois desse  per√≠odo a tend√™ncia fica muito menos evidente, praticamente sugerindo uma estacionaridade, por√©m, os testes de Dickey-Fuller realizados anteriormente com p-valor maior que 0.05, sugere que rejeitemos a hip√≥tese de estacionariedade na s√©rie.\n",
    "\n",
    "###### An√°lise de sazonalidade:\n",
    "        Podemos observar ciclos semanais que provavelmente s√£o explicadas pela sistem√°tica de notifica√ß√µes, onde os dados ficam acumulados para c√°lculo no √≠nicio da semana e s√£o sumarizados e apresentados mais para o meio da semana. Com rela√ß√£o ao tipo de sazonalidade, o gr√°fico apresenta algumas varia√ß√µes ao longo da s√©rie temporal, principalmente nos per√≠odos de tend√™ncia crescente, portanto, consideramos mais adequada representarmos a sazonalidade como sendo do tipo  multiplicativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gr√°ficos de autocorrela√ß√£o e autocorrela√ß√£o parcial\n",
    "fig = plot_acf(data['deaths'],title='Autocorrela√ß√£o: Deaths',lags=50)\n",
    "fig = plot_pacf(data['deaths'],title='Autocorrela√ß√£o Parcial: Deaths',lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### An√°lise de autocorrela√ß√£o:\n",
    "        Podemos observar que a correla√ß√£o entre a s√©rie original at√© a s√©rie com atraso lag=16 s√£o bastante significativas  pois encontram-se maiores que o intervalo de confian√ßa (√°rea azul), portanto, precisamos lev√°-las em considera√ß√£o  durante a modelagem. Notamos tamb√©m que h√° presen√ßa de sazonalidade nos dados, apresentando vales/picos em ciclos de 7 dias.\n",
    "        \n",
    "###### An√°lise de autocorrela√ß√£o parcial:  \n",
    "        Podemos verificar que a partir do lag=3 a correla√ß√£o cai bastante, apresentando valores por volta 0.4. Mesmo as  correla√ß√µes n√£o sendo t√£o altas, elas ainda s√£o importantes para o modelo, pois apresentam-se fora do intervalo de  confian√ßa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#m√©dia movel simples\n",
    "passageiros['MMS-6-meses'] = passageiros['Milhares de passageiros'].rolling(window=6).mean()\n",
    "passageiros['MMS-12-meses'] = passageiros['Milhares de passageiros'].rolling(window=12).mean()\n",
    "\n",
    "#mediana m√≥vel\n",
    "passageiros['MedMS-6-meses'] = passageiros['Milhares de passageiros'].rolling(window=6).median()\n",
    "passageiros['MedMS-12-meses'] = passageiros['Milhares de passageiros'].rolling(window=12).median()\n",
    "\n",
    "#m√©dia m√≥vel exponencialmente ponderada (MMEP)\n",
    "passageiros['MMEP12'] = passageiros['Milhares de passageiros'].ewm(span=12,adjust=False).mean()\n",
    "\n",
    "#DICA:\n",
    "#comparando MMS com MMEP\n",
    "passageiros[['Milhares de passageiros','MMS-12-meses','MMEP12']].plot(figsize=(12,9)).autoscale(axis='x',tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses m√©todos n√£o levam em considera√ß√£o que a s√©rie tem uma componente de tend√™ncia.\n",
    "\n",
    "As m√©dias m√≥veis simples t√™m algumas desvantagens:\n",
    "\n",
    "‚Ä¢ Janelas menores levar√£o a mais ru√≠do, em vez de sinal\n",
    "\n",
    "‚Ä¢ Ele nunca alcan√ßar√° o pico ou vale m√°ximo dos dados devido ao c√°lculo da m√©dia.\n",
    "\n",
    "‚Ä¢ N√£o informa sobre poss√≠veis comportamentos futuros, tudo o que realmente faz √© descrever tend√™ncias em seus dados.\n",
    "\n",
    "‚Ä¢ Valores hist√≥ricos extremos podem distorcer significativamente a MM\n",
    "\n",
    "Uma poss√≠vel proposta para contornar esses problemas √© a MMEP (m√©dia m√≥vel exponencialmente ponderada).\n",
    "\n",
    "A MMEP permite reduzir o efeito de atraso da MMS e dar√° mais peso aos valores que ocorreram mais recentemente (aplicando mais peso aos valores mais recentes, portanto, o nome). A quantidade de peso aplicada aos valores mais recentes depender√° dos par√¢metros reais usados na MMEP e do n√∫mero de per√≠odos determinados pelo tamanho da janela.\n",
    "\n",
    "Span corresponde ao que √© comumente chamado uma ‚Äúm√©dia m√≥vel exponencialmente\n",
    "ponderada em N dias‚Äù.\n",
    "\n",
    "‚Ä¢ Centro de massa tem uma interpreta√ß√£o f√≠sica e pode ser pensado em termos do span: c =\n",
    "(s ‚àí 1)/2\n",
    "\n",
    "‚Ä¢ Meia-vida √© o per√≠odo de tempo para o peso exponencial se reduzir pela metade.\n",
    "\n",
    "‚Ä¢ Alpha especifica o fator de suaviza√ß√£o diretamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m√©todos de suaviza√ß√£o holt e holt-winters para a vari√°vel mortes:\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "\n",
    "#treinamento e predi√ß√£o pelo m√©todo de Holt\n",
    "adjustH = ExponentialSmoothing(train[train['deaths']>0]['deaths'],trend='mul').fit() #filtro de mortes > 0\n",
    "predictH = adjustH.forecast(21).rename('Previs√£o Holt')\n",
    "predictH.index = data.index[226:]\n",
    "\n",
    "#treinamento e predi√ß√£o pelo m√©todo de Holt-Winters\n",
    "adjustHW = ExponentialSmoothing(train[train['deaths']>0]['deaths'],trend='mul',\n",
    "                                seasonal='mul',seasonal_periods=7).fit() #filtro de mortes > 0\n",
    "predictHW = adjustHW.forecast(21).rename('Previs√£o Holt-Winters')\n",
    "predictHW.index = data.index[226:]\n",
    "\n",
    "#DICA:\n",
    "#imprimindo dados com a predi√ß√£o realizada pelo m√©todo de holt e holt-winters\n",
    "test['holt'] = predictH\n",
    "test['holt-winters'] = predictHW\n",
    "\n",
    "train['deaths'].plot(legend=True,label='Treino',title='Predi√ß√£o do modelo pelo m√©todo de Holt e Holt-Winters')\n",
    "test['deaths'].plot(legend=True,label='Teste',figsize=(16,6))\n",
    "predictH.plot(legend=True,label='Previs√£o Holt')\n",
    "fig = predictHW.plot(legend=True,label='Previs√£o Holt-Winters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vantagens e desvantagens m√©todos de suaviza√ß√£o\n",
    "\n",
    "##### M√©dia m√≥vel simples:\n",
    "\n",
    "‚Ä¢ Simples implementa√ß√£o;\n",
    "\n",
    "‚Ä¢ Aplic√°vel mesmo com um n√∫mero pequeno de observa√ß√µes;\n",
    "\n",
    "‚Ä¢ Flexibilidade de acordo com o tamanho da janela\n",
    "\n",
    "Por√©m\n",
    "\n",
    "‚Ä¢ M√©todo descritivo e n√£o recomend√°vel para previs√µes\n",
    "\n",
    "‚Ä¢ N√£o leva em considera√ß√£o a componente de tend√™ncia na s√©rie\n",
    "\n",
    "\n",
    "##### M√©dia m√≥vel exponencialmente ponderada\n",
    "\n",
    "‚Ä¢ F√°cil compreens√£o e aplicabilidade;\n",
    "\n",
    "‚Ä¢ Necessidade de armazenar somente Zt, Zt e Œ±\n",
    "\n",
    "Por√©m\n",
    "\n",
    "‚Ä¢ M√©todo descritivo e n√£o recomend√°vel para previs√µes\n",
    "\n",
    "‚Ä¢ N√£o leva em considera√ß√£o a componente de tend√™ncia na s√©rie\n",
    "\n",
    "##### M√©todo de Holt\n",
    "‚Ä¢ Pode ser usado para prever s√©ries que tem tend√™ncia\n",
    "\n",
    "‚Ä¢ Apresenta um n√≠vel maior de dificuldade para encontrar os valores apropriados para as constantes de suaviza√ß√£o A e C, em geral que minimizem a soma dos quadrados dos errosdos ajustes\n",
    "\n",
    "##### M√©todo de Holt-Winters\n",
    "‚Ä¢ Pode ser usado para prever s√©ries que tem tend√™ncia e sazonalidade, seja ela aditiva ou\n",
    "multiplicativa\n",
    "\n",
    "Por√©m\n",
    "\n",
    "‚Ä¢ Impossibilidade e dificuldade para estudar propriedades estat√≠sticas como m√©dia e vari√¢ncia\n",
    "das previs√µes, e consequentemente constru√ß√£o de intervalos de confian√ßa para as previs√µes\n",
    "\n",
    "‚Ä¢ N√≠vel maior de dificuldade para encontrar os valores apropriados para as constantes de\n",
    "suaviza√ß√£o A, C e D, em geral que minimizem a soma dos quadrados dos erros dos ajustes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estacionaridade\n",
    "\n",
    "Uma s√©rie temporal √© estacion√°ria se a sua m√©dia, vari√¢ncia e autocovari√¢ncia s√£o fixas para quaisquer dois pontos equidistantes. Isso significa que, independente de onde tomarmos um subconjunto da s√©rie, a m√©dia, vari√¢ncia, autocorrela√ß√£o devem se manter constantes.\n",
    "\n",
    "‚Ä¢ Uma s√©rie que apresenta sazonalidade ou tend√™ncia n√£o √© estacion√°ria.\n",
    "\n",
    "O Teste de Dickey-Fuller √© usado para testar estacionariedade em um contexto de modelos autorregressivos.\n",
    "\n",
    " -> p-valor maior que 0.05, sugere que rejeitemos a hip√≥tese de estacionariedade na s√©rie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando estacionaridade - primeira e segunda diferen√ßa\n",
    "\n",
    "#testando a estacionariedade com Dickey-Fuller\n",
    "result = adfuller(data['deaths'], autolag='AIC')\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# s√©ries de m√©dia e desvio-padr√£o m√≥vel com janela de 12 meses\n",
    "passageiros['MMS-12'] = passageiros['Milhares de passageiros'].rolling(window=12).mean()\n",
    "passageiros['DP-12'] = passageiros['Milhares de passageiros'].rolling(window=12).std()\n",
    "passageiros[['Milhares de passageiros','MMS-12','DP-12']].plot();\n",
    "\n",
    " # Primeiras diferen√ßas\n",
    "y = np.diff(passageiros['Milhares de passageiros'])\n",
    "x = passageiros.iloc[1:].index\n",
    "plt.plot(x,y);\n",
    "\n",
    "# Segundas diferen√ßas\n",
    "y2 = np.diff(y)\n",
    "x2 = x[1:]\n",
    "plt.plot(x2,y2);\n",
    "\n",
    "#boxplot por perido\n",
    "import seaborn as sns\n",
    "passageiros['Ano'] = passageiros.index.year\n",
    "sns.boxplot(x=passageiros['Ano'], y=passageiros['Milhares de passageiros'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fun√ß√£o Autocovari√¢ncia e autocorrela√ß√£o\n",
    "\n",
    "Antes de falar de autocovari√¢ncia e autocorrela√ß√£o, o que √© covari√¢ncia e correla√ß√£o?\n",
    "\n",
    "Basicamente, a covari√¢ncia √© uma medida de variabilidade conjunta entre duas vari√°veis aleat√≥rias. Ela mede a for√ßa da associa√ß√£o linear entre essas duas vari√°veis. E a correla√ß√£o √© essa medida de associa√ß√£o linear padronizada, de forma que assuma valores entre -1 e 1. O sinal da covari√¢ncia e da correla√ß√£o indica se as vari√°veis se associam de forma positiva ou\n",
    "negativa.\n",
    "\n",
    "Correla√ß√£o n√£o significa causalidade!\n",
    "\n",
    "\n",
    "A autocorrela√ß√£o, tamb√©m conhecida como correla√ß√£o serial, √© a correla√ß√£o de um sinal com uma c√≥pia atrasada de si mesma em fun√ß√£o do atraso (lag). Informalmente, √© a semelhan√ßa entre as observa√ß√µes em fun√ß√£o do intervalo de tempo entre elas.\n",
    "\n",
    "O correlograma √© uma representa√ß√£o das autocorrela√ß√µes entre as observa√ß√µes da s√©rie temporal. Ou seja, cada ponto do gr√°fico representa a correla√ß√£o entre a s√©rie original e a s√©rie com o atraso correspondente.\n",
    "\n",
    "A autocorrela√ß√£o parcial √© uma medida de associa√ß√£o linear de duas vari√°veis ap√≥s remover o efeito de outras vari√°veis que afetam ambas. Na pr√°tica, modelos lineares s√£o ajustados para a s√©rie ‚Äúcorrente‚Äù com a s√©rie em atraso como preditor, e os res√≠duos desse modelo s√£o utilizados para o pr√≥ximo passo, calcula-se a correla√ß√£o entre os res√≠duos e a pr√≥xima s√©rie em atraso e assim por diante.\n",
    "\n",
    "<img src='.\\img\\autocorrelacao.jpg'>\n",
    "\n",
    "\n",
    "#### Fun√ß√£o de autocorrela√ß√£o (fac) e fun√ß√£o de autocorrela√ß√£o parcial (facp) para processos AR, MA, ARMA\n",
    "\n",
    "Fun√ß√£o de autocorrela√ß√£o (fac):\n",
    "\n",
    "1. Um processo AR(p) tem fac que decai de acordo com exponenciais ou senoides amortecidas,\n",
    "infinita em extens√£o;\n",
    "\n",
    "2. Um processo MA(q) tem fac finita, no sentido de que ela apresenta um corte ap√≥s o ‚Äúlag‚Äù q;\n",
    "\n",
    "3. Um processo ARMA (p,q) tem fac infinita em extens√£o, a qual decai de acordo com exponenciais e/ou senoides amortecidas ap√≥s o ‚Äúlaq‚Äù q-p.\n",
    "\n",
    "\n",
    "A fun√ß√£o de autocorrela√ß√£o parcial tamb√©m pode auxiliar na identifica√ß√£o do modelo. Entre outras\n",
    "caracter√≠sticas,\n",
    "\n",
    "1. Um processo MA(q) tem facp que se comporta de maneira similar √† fac de um processo\n",
    "AR(p), com decaimento exponencial e/ou senoides amortecidas;\n",
    "\n",
    "2. Um processo ARMA(p,q) tem facp que se comporta como a facp de um processo MA puro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√µes para c√°lculo da autocorrela√ß√£o e autocorrela√ß√£o parcial\n",
    "from statsmodels.tsa.stattools import acovf, acf, pacf, pacf_yw, pacf_ols\n",
    "acf(df1['Milhares de passageiros'])\n",
    "pacf(df1['Milhares de passageiros'])\n",
    "pacf(df1['Milhares de passageiros'], method='ols')\n",
    "pacf_ols(df1['Milhares de passageiros'])\n",
    "\n",
    "#representa√ß√£o grafica autocorrela√ß√£o\n",
    "from pandas.plotting import lag_plot\n",
    "lag_plot(df1['Milhares de passageiros']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos ARIMA\n",
    "\n",
    "As etapas s√£o as mesmas que para o ARMA (p, q), exceto que aplicaremos um componente diferencial para tornar o conjunto de dados estacion√°rio. \n",
    "\n",
    "Componentes de um modelo ARIMA (p,d,q):\n",
    "\n",
    "‚Ä¢ AR (p): Componentes autorregressivas, utilizam a rela√ß√£o de depend√™ncia entre a observa√ß√£o corrente e as observa√ß√µes em um per√≠odo pr√©vio. O termo autoregress√£o descreve uma regress√£o da vari√°vel contra ela mesma. Uma regress√£o autom√°tica √© executada em um conjunto de valores defasados da ordem p.\n",
    "\n",
    "‚Ä¢ Integrado (d): Diferen√ßas para tornar a s√©rie estacion√°ria\n",
    "\n",
    "‚Ä¢ MA (q): Componentes de m√©dias m√≥veis, utilizam a depend√™ncia entre uma oberva√ß√£o e\n",
    "um erro residual de um modelo de m√©dia m√≥vel aplicado a observa√ß√µes em atraso.\n",
    "\n",
    "Os modelos ARMA podem ser usado para s√©ries estacion√°rias se as ra√≠zes de œÜ(B) = 0 ca√≠rem todas fora do c√≠rculo unit√°rio.\n",
    "Para s√©ries n√£o-estacion√°rias com uma componente de tend√™ncia, os modelos ARIMA podem ser mais adequados.\n",
    "\n",
    "#### Modelos SARIMA\n",
    "\n",
    "Modelos SARIMA (p,d,q)x(P, D,Q)m\n",
    "‚Ä¢ SARIMA: ARIMA com sazonalidade\n",
    "\n",
    "Componentes de um modelo SARIMA (p,d,q)x(P,D,Q)m:\n",
    "‚Ä¢ (p, d, q): componentes n√£o-sazonais\n",
    "‚Ä¢ (P, D, Q)m: componentes sazonais\n",
    "\n",
    "\n",
    "Uma estrat√©gia para a constru√ß√£o do modelo ser√° baseada em um ciclo iterativo, na qual a escolha da estrutura do modelo √© baseada nos pr√≥prios dados:\n",
    "\n",
    "1. Uma classe geral de modelos √© considerada para a an√°lise, no caso modelos ARIMA\n",
    "(especifica√ß√£o)\n",
    "\n",
    "2. H√° identifica√ß√£o do modelo, com base na an√°lise de autocorrela√ß√µes, autocorrela√ß√µes parciais e outros crit√©rios\n",
    "\n",
    "3. Estima√ß√£o dos par√¢metros do modelo identificado.\n",
    "\n",
    "4. Verifica√ß√£o ou diagn√≥stico do modelo ajustado, por meio de uma an√°lise de res√≠duos, para saber se esse modelo √© adequado para fazer previs√£o, por exemplo\n",
    "\n",
    "Se o modelo n√£o for adequado, as estapas 2, 3 e 4 se repetem at√© obter um ajuste satisfat√≥rio. A\n",
    "etapa mais trabalhosa √© a identifica√ß√£o.\n",
    "\n",
    "\n",
    "###### Descri√ß√£o do modelo selecionado: SARIMAX(3, 1, 3)x(1, 0, 0, 7)\n",
    "\n",
    "    - Componente n√£o sazonal autorregressiva de ordem 3\n",
    "    - Componente n√£o sazonal integrado de ordem 1\n",
    "    - Componente n√£o sazonal de m√©dias m√≥veis de ordem 3\n",
    "    - Componente sazonal autorregressiva de primeira ordem com per√≠odo 7\n",
    "    - Sem componente sazonal integrada\n",
    "    - Sem componente sazonal de m√©dia m√≥vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicando stepwise para selecionar o melhor modelo SARIMA\n",
    "auto_arima(train['deaths'],seasonal=True,m=7).summary()\n",
    "\n",
    "stepwise_fit = auto_arima(train['deaths'], start_p=0, start_q=0,max_p=6,max_q=3,m=7,\n",
    "                          seasonal=True,\n",
    "                          trace=True,\n",
    "                          error_action='ignore',\n",
    "                          supress_warnings=True,\n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()\n",
    "\n",
    "#predi√ß√£o utilizando o modelo SARIMA identificado pelo stepwise\n",
    "start = len(train)\n",
    "end = len(train) + len(test)-1\n",
    "predict_SARIMA = adjustSARIMA.predict(start=start,end=end,\n",
    "                                      dynamic=False,typ='levels').rename('Previs√µes SARIMA(3, 1, 3)x(1, 0, 0, 7)')\n",
    "predict_SARIMA.index = test.index\n",
    "\n",
    "\n",
    "#DICA:\n",
    "#imprimindo dados com a predi√ß√£o realizada pelo SARIMA\n",
    "train['sarima'] = adjustSARIMA.fittedvalues\n",
    "test['sarima'] = predict_SARIMA\n",
    "\n",
    "train['deaths'].plot(legend=True,label='Treino',title='Predi√ß√£o do modelo SARIMA(3, 1, 3)x(1, 0, 0, 7)')\n",
    "test['deaths'].plot(legend=True,label='Teste',figsize=(16,6))\n",
    "fig = predict_SARIMA.plot(legend=True,label='Previs√£o SARIMA(3, 1, 3)x(1, 0, 0, 7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Res√≠duos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma forma de obter os res√≠duos pelo ajuste do modelo\n",
    "res√≠duos = resultados.resid\n",
    "res√≠duos.describe()\n",
    "plt.boxplot(res√≠duos)\n",
    "plot_acf(res√≠duos, lags=30)\n",
    "plot_pacf(res√≠duos, lags=30)\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "res√≠duos.hist()\n",
    "pyplot.show()\n",
    "res√≠duos.plot(kind='kde')\n",
    "pyplot.show()\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "qqplot(res√≠duos)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vari√°veis end√≥genas e ex√≥genas\n",
    "\n",
    "Em modelos econ√¥micos e econom√©tricos, uma vari√°vel ex√≥gena refere-se a uma vari√°vel que √© determinada fora do modelo e representa as entradas de um modelo. Em outras palavras, vari√°veis ex√≥genas s√£o fixadas no momento em que s√£o introduzidas no modelo. Em contraste, vari√°veis end√≥genas s√£o determinadas dentro do modelo e, portanto, representam as sa√≠das de um modelo. O modelo especificado com as vari√°veis mostra como a mudan√ßa de uma vari√°vel ex√≥gena coeteris paribus afeta todas as vari√°veis end√≥genas.\n",
    "\n",
    "Geralmente tamb√©m vari√°veis explicativas n√£o s√£o perturba√ß√µes de uma fun√ß√£o de regress√£o, mas est√£o correlacionados, sendo por isso chamadas de vari√°veis ex√≥genas.\n",
    "\n",
    "Nos modelos de desigualdade, os termos \"vari√°vel declarada (regressiva) - vari√°vel end√≥gena\", bem como \"vari√°vel explicativa (regressor) - vari√°vel ex√≥gena\" se aplicam. Em contraste, em modelos de multi-equa√ß√£o, as vari√°veis end√≥genas podem ser regressores e regress√µes de areia; vari√°veis ex√≥genas, no entanto, apenas nos regressores.\n",
    "\n",
    "#### Previs√£o Bayesiana\n",
    "\n",
    "Os modelos de s√©ries temporais estruturais (STS) s√£o vistos como uma fam√≠lia de modelos de\n",
    "probabilidade que incluem\n",
    "\n",
    "‚Ä¢ processos autorregressivos,\n",
    "\n",
    "‚Ä¢ m√©dias m√≥veis,\n",
    "\n",
    "‚Ä¢ tend√™ncias lineares locais,\n",
    "\n",
    "‚Ä¢ sazonalidade e\n",
    "\n",
    "‚Ä¢ regress√£o e sele√ß√£o de vari√°veis em covari√°veis externas (outras s√©ries temporais potencialmente relacionadas √†s s√©ries de interesse).\n",
    "\n",
    "\n",
    "De uma forma bem simplificada, um modelo de s√©ries temporais estruturais considera que a s√©rie\n",
    "temporal pode ser escrita como uma soma de n componentes\n",
    "\n",
    "\n",
    "A modelagem bayesiana assume que os par√¢metros desse modelo seguem distribui√ß√µes de probabilidade (distribui√ß√µes a priori) e, a partir da distribui√ß√£o conjunta dos par√¢metros e dados, obt√©m\n",
    "estimativas a partir da distribui√ß√£o a posteriori dos par√¢metros dadas as observa√ß√µes. No pacote tensorflow_probability s√£o implementados m√©todos de infer√™ncia variacional e Monte Carlo\n",
    "Hamiltoniano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redes Din√¢micas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padronizando os dados para aplicar modelo de redes din√¢micas\n",
    "scaler = MinMaxScaler().fit(train['deaths'].values.reshape(-1,1))\n",
    "scaled_train = scaler.transform(train['deaths'].values.reshape(-1,1))\n",
    "scaled_test = scaler.transform(test['deaths'].values.reshape(-1,1))\n",
    "\n",
    "#modelo de redes din√¢micas LSTM (Long Short-Term Memory)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu',input_shape=(7,1))) #camada LSTM com 100 neur√¥nios\n",
    "model.add(Dense(1)) #camada de sa√≠da com 1 output\n",
    "model.compile(optimizer='adam',loss='mse') #fun√ß√£o de perda de erro quadr√°tico m√©dio\n",
    "model.summary()\n",
    "\n",
    "#ajustando o modelo\n",
    "model.fit_generator(generator, epochs=100) #com 100 intera√ß√µes\n",
    "\n",
    "#plotando um gr√°fico de perda, resultante das itera√ß√µes do ajuste do modelo\n",
    "loss_per_epoch = model.history.history['loss']\n",
    "fig = plt.plot(range(len(loss_per_epoch)),loss_per_epoch)\n",
    "\n",
    "#realizando a previs√£o com os dados de teste\n",
    "test_predictions = []\n",
    "first_batch = scaled_train[-7:]\n",
    "current_batch = first_batch.reshape((1,7,1))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    current_prediction = model.predict(current_batch)[0]\n",
    "    test_predictions.append(current_prediction)\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_prediction]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliando modelos\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "#utilizando erro quadr√°tico m√©dio e erro absoluto m√©dio para compara√ß√£o \n",
    "\n",
    "#compara√ß√£o entre modelos usando MSE\n",
    "print('  _____________________________________________')\n",
    "print(' |               Avali√ß√£o MSE                  |')\n",
    "print(' | ____________________________________________|')\n",
    "print(' | HOLT:                           |', '%.2f' % mean_squared_error(test['deaths'],test['holt']),'|')\n",
    "print(' | HOLT-WINTERS:                   |','%.2f' % mean_squared_error(test['deaths'],test['holt-winters']),' |')\n",
    "print(' | SARIMA(3, 1, 3)x(1, 0, 0, 7):   |','%.2f' % mean_squared_error(test['deaths'],test['sarima']),' |')\n",
    "print(' | LSTM:                           |','%.2f' % mean_squared_error(test['deaths'],test['lstm']),'|')\n",
    "print(' |_________________________________|___________|')\n",
    "print()\n",
    "print()\n",
    "#compara√ß√£o entre modelos usando MAE\n",
    "print('  _____________________________________________')\n",
    "print(' |               Avali√ß√£o MAE                  |')\n",
    "print(' | ____________________________________________|')\n",
    "print(' | HOLT:                           |','%.2f' % mean_absolute_error(test['deaths'],test['holt']),'   |')\n",
    "print(' | HOLT-WINTERS:                   |','%.2f' % mean_absolute_error(test['deaths'],test['holt-winters']),'   |')\n",
    "print(' | SARIMA(3, 1, 3)x(1, 0, 0, 7):   |','%.2f' % mean_absolute_error(test['deaths'],test['sarima']),'   |')\n",
    "print(' | LSTM:                           |','%.2f' % mean_absolute_error(test['deaths'],test['lstm']),'   |')\n",
    "print(' |_________________________________|___________|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
