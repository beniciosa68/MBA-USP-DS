{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientações - Prof. Louzada\n",
    "3. Uma questão será relacionada às disciplinas “Estatística para Ciência de Dados” e “Aprendizado Dinâmico”. Nessa questão, será priorizada a modelagem estatística em aspectos práticos, como a estimação de parâmetros, testes de hipóteses, análise de resíduos, modelos com componentes autorregressivas, integradas e de média móvel. Será disponibilizado um notebook com pacotes com versões específicas recomendadas e alguns comandos. O enunciado trará informações adicionais necessárias para a resolução da questão e as respostas deverão ser apresentadas no moodle. O notebook com as análises desenvolvidas deverá ser adicionado em upload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 04, 05, 06, 07 e Avaliação Final - Estatistica Ciencia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='.\\img\\estimacao-intervalar.jpg'>\n",
    "<img src='.\\img\\hipotese.jpg'>\n",
    "<img src='.\\img\\hipotese2.jpg'>\n",
    "<img src='.\\img\\unicaudal.jpg'>\n",
    "<img src='.\\img\\bicaudal.jpg'>\n",
    "\n",
    "\n",
    "#### Conceitos:\n",
    "Estimativa: Valor do estimador calculado com os dados de uma amostra\n",
    "\n",
    "Distribuição Normal: Distribuição de probabilidades de um estimador\n",
    "\n",
    "Erro padrão: Medida da variabilidade de um estimador\n",
    "\n",
    "Parâmetro: Quantidade, normalmente desconhecida, que especifica uma distribuição de probabilidades na população\n",
    "\n",
    "Estimador: Função da amostra que representa valores plausíveis para o parâmetro desconhecido de interesse\n",
    "\n",
    "Função de Verossimilhança: É um produtório de uma função de dsitribuição de probabilidade (que pode ser qq uma, bernoulli, poisson, normal, etc). Cada uma terá sua fórmula especifica. Precisamos maximizar essa função de verossimilhança para podermos obter os melhores parâmetros para o nosso estimador, gerando a melhor estimativa. Porque estimativa? Pq não conhecemos todos os parâmetros dos nossos dados, temos apenas uma amostra. \n",
    "\n",
    "Inferência Estatística: a idéia é que a partir dessa amostra conseguimos ESTIMAR o todo. Para maximizar, derivamos e igualamos a zero. E para saber o quão confiável é essa estimativa, nós encontramos o intervalo de confiança. Ele é a amplitude (maior menos menor) da curva de distribuição.\n",
    "\n",
    "#### Teste de hipóteses\n",
    "\n",
    "Passos de um teste de hipóteses\n",
    "\n",
    "● Especificar (em termos dos parâmetros) as hipóteses H0 e Ha\n",
    "\n",
    "● Especificar a estatística do teste e sua distribuição, sob H0\n",
    "\n",
    "● Fixar o nível de significância do teste (α)\n",
    "\n",
    "● Calcular o p-valor (ou a região crítica do teste)\n",
    "\n",
    "● Decidir entre H0e Ha, comparando o p-valor com α (ou verificando se a estatística do teste pertence ou não à região crítica)\n",
    "\n",
    "Para o nível descritivel do p-valor, temos: \n",
    "p-valor > alpha: decide por Ho, \n",
    "p-valor <= alpha: decide por Ha\n",
    "\n",
    "Erro do tipo II: é quando aceitamos a hipótese nula, sendo a hipótese alternativa verdadeira\n",
    "\n",
    "Poder do teste: pi (poder do teste) = 1 - beta, sendo beta é a Probabilidade(erro tipo II). O alpha é a Probabilidade(erro tipo I). O objetivo é fixar alpha para diminuir ao máximo o erro do tipo I que é mais crítico, porém, para somar 100% ainda temos que considerar as probabilidades sem erros, portanto, 100% seria: P(erro tipo I) + P(erro tipo II) + P(sem erros para H0) + P(sem erros para Ha)\n",
    "\n",
    "A bicaudal leva em consideração um hipótese que pode ocorrer dos dois lados, com intervalos tanto > quanto <, portanto, \n",
    "cabe numa situação em que H0 é igual e Ha é diferente. Portanto a afirmativa é verdadeira.\n",
    "\n",
    "alpha (nivel de significancia) = P(erro tipo I) = probabilidade de escolhermos Ha, sendo H0 verdadeira.\n",
    "\n",
    "Aumentar ou diminuir o alpha não influência no resultado do p-valor.\n",
    "\n",
    "\n",
    "\n",
    "#### Estatística na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estatística na prática:\n",
    "\n",
    "#funcoes\n",
    "def fatorial(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1 \n",
    "    else:\n",
    "        return n * fatorial(n - 1) \n",
    "    \n",
    "def combinacao(n,x):\n",
    "    return (fatorial(n))/(fatorial(x)*fatorial(n-x))\n",
    "\n",
    "def binomial(n,p,x):\n",
    "    return combinacao(n,x)*(p**x)*((1-p)**(n-x))\n",
    "\n",
    "def bernoulli(p,x):\n",
    "    return (p**x)*((1-p)**(1-x))\n",
    "\n",
    "#exemplo de aplicação:\n",
    "#P(X>=4) = 1 - P(X<=3)\n",
    "#P(X<=3) = P(X=0) + P(X=1) + P(X=2) + P(X=3)\n",
    "P_X_menor_igual_3 = binomial(10,0.2,0) + binomial(10,0.2,1) + binomial(10,0.2,2) + binomial(10,0.2,3)\n",
    "P_X_maior_igual_4 = 1 - P_X_menor_igual_3\n",
    "\n",
    "#Esperança e variância:\n",
    "#Binomial\n",
    "E(X) = np\n",
    "V(X) = np.(1-p)\n",
    "\n",
    "#Poisson\n",
    "E(Y) = np = lambda\n",
    "V(Y) = np = lambda\n",
    "\n",
    "#Testes para verificar normalidade da distribuição:\n",
    "\n",
    "#Teste de Kolmogorov-Smirnov:\n",
    "ks_stat_homem, ks_p_homem = kstest(peso_homem,'norm',args=(peso_homem.describe()[1],peso_homem.describe()[2]),N=len(peso_homem))\n",
    "ks_stat_mulher, ks_p_mulher = kstest(peso_mulher,'norm',args=(peso_mulher.describe()[1],peso_mulher.describe()[2]),N=len(peso_mulher))\n",
    "#para número da amostra > 50:\n",
    "ks_val_homem = 1.36/(np.sqrt(len(peso_homem)))\n",
    "ks_val_mulher = 1.36/(np.sqrt(len(peso_mulher)))\n",
    "ks_res_homem = 'Distribuição é normal' if ks_val_homem >= ks_stat_homem else 'Distribuição NÃO é normal'\n",
    "ks_res_mulher = 'Distribuição é normal' if ks_val_mulher >= ks_stat_mulher else 'Distribuição NÃO é normal'\n",
    "\n",
    "\n",
    "#Teste Anderson-Darling:\n",
    "ad_stat_homem, ad_p_homem, ad_ic_homem = anderson(peso_homem,'norm')\n",
    "ad_stat_mulher, ad_p_mulher, ad_ic_mulher = anderson(peso_mulher,'norm')\n",
    "#para ic 5%\n",
    "ad_res_homem = 'Distribuição é normal' if ad_stat_homem < ad_p_homem[2] else 'Distribuição NÃO é normal'\n",
    "ad_res_mulher = 'Distribuição é normal' if ad_stat_mulher < ad_p_mulher[2] else 'Distribuição NÃO é normal'\n",
    "\n",
    "\n",
    "#Teste de Shapiro-Wilk:\n",
    "#função shapiro -> é um teste estatístico considerando que temos uma amostra normal, ou seja, retorna o p-valor para a normalidade\n",
    "#o primeiro valor do shapiro é o resultado de um teste estatístico e o segundo é o p-valor, que é a probabilidade de aceitarmos\n",
    "#ou rejeitarmos uma hipótese baseada nesse teste estatístico feito.\n",
    "sh_stat_homem, sh_p_homem = shapiro(peso_homem)\n",
    "sh_stat_mulher, sh_p_mulher = shapiro(peso_mulher)\n",
    "#para ic 5%\n",
    "sh_res_homem = 'Distribuição é normal' if sh_p_homem > 0.05 else 'Distribuição NÃO é normal'\n",
    "sh_res_mulher = 'Distribuição é normal' if sh_p_mulher > 0.05 else 'Distribuição NÃO é normal'\n",
    "\n",
    "#Avaliando resultados dos testes de normalidade HOMEM\n",
    "print('Resultados dos testes de normalidade para homem:')\n",
    "print()\n",
    "print('|----------Teste----------|----------Estatística--------|----------P-Valor----------|----------Resultado (IC 5%)----------|')\n",
    "print('|    Kolmogorov-Smirnov   |','   ',ks_stat_homem,'    |','   ',ks_p_homem,'   |','       ',ks_res_homem,'      |')\n",
    "print('|-------------------------|-----------------------------|---------------------------|-------------------------------------|')\n",
    "print('|    Anderson-Darling     |','   ',ad_stat_homem,'     |','   ',ad_p_homem[2],'                |','       ',ad_res_homem,'      |')\n",
    "print('|-------------------------|-----------------------------|---------------------------|-------------------------------------|')\n",
    "print('|    Shapiro-Wilk         |','   ',sh_stat_homem,'     |','   ',sh_p_homem,'  |','       ',sh_res_homem,'  |')\n",
    "print('|-------------------------|-----------------------------|---------------------------|-------------------------------------|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar se as amostras são independentes ou dependentes\n",
    "\n",
    "#teste Z de dependência\n",
    "#para este caso vamos considerar peso como uma variável quantitativa e compará-la entre os dois sexos.\n",
    "stat_peso_homem = [np.mean(peso_homem),np.std(peso_homem),np.std(peso_homem)**2]\n",
    "stat_peso_mulher = [np.mean(peso_mulher),np.std(peso_mulher),np.std(peso_mulher)**2]\n",
    "#valor da estatística do teste\n",
    "Z = (stat_peso_homem[0]-stat_peso_mulher[0])/(math.sqrt((stat_peso_homem[2])/ math.sqrt(len(peso_homem)+len(peso_mulher))))  # estatístiva do teste Z\n",
    "z_pc = norm.ppf(0.05)  #cálculo do ponto crítico (x_corte)\n",
    "z_pvalor = norm.cdf(Z)  #cálculo do p-valor\n",
    "z_res = 'Amostras independentes' if z_pvalor > 0.05 else 'Amostras dependentes'\n",
    "print('Resultado:',z_pvalor)\n",
    "print(z_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar a igualde das variâncias\n",
    "\n",
    "#A hipótese nula é que as variâncias são iguais, e a hipótese alternativa é que as variâncias não são iguais. \n",
    "#Use o teste de Levene quando os dados forem provenientes de distribuições contínuas, mas não necessariamente normais.\n",
    "#O método de cálculo para o teste de Levene é uma modificação do procedimento de Levene (Levene, 1960) que foi desenvolvido \n",
    "#por Brown e Forsythe (1974). Esse método considera as distâncias das observações da mediana da amostra em vez da média da \n",
    "#amostra. O uso da mediana da amostra, em vez da média da amostra, torna o teste mais robusto para amostras de menor dimensão \n",
    "#e torna o procedimento assintoticamente livre de distribuição. Se o valor de p for menor do que o de seu nível de α, \n",
    "#rejeite a hipótese nula de que as variâncias são iguais.\n",
    "\n",
    "#teste de levene\n",
    "lv_stat, lv_p = levene(peso_homem, peso_mulher)\n",
    "lv_res = 'Não existe grande diferença na variância.' if lv_p > 0.05 else 'Existe diferença na variância.'\n",
    "print('Resultado:',lv_p)\n",
    "print(lv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar hipóteses com teste de hipótese\n",
    "\n",
    "#teste t de Student - t-student bicaudal\n",
    "t_stat, t_p = ttest_ind(peso_homem, peso_mulher)\n",
    "t_res = 'Aceita hipótese H0' if t_p > 0.05 else 'Rejeita hipótese H0'\n",
    "print('Resultado:',t_p)\n",
    "print(t_res)\n",
    "\n",
    "#teste unicaudal\n",
    "# Podemos fazer um teste unicaudal com H0: p = 0.9 versus Ha: p > 0.9\n",
    "# Note que, neste caso, o Ha está à direita de H0 e, portanto, o p-valor deve ser calculado a partir da cauda direita\n",
    "n = df.eval('Ano == \"2001-2005\"').sum()\n",
    "p_observado = proporcao.loc['2001-2005']\n",
    "p_valor = 1- stats.norm.cdf(p_observado, .9, np.sqrt(.9 * .1 / n)) # calculado com a cauda da direita\n",
    "p_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar teste se duas amostras tem distribuições diferentes:\n",
    "from scipy import stats\n",
    "#usaremos o teste não paramétrico Mann–Whitney U\n",
    "#A hipótese nula é de que as distribuições são iguais.\n",
    "stats.mannwhitneyu(inalterado, melhorado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressão linear simples\n",
    "from statsmodels.formula.api import ols\n",
    "#Modelo 1: Tamanho\n",
    "mod1 = ols('Peso ~ Tamanho',data=data)\n",
    "res1 = mod1.fit()\n",
    "print(res1.summary())\n",
    "\n",
    "\n",
    "#regressão linear múltipla\n",
    "\n",
    "#Modelo 2: Tamanho, Homem e Acima45 com todas as interações possíveis\n",
    "mod2 = ols('Peso ~ Tamanho * Homem * Acima45',data=data)\n",
    "res2 = mod2.fit()\n",
    "print(res2.summary())\n",
    "\n",
    "#Modelo 3: Tamanho, Homem, Acima45 e respectivas interações\n",
    "mod3 = ols('Peso ~ Tamanho * Homem + Tamanho * Acima45 + Homem * Acima45',data=data)\n",
    "res3 = mod3.fit()\n",
    "print(res3.summary())\n",
    "\n",
    "#Modelo 4: Tamanho, Homem, Acima45. Interações Tamanho - Acima45 e Homem - Acima45\n",
    "mod4 = ols('Peso ~ Tamanho + Tamanho * Acima45 + Homem * Acima45',data=data)\n",
    "res4 = mod4.fit()\n",
    "print(res4.summary())\n",
    "\n",
    "#Modelo 5: Tamanho, Homem, Acima45. Interação: Tamanho - Acima45.\n",
    "mod5 = ols('Peso ~ Tamanho + Homem + Tamanho * Acima45',data=data)\n",
    "res5 = mod5.fit()\n",
    "print(res5.summary())\n",
    "\n",
    "#Modelo 6: Tamanho, Homem, Acima45 e sem interações\n",
    "mod6 = ols('Peso ~ Tamanho + Homem + Acima45',data=data)\n",
    "res6 = mod6.fit()\n",
    "print(res6.summary())\n",
    "\n",
    "#Modelo 7: Tamanho, Homem, Acima45, sem interações, com Tamanho padronizado\n",
    "#padronização (variável menos média dividido pelo desvio padrão)\n",
    "Tamanho_p = (data.Tamanho-np.mean(data.Tamanho))/(np.std(data.Tamanho))\n",
    "mod7 = ols('Peso ~ Tamanho_p + Homem + Acima45',data=data)\n",
    "res7 = mod7.fit()\n",
    "print(res7.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicolinearidade:\n",
    "Voltando com as suspeitas de multicolinearidade identificadas no início da análise, iremos utilizar o cálculo do fator de inflação da variância (VIF) para as variáveis explicativas do modelo. O critério a ser utilizado para análise do VIF será:\n",
    "\n",
    "VIF for igual à 1 não há multicolinearidade entre os fatores;\n",
    "\n",
    "VIF acima de 1, as preditoras podem estar correlacionadas.\n",
    "\n",
    "De 1 até 5: indica alguma correlação, porém, não o suficiente para impactar no modelo;\n",
    "De 5 até 10: alta correlação podendo gerar impacto no modelo;\n",
    "Acima de 10: coeficientes de regressão estão mal estimados devidos à multicolinearidade;\n",
    "\n",
    "Com estes resultados para o VIF, temos a indicação que existe alguma correlação entre as variáveis, porém, ela não é suficiente para impactar no modelo, portanto, vamos considerar esse como sendo o modelo final ajustado.\n",
    "\n",
    "As suposições do nosso modelo ajustado precisam ser validadas para que os resultados sejam confiáveis, para isso vamos realizar a análise de resíduos. A idéia por trás é que se o modelo for apropriado, os resíduos devem refletir algumas propriedades:\n",
    "\n",
    "i.  𝜀𝑖  e  𝜀𝑗  são independentes  (𝑖≠𝑗) ;\n",
    "\n",
    "ii.  𝑉𝑎𝑟(𝜀𝑖)=𝜎2  (constante);\n",
    "\n",
    "iii.  𝜀𝑖∼𝑁(0,𝜎2)  (normalidade);\n",
    "\n",
    "iv. Modelo é linear;\n",
    "\n",
    "v. Não existir outliers (pontos atípicos) influentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula o VIF (Inflação da Variância)\n",
    "var = res6.model.exog\n",
    "vif = [variance_inflation_factor(var, i) for i in range(var.shape[1])]\n",
    "\n",
    "print('|==================|==============================|')\n",
    "print('|     Variáveis    |              VIF             |')\n",
    "print('|------------------|------------------------------|')\n",
    "print('|   Intercept','     |    ',vif[0],'      |',)\n",
    "print('|   Tamanho','       |    ',vif[1],'       |',)\n",
    "print('|   Homem','         |    ',vif[2],'      |',)\n",
    "print('|   Acima45','       |    ',vif[3],'      |',)\n",
    "print('|------------------|------------------------------|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo parâmetros no modelo 7, modelo final ajustado\n",
    "\n",
    "# valores preditos de E(Y)\n",
    "ypred=res7.fittedvalues\n",
    "\n",
    "# objeto para a análise de pontos influentes\n",
    "infl = res7.get_influence()\n",
    "\n",
    "# diagonal da matriz hat\n",
    "hii = infl.hat_matrix_diag\n",
    "\n",
    "# resíduo studentizado (internamente)\n",
    "res_stud = infl.resid_studentized_internal\n",
    "\n",
    "# resíduo studentizado com i-ésima observação deletada (externamente)\n",
    "res_stud_del = infl.resid_studentized_external\n",
    "\n",
    "# DFFITS\n",
    "(dffits,k) = infl.dffits\n",
    "\n",
    "# Distância de Cook\n",
    "(cook,p) = infl.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnóstico de independência\n",
    "\n",
    "#Este teste serve para detectar dependência nos resíduos de uma análise de regressão. \n",
    "#A estatística do teste sempre irá varias entre 0 e 4. Quanto mais próximo de 0, maior a evidência de uma correlação \n",
    "#positiva e quanto mais próxima de 4, correlação negativa. Uma estatística próxima de 2, indica que não temos correlação \n",
    "#nos resíduos, ou seja, são independentes.\n",
    "\n",
    "#teste de durbin watson\n",
    "print('Estatística de Durbin Watson:',durbin_watson(res7.resid,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnóstico de homoscedasticidade (variância constante)\n",
    "\n",
    "plt.scatter(ypred,res_stud) # gráfico dos resíduos versus valores ajustados (valores preditos)\n",
    "plt.ylabel('resíduo studentizado')\n",
    "plt.xlabel('valor ajustado')\n",
    "plt.hlines(0,xmin=min(ypred),xmax=max(ypred),color='gray')\n",
    "print('Observações mais críticas:')\n",
    "print(df.index[res_stud<-3],df.index[res_stud>3]) # identifica as observações mais críticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnóstico de normalidade\n",
    "\n",
    "probplot(res_stud, plot=plt)\n",
    "plt.xlabel('quantis teóricos')\n",
    "plt.ylabel('resíduos ordenados')\n",
    "plt.show()\n",
    "print(data.index[res_stud<-2],data.index[res_stud>2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pontos influentes (DFFITS e distância de cook)\n",
    "\n",
    "#dffits\n",
    "plt.scatter(data.index, dffits)\n",
    "plt.ylabel('DFFITS')\n",
    "plt.hlines(0,xmin=1,xmax=102,color='gray')\n",
    "plt.xlabel('índice')\n",
    "print(df.index[dffits>1]) # valores que estão acima de 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O DFFITS mede a influência que a observação i tem sobre seu próprio valor ajustado. Uma observação é um ponto influente, se:\n",
    "* $ |DFFITS_{(i)}|~>~ 1 $, para amostras pequenas ou médias\n",
    "* $ |DFFITS_{(i)}|~>~ 2\\sqrt{(p+1)/n} $, para amostras grandes, no qual $ (p+1) $ é o número de parâmetros.\n",
    "\n",
    "Neste caso, não temos valores que estão acima de 1, portanto, não temos pontos influentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distância de cook\n",
    "plt.scatter(data.index, cook)\n",
    "plt.ylabel('distância de Cook')\n",
    "plt.xlabel('índice')\n",
    "print(data.index[cook>f.ppf(.5, 5, 126)]) # valores que estão acima do percentil 50 de uma distribuição F(p,n-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distância de Cook mede a influência da observação i sobre todos n valores ajustados. Uma observação é um ponto influente, se:\n",
    "* $ D_i~>~1 $.\n",
    "\n",
    "Neste caso, não temos valores que estão acima de 1, portanto, não temos pontos influentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagonal da matriz chapéu\n",
    "plt.scatter(data.index, hii)\n",
    "plt.ylabel('$h_{ii}$')\n",
    "plt.xlabel('índice')\n",
    "print(data.index[hii>.1])   # h_ii>2p/n (neste caso, .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diagonal da matriz chapéu $ H $ é uma medida padronizada da distância da i-ésima observação para o centro do espaço definido pelas variáveis explicativas. Uma observação é um ponto influente, se:\n",
    "* $ h_{ii} > 2(p+1)/n $\n",
    "\n",
    "Neste caso, não temos ocorrências destacadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos dos resíduos\n",
    "sm.graphics.influence_plot(res7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar Aula 03, 04 e Avaliação Final - Aprendizado Dinamico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O que é uma série temporal?\n",
    "Uma série temporal é qualquer conjunto de observações ordenadas no tempo.\n",
    "\n",
    "São objetivos gerais dos estudos de séries temporais:\n",
    "\n",
    "• Identificar padrões como tendência, sazonalidade, observações discrepantes (outliers);\n",
    "\n",
    "• Usar a variação passada de uma série para predizer valores futuros. Embora não seja possível prever exatamente os valores futuros, podemos predizer um comportamento aproximado das próximas observações;\n",
    "\n",
    "• Entender a variação conjunta de duas séries, e utilizar uma série para explicar a variação em\n",
    "outra série.\n",
    "\n",
    "#### Tendência\n",
    "\n",
    "<img src='.\\img\\tendencia.jpg'>\n",
    "\n",
    "#### Sazonalidade\n",
    "Comportamento da série temporal tende a se repetir a cada s períodos de tempo.\n",
    "\n",
    "<img src='.\\img\\sazonalidade.jpg'>\n",
    "\n",
    "Aditiva. A série apresenta flutuações sazonais mais ou menos constantes não importando o\n",
    "nível global da série.\n",
    "\n",
    "Multiplicativa. O tamanho das flutuações sazonais varia dependendo do nível global da\n",
    "série.\n",
    "\n",
    "Métodos para estimar a sazonalidade:\n",
    "\n",
    "Método da regressão (método determinístico) em que as covariáveis são variáveis periódicas, por exemplo seno, cosseno ou variáveis ‘dummy’, que são variáveis indicadoras\n",
    "\n",
    "Médias móveis (método estocástico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decomposição dos dados de mortes em tendência e sazonalidade:\n",
    "decomposicao = seasonal_decompose(data[data['deaths']>0]['deaths'],model='multiplicative', period=7) #filtrando mortes > 0\n",
    "fig = decomposicao.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Análise de tendência:\n",
    "        É possível observar uma tendência crescente no período de abril. A curva segue depois com uma tendência decrescente até meados de junho. No final de junho, temos um pequeno período de elevação, seguido logo de queda. Depois desse  período a tendência fica muito menos evidente, praticamente sugerindo uma estacionaridade, porém, os testes de Dickey-Fuller realizados anteriormente com p-valor maior que 0.05, sugere que rejeitemos a hipótese de estacionariedade na série.\n",
    "\n",
    "###### Análise de sazonalidade:\n",
    "        Podemos observar ciclos semanais que provavelmente são explicadas pela sistemática de notificações, onde os dados ficam acumulados para cálculo no ínicio da semana e são sumarizados e apresentados mais para o meio da semana. Com relação ao tipo de sazonalidade, o gráfico apresenta algumas variações ao longo da série temporal, principalmente nos períodos de tendência crescente, portanto, consideramos mais adequada representarmos a sazonalidade como sendo do tipo  multiplicativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gráficos de autocorrelação e autocorrelação parcial\n",
    "fig = plot_acf(data['deaths'],title='Autocorrelação: Deaths',lags=50)\n",
    "fig = plot_pacf(data['deaths'],title='Autocorrelação Parcial: Deaths',lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Análise de autocorrelação:\n",
    "        Podemos observar que a correlação entre a série original até a série com atraso lag=16 são bastante significativas  pois encontram-se maiores que o intervalo de confiança (área azul), portanto, precisamos levá-las em consideração  durante a modelagem. Notamos também que há presença de sazonalidade nos dados, apresentando vales/picos em ciclos de 7 dias.\n",
    "        \n",
    "###### Análise de autocorrelação parcial:  \n",
    "        Podemos verificar que a partir do lag=3 a correlação cai bastante, apresentando valores por volta 0.4. Mesmo as  correlações não sendo tão altas, elas ainda são importantes para o modelo, pois apresentam-se fora do intervalo de  confiança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#média movel simples\n",
    "passageiros['MMS-6-meses'] = passageiros['Milhares de passageiros'].rolling(window=6).mean()\n",
    "passageiros['MMS-12-meses'] = passageiros['Milhares de passageiros'].rolling(window=12).mean()\n",
    "\n",
    "#mediana móvel\n",
    "passageiros['MedMS-6-meses'] = passageiros['Milhares de passageiros'].rolling(window=6).median()\n",
    "passageiros['MedMS-12-meses'] = passageiros['Milhares de passageiros'].rolling(window=12).median()\n",
    "\n",
    "#média móvel exponencialmente ponderada (MMEP)\n",
    "passageiros['MMEP12'] = passageiros['Milhares de passageiros'].ewm(span=12,adjust=False).mean()\n",
    "\n",
    "#DICA:\n",
    "#comparando MMS com MMEP\n",
    "passageiros[['Milhares de passageiros','MMS-12-meses','MMEP12']].plot(figsize=(12,9)).autoscale(axis='x',tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses métodos não levam em consideração que a série tem uma componente de tendência.\n",
    "\n",
    "As médias móveis simples têm algumas desvantagens:\n",
    "\n",
    "• Janelas menores levarão a mais ruído, em vez de sinal\n",
    "\n",
    "• Ele nunca alcançará o pico ou vale máximo dos dados devido ao cálculo da média.\n",
    "\n",
    "• Não informa sobre possíveis comportamentos futuros, tudo o que realmente faz é descrever tendências em seus dados.\n",
    "\n",
    "• Valores históricos extremos podem distorcer significativamente a MM\n",
    "\n",
    "Uma possível proposta para contornar esses problemas é a MMEP (média móvel exponencialmente ponderada).\n",
    "\n",
    "A MMEP permite reduzir o efeito de atraso da MMS e dará mais peso aos valores que ocorreram mais recentemente (aplicando mais peso aos valores mais recentes, portanto, o nome). A quantidade de peso aplicada aos valores mais recentes dependerá dos parâmetros reais usados na MMEP e do número de períodos determinados pelo tamanho da janela.\n",
    "\n",
    "Span corresponde ao que é comumente chamado uma “média móvel exponencialmente\n",
    "ponderada em N dias”.\n",
    "\n",
    "• Centro de massa tem uma interpretação física e pode ser pensado em termos do span: c =\n",
    "(s − 1)/2\n",
    "\n",
    "• Meia-vida é o período de tempo para o peso exponencial se reduzir pela metade.\n",
    "\n",
    "• Alpha especifica o fator de suavização diretamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#métodos de suavização holt e holt-winters para a variável mortes:\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "\n",
    "#treinamento e predição pelo método de Holt\n",
    "adjustH = ExponentialSmoothing(train[train['deaths']>0]['deaths'],trend='mul').fit() #filtro de mortes > 0\n",
    "predictH = adjustH.forecast(21).rename('Previsão Holt')\n",
    "predictH.index = data.index[226:]\n",
    "\n",
    "#treinamento e predição pelo método de Holt-Winters\n",
    "adjustHW = ExponentialSmoothing(train[train['deaths']>0]['deaths'],trend='mul',\n",
    "                                seasonal='mul',seasonal_periods=7).fit() #filtro de mortes > 0\n",
    "predictHW = adjustHW.forecast(21).rename('Previsão Holt-Winters')\n",
    "predictHW.index = data.index[226:]\n",
    "\n",
    "#DICA:\n",
    "#imprimindo dados com a predição realizada pelo método de holt e holt-winters\n",
    "test['holt'] = predictH\n",
    "test['holt-winters'] = predictHW\n",
    "\n",
    "train['deaths'].plot(legend=True,label='Treino',title='Predição do modelo pelo método de Holt e Holt-Winters')\n",
    "test['deaths'].plot(legend=True,label='Teste',figsize=(16,6))\n",
    "predictH.plot(legend=True,label='Previsão Holt')\n",
    "fig = predictHW.plot(legend=True,label='Previsão Holt-Winters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vantagens e desvantagens métodos de suavização\n",
    "\n",
    "##### Média móvel simples:\n",
    "\n",
    "• Simples implementação;\n",
    "\n",
    "• Aplicável mesmo com um número pequeno de observações;\n",
    "\n",
    "• Flexibilidade de acordo com o tamanho da janela\n",
    "\n",
    "Porém\n",
    "\n",
    "• Método descritivo e não recomendável para previsões\n",
    "\n",
    "• Não leva em consideração a componente de tendência na série\n",
    "\n",
    "\n",
    "##### Média móvel exponencialmente ponderada\n",
    "\n",
    "• Fácil compreensão e aplicabilidade;\n",
    "\n",
    "• Necessidade de armazenar somente Zt, Zt e α\n",
    "\n",
    "Porém\n",
    "\n",
    "• Método descritivo e não recomendável para previsões\n",
    "\n",
    "• Não leva em consideração a componente de tendência na série\n",
    "\n",
    "##### Método de Holt\n",
    "• Pode ser usado para prever séries que tem tendência\n",
    "\n",
    "• Apresenta um nível maior de dificuldade para encontrar os valores apropriados para as constantes de suavização A e C, em geral que minimizem a soma dos quadrados dos errosdos ajustes\n",
    "\n",
    "##### Método de Holt-Winters\n",
    "• Pode ser usado para prever séries que tem tendência e sazonalidade, seja ela aditiva ou\n",
    "multiplicativa\n",
    "\n",
    "Porém\n",
    "\n",
    "• Impossibilidade e dificuldade para estudar propriedades estatísticas como média e variância\n",
    "das previsões, e consequentemente construção de intervalos de confiança para as previsões\n",
    "\n",
    "• Nível maior de dificuldade para encontrar os valores apropriados para as constantes de\n",
    "suavização A, C e D, em geral que minimizem a soma dos quadrados dos erros dos ajustes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estacionaridade\n",
    "\n",
    "Uma série temporal é estacionária se a sua média, variância e autocovariância são fixas para quaisquer dois pontos equidistantes. Isso significa que, independente de onde tomarmos um subconjunto da série, a média, variância, autocorrelação devem se manter constantes.\n",
    "\n",
    "• Uma série que apresenta sazonalidade ou tendência não é estacionária.\n",
    "\n",
    "O Teste de Dickey-Fuller é usado para testar estacionariedade em um contexto de modelos autorregressivos.\n",
    "\n",
    " -> p-valor maior que 0.05, sugere que rejeitemos a hipótese de estacionariedade na série."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando estacionaridade - primeira e segunda diferença\n",
    "\n",
    "#testando a estacionariedade com Dickey-Fuller\n",
    "result = adfuller(data['deaths'], autolag='AIC')\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# séries de média e desvio-padrão móvel com janela de 12 meses\n",
    "passageiros['MMS-12'] = passageiros['Milhares de passageiros'].rolling(window=12).mean()\n",
    "passageiros['DP-12'] = passageiros['Milhares de passageiros'].rolling(window=12).std()\n",
    "passageiros[['Milhares de passageiros','MMS-12','DP-12']].plot();\n",
    "\n",
    " # Primeiras diferenças\n",
    "y = np.diff(passageiros['Milhares de passageiros'])\n",
    "x = passageiros.iloc[1:].index\n",
    "plt.plot(x,y);\n",
    "\n",
    "# Segundas diferenças\n",
    "y2 = np.diff(y)\n",
    "x2 = x[1:]\n",
    "plt.plot(x2,y2);\n",
    "\n",
    "#boxplot por perido\n",
    "import seaborn as sns\n",
    "passageiros['Ano'] = passageiros.index.year\n",
    "sns.boxplot(x=passageiros['Ano'], y=passageiros['Milhares de passageiros'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função Autocovariância e autocorrelação\n",
    "\n",
    "Antes de falar de autocovariância e autocorrelação, o que é covariância e correlação?\n",
    "\n",
    "Basicamente, a covariância é uma medida de variabilidade conjunta entre duas variáveis aleatórias. Ela mede a força da associação linear entre essas duas variáveis. E a correlação é essa medida de associação linear padronizada, de forma que assuma valores entre -1 e 1. O sinal da covariância e da correlação indica se as variáveis se associam de forma positiva ou\n",
    "negativa.\n",
    "\n",
    "Correlação não significa causalidade!\n",
    "\n",
    "\n",
    "A autocorrelação, também conhecida como correlação serial, é a correlação de um sinal com uma cópia atrasada de si mesma em função do atraso (lag). Informalmente, é a semelhança entre as observações em função do intervalo de tempo entre elas.\n",
    "\n",
    "O correlograma é uma representação das autocorrelações entre as observações da série temporal. Ou seja, cada ponto do gráfico representa a correlação entre a série original e a série com o atraso correspondente.\n",
    "\n",
    "A autocorrelação parcial é uma medida de associação linear de duas variáveis após remover o efeito de outras variáveis que afetam ambas. Na prática, modelos lineares são ajustados para a série “corrente” com a série em atraso como preditor, e os resíduos desse modelo são utilizados para o próximo passo, calcula-se a correlação entre os resíduos e a próxima série em atraso e assim por diante.\n",
    "\n",
    "<img src='.\\img\\autocorrelacao.jpg'>\n",
    "\n",
    "\n",
    "#### Função de autocorrelação (fac) e função de autocorrelação parcial (facp) para processos AR, MA, ARMA\n",
    "\n",
    "Função de autocorrelação (fac):\n",
    "\n",
    "1. Um processo AR(p) tem fac que decai de acordo com exponenciais ou senoides amortecidas,\n",
    "infinita em extensão;\n",
    "\n",
    "2. Um processo MA(q) tem fac finita, no sentido de que ela apresenta um corte após o “lag” q;\n",
    "\n",
    "3. Um processo ARMA (p,q) tem fac infinita em extensão, a qual decai de acordo com exponenciais e/ou senoides amortecidas após o “laq” q-p.\n",
    "\n",
    "\n",
    "A função de autocorrelação parcial também pode auxiliar na identificação do modelo. Entre outras\n",
    "características,\n",
    "\n",
    "1. Um processo MA(q) tem facp que se comporta de maneira similar à fac de um processo\n",
    "AR(p), com decaimento exponencial e/ou senoides amortecidas;\n",
    "\n",
    "2. Um processo ARMA(p,q) tem facp que se comporta como a facp de um processo MA puro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções para cálculo da autocorrelação e autocorrelação parcial\n",
    "from statsmodels.tsa.stattools import acovf, acf, pacf, pacf_yw, pacf_ols\n",
    "acf(df1['Milhares de passageiros'])\n",
    "pacf(df1['Milhares de passageiros'])\n",
    "pacf(df1['Milhares de passageiros'], method='ols')\n",
    "pacf_ols(df1['Milhares de passageiros'])\n",
    "\n",
    "#representação grafica autocorrelação\n",
    "from pandas.plotting import lag_plot\n",
    "lag_plot(df1['Milhares de passageiros']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos ARIMA\n",
    "\n",
    "As etapas são as mesmas que para o ARMA (p, q), exceto que aplicaremos um componente diferencial para tornar o conjunto de dados estacionário. \n",
    "\n",
    "Componentes de um modelo ARIMA (p,d,q):\n",
    "\n",
    "• AR (p): Componentes autorregressivas, utilizam a relação de dependência entre a observação corrente e as observações em um período prévio. O termo autoregressão descreve uma regressão da variável contra ela mesma. Uma regressão automática é executada em um conjunto de valores defasados da ordem p.\n",
    "\n",
    "• Integrado (d): Diferenças para tornar a série estacionária\n",
    "\n",
    "• MA (q): Componentes de médias móveis, utilizam a dependência entre uma obervação e\n",
    "um erro residual de um modelo de média móvel aplicado a observações em atraso.\n",
    "\n",
    "Os modelos ARMA podem ser usado para séries estacionárias se as raízes de φ(B) = 0 caírem todas fora do círculo unitário.\n",
    "Para séries não-estacionárias com uma componente de tendência, os modelos ARIMA podem ser mais adequados.\n",
    "\n",
    "#### Modelos SARIMA\n",
    "\n",
    "Modelos SARIMA (p,d,q)x(P, D,Q)m\n",
    "• SARIMA: ARIMA com sazonalidade\n",
    "\n",
    "Componentes de um modelo SARIMA (p,d,q)x(P,D,Q)m:\n",
    "• (p, d, q): componentes não-sazonais\n",
    "• (P, D, Q)m: componentes sazonais\n",
    "\n",
    "\n",
    "Uma estratégia para a construção do modelo será baseada em um ciclo iterativo, na qual a escolha da estrutura do modelo é baseada nos próprios dados:\n",
    "\n",
    "1. Uma classe geral de modelos é considerada para a análise, no caso modelos ARIMA\n",
    "(especificação)\n",
    "\n",
    "2. Há identificação do modelo, com base na análise de autocorrelações, autocorrelações parciais e outros critérios\n",
    "\n",
    "3. Estimação dos parâmetros do modelo identificado.\n",
    "\n",
    "4. Verificação ou diagnóstico do modelo ajustado, por meio de uma análise de resíduos, para saber se esse modelo é adequado para fazer previsão, por exemplo\n",
    "\n",
    "Se o modelo não for adequado, as estapas 2, 3 e 4 se repetem até obter um ajuste satisfatório. A\n",
    "etapa mais trabalhosa é a identificação.\n",
    "\n",
    "\n",
    "###### Descrição do modelo selecionado: SARIMAX(3, 1, 3)x(1, 0, 0, 7)\n",
    "\n",
    "    - Componente não sazonal autorregressiva de ordem 3\n",
    "    - Componente não sazonal integrado de ordem 1\n",
    "    - Componente não sazonal de médias móveis de ordem 3\n",
    "    - Componente sazonal autorregressiva de primeira ordem com período 7\n",
    "    - Sem componente sazonal integrada\n",
    "    - Sem componente sazonal de média móvel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicando stepwise para selecionar o melhor modelo SARIMA\n",
    "auto_arima(train['deaths'],seasonal=True,m=7).summary()\n",
    "\n",
    "stepwise_fit = auto_arima(train['deaths'], start_p=0, start_q=0,max_p=6,max_q=3,m=7,\n",
    "                          seasonal=True,\n",
    "                          trace=True,\n",
    "                          error_action='ignore',\n",
    "                          supress_warnings=True,\n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()\n",
    "\n",
    "#predição utilizando o modelo SARIMA identificado pelo stepwise\n",
    "start = len(train)\n",
    "end = len(train) + len(test)-1\n",
    "predict_SARIMA = adjustSARIMA.predict(start=start,end=end,\n",
    "                                      dynamic=False,typ='levels').rename('Previsões SARIMA(3, 1, 3)x(1, 0, 0, 7)')\n",
    "predict_SARIMA.index = test.index\n",
    "\n",
    "\n",
    "#DICA:\n",
    "#imprimindo dados com a predição realizada pelo SARIMA\n",
    "train['sarima'] = adjustSARIMA.fittedvalues\n",
    "test['sarima'] = predict_SARIMA\n",
    "\n",
    "train['deaths'].plot(legend=True,label='Treino',title='Predição do modelo SARIMA(3, 1, 3)x(1, 0, 0, 7)')\n",
    "test['deaths'].plot(legend=True,label='Teste',figsize=(16,6))\n",
    "fig = predict_SARIMA.plot(legend=True,label='Previsão SARIMA(3, 1, 3)x(1, 0, 0, 7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resíduos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma forma de obter os resíduos pelo ajuste do modelo\n",
    "resíduos = resultados.resid\n",
    "resíduos.describe()\n",
    "plt.boxplot(resíduos)\n",
    "plot_acf(resíduos, lags=30)\n",
    "plot_pacf(resíduos, lags=30)\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "resíduos.hist()\n",
    "pyplot.show()\n",
    "resíduos.plot(kind='kde')\n",
    "pyplot.show()\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "qqplot(resíduos)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis endógenas e exógenas\n",
    "\n",
    "Em modelos econômicos e econométricos, uma variável exógena refere-se a uma variável que é determinada fora do modelo e representa as entradas de um modelo. Em outras palavras, variáveis exógenas são fixadas no momento em que são introduzidas no modelo. Em contraste, variáveis endógenas são determinadas dentro do modelo e, portanto, representam as saídas de um modelo. O modelo especificado com as variáveis mostra como a mudança de uma variável exógena coeteris paribus afeta todas as variáveis endógenas.\n",
    "\n",
    "Geralmente também variáveis explicativas não são perturbações de uma função de regressão, mas estão correlacionados, sendo por isso chamadas de variáveis exógenas.\n",
    "\n",
    "Nos modelos de desigualdade, os termos \"variável declarada (regressiva) - variável endógena\", bem como \"variável explicativa (regressor) - variável exógena\" se aplicam. Em contraste, em modelos de multi-equação, as variáveis endógenas podem ser regressores e regressões de areia; variáveis exógenas, no entanto, apenas nos regressores.\n",
    "\n",
    "#### Previsão Bayesiana\n",
    "\n",
    "Os modelos de séries temporais estruturais (STS) são vistos como uma família de modelos de\n",
    "probabilidade que incluem\n",
    "\n",
    "• processos autorregressivos,\n",
    "\n",
    "• médias móveis,\n",
    "\n",
    "• tendências lineares locais,\n",
    "\n",
    "• sazonalidade e\n",
    "\n",
    "• regressão e seleção de variáveis em covariáveis externas (outras séries temporais potencialmente relacionadas às séries de interesse).\n",
    "\n",
    "\n",
    "De uma forma bem simplificada, um modelo de séries temporais estruturais considera que a série\n",
    "temporal pode ser escrita como uma soma de n componentes\n",
    "\n",
    "\n",
    "A modelagem bayesiana assume que os parâmetros desse modelo seguem distribuições de probabilidade (distribuições a priori) e, a partir da distribuição conjunta dos parâmetros e dados, obtém\n",
    "estimativas a partir da distribuição a posteriori dos parâmetros dadas as observações. No pacote tensorflow_probability são implementados métodos de inferência variacional e Monte Carlo\n",
    "Hamiltoniano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redes Dinâmicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padronizando os dados para aplicar modelo de redes dinâmicas\n",
    "scaler = MinMaxScaler().fit(train['deaths'].values.reshape(-1,1))\n",
    "scaled_train = scaler.transform(train['deaths'].values.reshape(-1,1))\n",
    "scaled_test = scaler.transform(test['deaths'].values.reshape(-1,1))\n",
    "\n",
    "#modelo de redes dinâmicas LSTM (Long Short-Term Memory)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu',input_shape=(7,1))) #camada LSTM com 100 neurônios\n",
    "model.add(Dense(1)) #camada de saída com 1 output\n",
    "model.compile(optimizer='adam',loss='mse') #função de perda de erro quadrático médio\n",
    "model.summary()\n",
    "\n",
    "#ajustando o modelo\n",
    "model.fit_generator(generator, epochs=100) #com 100 interações\n",
    "\n",
    "#plotando um gráfico de perda, resultante das iterações do ajuste do modelo\n",
    "loss_per_epoch = model.history.history['loss']\n",
    "fig = plt.plot(range(len(loss_per_epoch)),loss_per_epoch)\n",
    "\n",
    "#realizando a previsão com os dados de teste\n",
    "test_predictions = []\n",
    "first_batch = scaled_train[-7:]\n",
    "current_batch = first_batch.reshape((1,7,1))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    current_prediction = model.predict(current_batch)[0]\n",
    "    test_predictions.append(current_prediction)\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_prediction]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliando modelos\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "#utilizando erro quadrático médio e erro absoluto médio para comparação \n",
    "\n",
    "#comparação entre modelos usando MSE\n",
    "print('  _____________________________________________')\n",
    "print(' |               Avalição MSE                  |')\n",
    "print(' | ____________________________________________|')\n",
    "print(' | HOLT:                           |', '%.2f' % mean_squared_error(test['deaths'],test['holt']),'|')\n",
    "print(' | HOLT-WINTERS:                   |','%.2f' % mean_squared_error(test['deaths'],test['holt-winters']),' |')\n",
    "print(' | SARIMA(3, 1, 3)x(1, 0, 0, 7):   |','%.2f' % mean_squared_error(test['deaths'],test['sarima']),' |')\n",
    "print(' | LSTM:                           |','%.2f' % mean_squared_error(test['deaths'],test['lstm']),'|')\n",
    "print(' |_________________________________|___________|')\n",
    "print()\n",
    "print()\n",
    "#comparação entre modelos usando MAE\n",
    "print('  _____________________________________________')\n",
    "print(' |               Avalição MAE                  |')\n",
    "print(' | ____________________________________________|')\n",
    "print(' | HOLT:                           |','%.2f' % mean_absolute_error(test['deaths'],test['holt']),'   |')\n",
    "print(' | HOLT-WINTERS:                   |','%.2f' % mean_absolute_error(test['deaths'],test['holt-winters']),'   |')\n",
    "print(' | SARIMA(3, 1, 3)x(1, 0, 0, 7):   |','%.2f' % mean_absolute_error(test['deaths'],test['sarima']),'   |')\n",
    "print(' | LSTM:                           |','%.2f' % mean_absolute_error(test['deaths'],test['lstm']),'   |')\n",
    "print(' |_________________________________|___________|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
