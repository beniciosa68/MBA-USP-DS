{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exerc4e5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYB3vnWFs1GJ"
      },
      "source": [
        "#instalando Java Runtime Environment (JRE) versão 8\n",
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGqrtdY3s79Q"
      },
      "source": [
        "#baixando Apache Spark versão 3.0.0\n",
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUk6kzVYtBW8"
      },
      "source": [
        "import os\n",
        "#configurando a variável de ambiente JAVA_HOME\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#configurando a variável de ambiente SPARK_HOME\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6jdO0wztHk_"
      },
      "source": [
        "%%capture\n",
        "#instalando o pacote findspark\n",
        "!pip install -q findspark==1.4.2\n",
        "#instalando o pacote pyspark\n",
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azP4CgVTteJE",
        "outputId": "61f4d365-ef4f-4ca9-8cbb-406017e9cb69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "\n",
        "data = [\"\"\"\n",
        "Apache Spark is a unified analytics engine for large-scale data processing.\n",
        "It providers high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
        "\"\"\"]\n",
        "\n",
        "WORD_LEN = 7\n",
        "\n",
        "output = spark. \\\n",
        "         parallelize(data). \\\n",
        "         flatMap(lambda element: element.split(\" \")). \\\n",
        "         filter(lambda element: True if len(element) == WORD_LEN else False). \\\n",
        "         count()\n",
        "\n",
        "print(f'Output...: {output}')\n",
        "spark.stop()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output...: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pttjUFBTtOpn",
        "outputId": "b575f0ac-4fd4-48e6-d415-97a5b4ba2429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "spark = pyspark.SparkContext(appName=\"Exer5_Aval6\")\n",
        "data = [2, 3, 5, 7, True, True, False]\n",
        "def combine_by_type(x, y):\n",
        "    if type(x) is bool and type(y) is bool:\n",
        "        return x and y\n",
        "    elif type(x) is int and type(y) is int:\n",
        "        return x + y\n",
        "    else:\n",
        "        raise Exception()\n",
        "output = spark.parallelize(data).map(lambda element: (type(element), element)).reduceByKey(combine_by_type).map(lambda element: element[1]).sortBy(lambda element: element).collect()\n",
        "print(f'Output...: {output}')\n",
        "spark.stop()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output...: [False, 17]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}