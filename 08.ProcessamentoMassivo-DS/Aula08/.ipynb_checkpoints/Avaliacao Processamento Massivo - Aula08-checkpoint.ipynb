{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\"> MBA em Ciência de Dados</span>\n",
    "# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
    "\n",
    "## <span style=\"color:blue\">Aula 08</span>\n",
    "**Material Produzido por Cristina Dutra de Aguiar Ciferri**<br>\n",
    "**Cemeai - ICMC/USP São Carlos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> As respostas devem ser fornecidas no Moodle. O notebook é apenas para a implementação dos códigos que fornecerão as respostas</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Considere as seguintes afirmações comparativas entre Pandas, Spark RDD e Spark SQL.\n",
    "\n",
    "I. É possível responder à consulta “Qual é a média dos salários recebidos por nível do cargo e por sexo no ano de 2019?” definida sobre a constelação de fatos da BI Solutions utilizando Pandas, especificando a consulta SQL textual como um parâmetro do método spark.sql() e utilizando métodos do módulo pyspark.sql, obtendo, em todos os casos, a mesma resposta.\n",
    "\n",
    "II. Considerando o modelo de 3Vs que define o conceito de big data, pode-se afirmar que, utilizando Apache Spark é possível manipular um gigantesco volume de dados, oferecer suporte para dados estruturados apenas e prover processamento de dados em lote e streaming.\n",
    "\n",
    "III. Tanto a biblioteca Pandas quanto o módulo pyspark.sql utilizam o conceito de DataFrames, sendo que a diferença está relacionada ao fato de que os DataFrames utilizados pelo módulo pyspark.sql são baseados em RDDs e, portanto, embutem aspectos do processamento paralelo e distribuído.\n",
    "\n",
    "IV. Especificar uma consulta usando a linguagem SQL como um parâmetro do método spark.sql() indica que o usuário deve especificar quais dados devem ser obtidos ao invés de como esses dados devem ser obtidos, sendo que a forma como os dados são obtidos é definida pelo otimizador de consultas de forma transparente.\n",
    "\n",
    "V. Especificar uma consulta usando Pandas ou os métodos do módulo pyspark.sql indica que o usuário deve especificar quais dados devem ser obtidos ao invés de como esses dados devem ser obtidos, sendo que a forma como os dados são obtidos é definida pelo otimizador de consultas de forma transparente.\n",
    "\n",
    "Marque:\n",
    "\n",
    "Escolha uma opção:<br>\n",
    "a. Se as afirmativas I, III e IV estão corretas.<br>\n",
    "b. Se as afirmativas I, III e V estão corretas.<br>\n",
    "c. Se as afirmativas I, II, III e IV estão corretas.<br>\n",
    "d. Se as afirmativas I, II, III e V estão corretas.<br>\n",
    "e. Se as afirmativas II, III e V estão corretas.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "Considerando a constelação de fatos da BI Solutions, assinale a alternativa que corresponde à resolução da seguinte consulta usando métodos do módulo pyspark.sql: “Liste, para cada funcionário, sua chave primária, seu nome e a soma dos salários recebidos. Ordene o resultado final em ordem ascendente pela chave primária do funcionário.”\n",
    "\n",
    "Escolha uma opção:\n",
    "```python\n",
    "a. pagamento\\\n",
    ".join(funcionario, on=”funcPK”)\\\n",
    ".select(”funcPK”, ”funcNome”, ”salario”)\\\n",
    ".groupBy(”funcPK”, ”funcNome”)\\\n",
    ".sum(”salario”)\\\n",
    ".orderBy(”funcPK”, ascending=False)\\\n",
    ".show()\n",
    "b. pagamento\\\n",
    ".join(funcionario, on=”funcPK”)\\\n",
    ".select(”funcPK”, ”funcNome”)\\\n",
    ".groupBy(”funcPK”, ”funcNome”)\\\n",
    ".sum(”salario”)\\\n",
    ".orderBy(”funcPK”)\\\n",
    ".show()\n",
    "c. pagamento\\\n",
    ".join(funcionario, on=”funcPK”)\\\n",
    ".select(”funcPK”, ”funcNome”, .sum(”salario”))\\\n",
    ".groupBy(”funcPK”, ”funcNome”)\\\n",
    ".orderBy(”funcPK”)\\\n",
    ".show()\n",
    "d. pagamento\\\n",
    ".join(funcionario, on=”funcPK”)\\\n",
    ".select(”funcPK”, ”funcNome”, ”salario”)\\\n",
    ".groupBy(”funcPK”, ”funcNome”)\\\n",
    ".sum(”salario”)\\\n",
    ".orderBy(”funcPK”)\\\n",
    ".show()\n",
    "e. pagamento\\\n",
    ".join(funcionario, on=”funcPK”)\\\n",
    ".select(”funcPK”, ”funcNome”)\\\n",
    ".groupBy(”funcPK”, ”funcNome”)\\\n",
    ".sum(”salario”)\\\n",
    ".orderBy(”funcPK”)\\\n",
    ".show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3\n",
    "Considerando a constelação de fatos da BI Solutions, assinale a alternativa que NÃO corresponde à resolução da seguinte consulta usando métodos do módulo pyspark.sql: “Liste o ano e o valor do salário recebido pelo funcionário identificado por funcPK = 147 na data identificada por dataPK = 5. ”\n",
    "\n",
    "Escolha uma opção:<br>\n",
    "```python\n",
    "a. pag = pagamento.filter(”funcPK = 147”)\n",
    "dat = data.filter(”dataPK = 5”)\n",
    "pag.join(dat, on=”dataPK”).show()\n",
    "\n",
    "b. pag = pagamento.filter(”funcPK = 147”).select(”dataPK”, ”salario”)\n",
    "dat = data.filter(”dataPK = 5”).select(”dataPK”, ”dataAno”)\n",
    "pag.join(dat, on=”dataPK”).select(”dataAno”, ”salario”).show()\n",
    "\n",
    "c. pagamento\\\n",
    ".join(data, on=”dataPK”)\\\n",
    ".select(”dataAno”, ”salario”)\\\n",
    ".filter(”dataPK = 5 AND funcPK = 147”)\\\n",
    ".show()\n",
    "\n",
    "d. pagamento\\\n",
    ".join(data, on=”dataPK”)\\\n",
    ".filter(”funcPK = 147”)\\\n",
    ".filter(”dataPK = 5”)\\\n",
    ".select(”dataAno”, ”salario”)\\\n",
    ".show()\n",
    "\n",
    "e. pagamento\\\n",
    ".join(data, on=”dataPK”)\\\n",
    ".filter(”funcPK = 147”)\\\n",
    ".select(”dataAno”, ”salario”)\\\n",
    ".filter(”dataPK = 5”)\\\n",
    ".show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4\n",
    "Considere as seguintes afirmações sobre os métodos do módulo pyspark.sql.\n",
    "\n",
    "I. Ambos os métodos join() e crossJoin() unem dois DataFrames, porém no método join() as linhas são combinadas com base na integridade referencial existente entre essas linhas, enquanto que o método crossJoin() combina quaisquer linhas, independente da integridade referencial existente entre elas.\n",
    "\n",
    "II. O método join() pode realizar diferentes tipos de junção, de acordo com o parâmetro how, sendo o valor padrão inner, mas também podendo definir outros valores como cross, outer, full, left, right.\n",
    "\n",
    "III. Ambos os métodos union() e unionAll() não eliminam as linhas repetidas do resultado final, sendo necessário aplicar o método distinct() na sequência para que as linhas repetidas sejam eliminadas.\n",
    "\n",
    "IV. Ambos os métodos rollup() e cube() criam vários níveis de agregação usando como base as colunas especificadas como parâmetro, gerando o mesmo resultado.\n",
    "\n",
    "V. O módulo orderBy() ordena os dados exibidos como resposta à consulta de modo descendente por padrão, sendo necessário definir explicitamente o modo crescente caso seja necessário.\n",
    "\n",
    "Indique se cada uma das afirmações é V (Verdadeira) ou F (Falsa).\n",
    "\n",
    "Escolha uma opção:<br>\n",
    "a. V - V - V - F - F<br>\n",
    "b. V - F - V - V - F<br>\n",
    "c. F - V - F - F - V<br>\n",
    "d. V - V - V - V - V<br>\n",
    "e. V - F - F - V - V<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5\n",
    "Considere a constelação de fatos da BI Solutions e um DataFrame que armazena todos os dados da relação data. Considere o esquema deste DataFrame ilustrado na Figura 1.\n",
    "\n",
    "Considere a sequência de comandos ilustrada na Figura 2.\n",
    "\n",
    "Assinale a alternativa que corresponde ao esquema do DataFrame data após a aplicação dos comandos ilustrados na Figura 2.\n",
    "\n",
    "<img src='.\\img1.png'>\n",
    "<img src='.\\img2.png'>\n",
    "<br>\n",
    "<br>\n",
    "a.<img src='.\\img3.png'><br>\n",
    "b.<img src='.\\img4.png'><br>\n",
    "c.<img src='.\\img5.png'><br>\n",
    "d.<img src='.\\img6.png'><br>\n",
    "e.<img src='.\\img7.png'><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
