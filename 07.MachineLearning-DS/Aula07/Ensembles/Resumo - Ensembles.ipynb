{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Ensembles](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "**Descrição**: São _meta-algoritmos_ que combinam várias técnicas de aprendizado de máquina em um único modelo preditivo ([Wisdom of the crowd](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd)).<br>\n",
    "**Objetivo**: Prover estabilidade. Diminuir a variância (bagging), viés (boosting) ou melhorar as predições (stacking).<br>\n",
    "**Estratégia**: Combinar as predições de modelos especializados. Um grupo de modelos _fracos_ juntos formam um modelo _forte_ ([United we stand](https://en.wikipedia.org/wiki/United_we_stand,_divided_we_fall)).<br>\n",
    "**Problemas**: Podem ser aplicados tanto em problemas de classificação quanto em regressão.<br>\n",
    "**Estruturas**:\n",
    "- Paralelo: Os modelos (heterogêneos ou homogêneos) são treinados paralelamente de forma indepentende e então combinados para gerar a previsão.\n",
    "- Sequencial: Os modelos são treinados sequencialmente de modo que o erro seja minimizado ao longo da sequência.<br>\n",
    "\n",
    "**Premissas** por *James Michael Surowiecki sobre Wisdom of the crowd*:\n",
    "- Independência: Capacidade de formar uma opinião independente da opinião alheia.\n",
    "- Decentralização: Capacidade de especializar e tirar conclusões baseado em informação local.\n",
    "- Diversidade: Capacidade de manter informação privada mesmo que seja uma opinião enviesada.\n",
    "- Agregação: Algum mecanismo capaz de tornar julgamentos privados em uma decisão coletiva.<br>\n",
    "\n",
    "**Observações**:\n",
    "- Não são recomendados quando é importante ter interpretabilidade e explicabilidade\n",
    "- Tempo de computação e desenvolvimento é alto!\n",
    "- Selecionar modelos para criar ensembles não é trivial\n",
    "- Nem sempre irá produzir resultados bons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bias-variance-tradeoff.svg\" width =\"300\" height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Erro} = \\text{Viés}(\\hat\\theta, \\theta) + \\text{Variância}(\\hat\\theta, \\theta) + \\text{Ruído}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"simple.svg\" width =\"500\" height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.223 Average expected loss\n",
      "0.160 Average bias\n",
      "0.157 Average variance\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "\n",
    "# A métrica 0-1_loss é calculada da seguinte forma:\n",
    "# se y != ŷ então 1 senão 0\n",
    "# O ruido é ignorado por uma questão de simplificação\n",
    "loss, bias, var = bias_variance_decomp(tree, X_train, y_train, X_test, y_test, loss='0-1_loss', random_seed=0)\n",
    "\n",
    "print('%.3f Average expected loss' % loss)\n",
    "print('%.3f Average bias' % bias)\n",
    "print('%.3f Average variance' % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "- Estrutura sequencial\n",
    "- Objetivo é reduzir o viés (bias) do modelo (não a variância)\n",
    "- Adequado para modelos com baixa variância e alto viés (Baseados em Árvores)\n",
    "- Baseado na [Hypothesis Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) a qual assume que um conjunto de modelos fracos são capazes de produzir modelos fortes.\n",
    "- Exemplos comuns: [ExtraTrees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier), [GradientBoosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier), [HistGradientBoosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boosting.svg\" width =\"500\" height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.150 Average expected loss\n",
      "0.068 Average bias\n",
      "0.131 Average variance\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "boosting = AdaBoostClassifier(base_estimator=tree, n_estimators=15, random_state=1)\n",
    "\n",
    "loss, bias, var = bias_variance_decomp(boosting, X_train, y_train, X_test, y_test, loss='0-1_loss', random_seed=1)\n",
    "\n",
    "print('%.3f Average expected loss' % loss)\n",
    "print('%.3f Average bias' % bias)\n",
    "print('%.3f Average variance' % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) (aka Bootstrap AGGregatING)\n",
    "- Estrutura paralela e faz a combinação dos modelos usando votação (classificação) ou média (regressão)\n",
    "- Objetivo é reduzir a variância do modelo (não o viés)\n",
    "- Adequado para modelos com alta variância e baixo viés (modelos complexos) e conjuntos de dados pequenos/moderados em número de instâncias.\n",
    "- Baseado na estratégia de [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) a qual faz uma _amostragem com reposição_ de mesma cardinalidade do conjunto de dados original permitindo assim reduzir a variância dos dados.\n",
    "- Exemplo comum: [RandomForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bagging.svg\" width =\"700\" height=700 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.168 Average expected loss\n",
      "0.140 Average bias\n",
      "0.096 Average variance\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "bagging = BaggingClassifier(base_estimator=tree, n_estimators=15, random_state=1)\n",
    "\n",
    "loss, bias, var = bias_variance_decomp(bagging, X_train, y_train, X_test, y_test, loss='0-1_loss', random_seed=1)\n",
    "\n",
    "print('%.3f Average expected loss' % loss)\n",
    "print('%.3f Average bias' % bias)\n",
    "print('%.3f Average variance' % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Stacking](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) (aka Stacked Generalization)\n",
    "- Estrutura paralela e faz combinação dos modelos usando um modelo (_e.g._ [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html))\n",
    "- Objetivo é aumentar o poder preditivo (diminuir o erro) do modelo\n",
    "- Baseado na ideia de que diferentes modelos podem aprender diferentes padrões nos dados mas não o suficiente para generalizar a maioria dos padrões. Dessa forma, a estratégia é usar um modelo que generalize os padrões encontrados por diferentes modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stacking.svg\" width =\"600\" height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048 Average expected loss\n",
      "0.040 Average bias\n",
      "0.019 Average variance\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('rdg', RidgeClassifier(normalize=True)),\n",
    "    ('gnb', GaussianNB()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('dt' , DecisionTreeClassifier(max_depth=10, random_state=1)),\n",
    "    ('svm', make_pipeline(StandardScaler(), SVC(random_state=42)))\n",
    "]\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "loss, bias, var = bias_variance_decomp(stacking, X_train, y_train, X_test, y_test, loss='0-1_loss', random_seed=1)\n",
    "\n",
    "print('%.3f Average expected loss' % loss)\n",
    "print('%.3f Average bias' % bias)\n",
    "print('%.3f Average variance' % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Bagging Ensemble with Python](https://machinelearningmastery.com/bagging-ensemble-with-python/)\n",
    "2. [Stacking Ensemble with Python](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/)\n",
    "3. [Ensemble Learning](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)\n",
    "4. [Main Approaches for Ensemble Learning](https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html)\n",
    "5. [Simple Guide for Ensemble Leraning Methods](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2)\n",
    "6. [Bias-Variance Decomposition](http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/)\n",
    "7. [MIT - Boosting](https://www.youtube.com/watch?v=UHBmv7qCey4)\n",
    "8. [Stacking and Blending](https://medium.com/@stevenyu530_73989/stacking-and-blending-intuitive-explanation-of-advanced-ensemble-methods-46b295da413c)\n",
    "9. [Imagem: Bias-Variance Tradeoff](https://www.machinelearningtutorial.net/wp-content/uploads/2017/01/bias-variance-tradeoff.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [vecstack](https://github.com/vecxoz/vecstack)\n",
    "2. [mlens](https://github.com/flennerhag/mlens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
